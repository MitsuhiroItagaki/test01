# Databricks SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆ†æãƒ„ãƒ¼ãƒ«

**æœ€å…ˆç«¯ã®AIé§†å‹•SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ„ãƒ¼ãƒ«**

Databricksã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€AIï¼ˆLLMï¼‰ã‚’æ´»ç”¨ã—ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šãƒ»æ”¹å–„æ¡ˆæç¤ºãƒ»SQLæœ€é©åŒ–ã‚’è¡Œã†åŒ…æ‹¬çš„ãªåˆ†æãƒ„ãƒ¼ãƒ«ã§ã™ã€‚

## âœ¨ ä¸»è¦æ©Ÿèƒ½

### ğŸ” **é«˜åº¦ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ**
- SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•è§£æ
- æ™‚é–“æ¶ˆè²»TOP10ãƒ—ãƒ­ã‚»ã‚¹ã®è©³ç´°åˆ†æï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«åè¡¨ç¤ºå¯¾å¿œï¼‰
- ã‚¹ãƒ”ãƒ«æ¤œå‡ºãƒ»ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ»ä¸¦åˆ—åº¦å•é¡Œã®ç‰¹å®š
- Photonã‚¨ãƒ³ã‚¸ãƒ³åˆ©ç”¨çŠ¶æ³ã®å¯è¦–åŒ–

### ğŸ¤– **ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼AIåˆ†æ**
- **Databricks Claude 3.7 Sonnet**ï¼ˆæ¨å¥¨ï¼‰
- **OpenAI GPT-4/GPT-4 Turbo**
- **Azure OpenAI**
- **Anthropic Claude**
- 128K tokenså¯¾å¿œãƒ»æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰

### ğŸ—‚ï¸ **Liquid Clusteringæœ€é©åŒ–**
- ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚«ãƒ©ãƒ ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ»JOINãƒ»GROUP BYæ¡ä»¶ã®è‡ªå‹•æŠ½å‡º
- ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¨å¥¨ã‚«ãƒ©ãƒ ã®ç‰¹å®š
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šè¦‹è¾¼ã¿ã®å®šé‡è©•ä¾¡

### ğŸš€ **SQLè‡ªå‹•æœ€é©åŒ–**
- ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®è‡ªå‹•æŠ½å‡º
- AIé§†å‹•ã«ã‚ˆã‚‹ã‚¯ã‚¨ãƒªæœ€é©åŒ–
- å®Ÿè¡Œå¯èƒ½ãªæœ€é©åŒ–SQLã®ç”Ÿæˆ
- ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆã®è‡ªå‹•ä½œæˆ

### ğŸ“Š **åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒ†ã‚£ãƒ³ã‚°**
- JSONãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ»Markdownãƒ»Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•ç”Ÿæˆ
- è¦–è¦šçš„ãªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¡¨ç¤º
- è©³ç´°ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®å®šé‡çš„è©•ä¾¡

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
ğŸ“¦ Databricks SQL Profiler Analysis Tool
â”œâ”€â”€ ğŸ“„ databricks_sql_profiler_analysis.py    # ğŸŒŸ ãƒ¡ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼ˆ24ã‚»ãƒ«æ§‹æˆï¼‰
â”œâ”€â”€ ğŸ“„ simple0.json                           # ã‚µãƒ³ãƒ—ãƒ«SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ ğŸ“„ README.md                              # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ ğŸ“ outputs/                               # ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå®Ÿè¡Œæ™‚ä½œæˆï¼‰
â”‚   â”œâ”€â”€ ğŸ“„ extracted_metrics_YYYYMMDD-HHMISS.json
â”‚   â”œâ”€â”€ ğŸ“„ bottleneck_analysis_YYYYMMDD-HHMISS.txt
â”‚   â”œâ”€â”€ ğŸ“„ original_query_YYYYMMDD-HHMISS.sql
â”‚   â”œâ”€â”€ ğŸ“„ optimized_query_YYYYMMDD-HHMISS.sql
â”‚   â”œâ”€â”€ ğŸ“„ optimization_report_YYYYMMDD-HHMISS.md
â”‚   â””â”€â”€ ğŸ“„ test_optimized_query_YYYYMMDD-HHMISS.py
â””â”€â”€ ğŸ“ samples/                               # è¿½åŠ ã‚µãƒ³ãƒ—ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    â”œâ”€â”€ ğŸ“„ largeplan.json
    â””â”€â”€ ğŸ“„ nophoton.json
```

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### ã‚¹ãƒ†ãƒƒãƒ— 1: Notebookã®ä½œæˆ

1. **Databricks ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹**ã§æ–°ã—ã„Notebookã‚’ä½œæˆ
2. è¨€èªã‚’ã€Œ**Python**ã€ã«è¨­å®š
3. `databricks_sql_profiler_analysis.py`ã®å†…å®¹ã‚’ã‚³ãƒ”ãƒ¼ï¼†ãƒšãƒ¼ã‚¹ãƒˆ

### ã‚¹ãƒ†ãƒƒãƒ— 2: åŸºæœ¬è¨­å®š

```python
# ğŸ“ åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šï¼ˆã‚»ãƒ«2ï¼‰
JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/simple0.json'

# ğŸ¤– LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®šï¼ˆã‚»ãƒ«3ï¼‰
LLM_CONFIG = {
    "provider": "databricks",  # "databricks", "openai", "azure_openai", "anthropic"
    "databricks": {
        "endpoint_name": "databricks-claude-3-7-sonnet"
    }
}
```

### ã‚¹ãƒ†ãƒƒãƒ— 3: é †æ¬¡å®Ÿè¡Œ

```bash
ğŸ”§ è¨­å®šãƒ»æº–å‚™ã‚»ã‚¯ã‚·ãƒ§ãƒ³     â†’ ã‚»ãƒ«2ã€œ10ã‚’å®Ÿè¡Œ
ğŸš€ ãƒ¡ã‚¤ãƒ³å‡¦ç†å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³  â†’ ã‚»ãƒ«11ã€œ17ã‚’å®Ÿè¡Œ
ğŸ”§ SQLæœ€é©åŒ–æ©Ÿèƒ½ã‚»ã‚¯ã‚·ãƒ§ãƒ³   â†’ ã‚»ãƒ«18ã€œ23ã‚’å®Ÿè¡Œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
ğŸ“š å‚è€ƒãƒ»å¿œç”¨ã‚»ã‚¯ã‚·ãƒ§ãƒ³      â†’ ã‚»ãƒ«24å‚ç…§
```

## ğŸ“‹ ã‚»ãƒ«æ§‹æˆè©³ç´°

### ğŸ”§ è¨­å®šãƒ»æº–å‚™ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆã‚»ãƒ«2-10ï¼‰
| ã‚»ãƒ« | æ©Ÿèƒ½ | èª¬æ˜ |
|-----|-----|-----|
| 2 | ğŸ“ åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š | JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®æŒ‡å®š |
| 3 | ğŸ¤– LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®š | AIåˆ†æãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®é¸æŠ |
| 4 | ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–¢æ•° | DBFS/FileStore/ãƒ­ãƒ¼ã‚«ãƒ«å¯¾å¿œ |
| 5 | ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºé–¢æ•° | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®æŠ½å‡º |
| 6 | ğŸ·ï¸ ãƒãƒ¼ãƒ‰åè§£æé–¢æ•° | æ„å‘³ã®ã‚ã‚‹ãƒãƒ¼ãƒ‰åã¸ã®å¤‰æ› |
| 7 | ğŸ¯ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯è¨ˆç®—é–¢æ•° | æŒ‡æ¨™è¨ˆç®—ã¨ã‚¹ãƒ”ãƒ«æ¤œå‡º |
| 8 | ğŸ§¬ Liquid Clusteringé–¢æ•° | ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æ |
| 9 | ğŸ¤– LLMåˆ†æé–¢æ•° | AIåˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ |
| 10 | ğŸ”Œ LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é–¢æ•° | å„AIã‚µãƒ¼ãƒ“ã‚¹æ¥ç¶š |

### ğŸš€ ãƒ¡ã‚¤ãƒ³å‡¦ç†å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆã‚»ãƒ«11-17ï¼‰
| ã‚»ãƒ« | æ©Ÿèƒ½ | èª¬æ˜ |
|-----|-----|-----|
| 11 | ğŸš€ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Ÿè¡Œ | JSONãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ |
| 12 | ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º | æ€§èƒ½æŒ‡æ¨™ã®æŠ½å‡ºã¨è¡¨ç¤º |
| 13 | ğŸ” ãƒœãƒˆãƒ«ãƒãƒƒã‚¯è©³ç´°åˆ†æ | TOP10æ™‚é–“æ¶ˆè²»ãƒ—ãƒ­ã‚»ã‚¹ |
| 14 | ğŸ’¾ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜ | JSONãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ› |
| 15 | ğŸ—‚ï¸ Liquid Clusteringåˆ†æ | ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¨å¥¨ |
| 16 | ğŸ“‹ LLMåˆ†ææº–å‚™ | AIåˆ†æã®å®Ÿè¡Œæº–å‚™ |
| 17 | ğŸ¯ AIåˆ†æçµæœè¡¨ç¤º | ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ |

### ğŸ”§ SQLæœ€é©åŒ–æ©Ÿèƒ½ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆã‚»ãƒ«18-23ï¼‰
| ã‚»ãƒ« | æ©Ÿèƒ½ | èª¬æ˜ |
|-----|-----|-----|
| 18 | ğŸ”§ æœ€é©åŒ–é–¢æ•°å®šç¾© | SQLæœ€é©åŒ–é–¢æ•°ã®å®šç¾© |
| 19 | ğŸš€ ã‚¯ã‚¨ãƒªæŠ½å‡º | ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®æŠ½å‡º |
| 20 | ğŸ¤– LLMæœ€é©åŒ–å®Ÿè¡Œ | AIé§†å‹•ã‚¯ã‚¨ãƒªæœ€é©åŒ– |
| 21 | ğŸ’¾ çµæœä¿å­˜ | æœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ |
| 22 | ğŸ§ª ãƒ†ã‚¹ãƒˆæº–å‚™ | å®Ÿè¡Œã‚¬ã‚¤ãƒ‰ã¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ |
| 23 | ğŸ å®Œäº†ã‚µãƒãƒªãƒ¼ | å…¨å‡¦ç†ã®å®Œäº†ç¢ºèª |

## ğŸ”§ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—è©³ç´°

### 1. LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¨­å®š

#### Databricks Claude 3.7 Sonnetï¼ˆæ¨å¥¨ï¼‰

```bash
# Databricks CLI ã§ã®ä½œæˆ
databricks serving-endpoints create \
  --name "databricks-claude-3-7-sonnet" \
  --config '{
    "served_entities": [{
      "entity_name": "databricks-claude-3-7-sonnet", 
      "entity_version": "1",
      "workload_type": "GPU_MEDIUM",
      "workload_size": "Small"
    }]
  }'
```

#### ä»–ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼

```python
# OpenAIè¨­å®šä¾‹
LLM_CONFIG = {
    "provider": "openai",
    "openai": {
        "api_key": "sk-...",  # ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°OPENAI_API_KEY
        "model": "gpt-4o",
        "max_tokens": 2000
    }
}

# Azure OpenAIè¨­å®šä¾‹  
LLM_CONFIG = {
    "provider": "azure_openai",
    "azure_openai": {
        "api_key": "your-azure-key",
        "endpoint": "https://your-resource.openai.azure.com/",
        "deployment_name": "gpt-4",
        "api_version": "2024-02-01"
    }
}

# Anthropicè¨­å®šä¾‹
LLM_CONFIG = {
    "provider": "anthropic", 
    "anthropic": {
        "api_key": "sk-ant-...",  # ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ANTHROPIC_API_KEY
        "model": "claude-3-5-sonnet-20241022"
    }
}
```

### 2. SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®å–å¾—

#### Databricks SQLã‚¨ãƒ‡ã‚£ã‚¿ã‹ã‚‰å–å¾—

1. **SQLã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ**
2. **Query History** â†’ å¯¾è±¡ã‚¯ã‚¨ãƒªã‚’é¸æŠ
3. **Query Profile** ã‚¿ãƒ– â†’ **Download Profile JSON**

#### ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ†ã‚£ãƒƒã‚¯å–å¾—

```sql
-- ã‚·ã‚¹ãƒ†ãƒ ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å–å¾—
SELECT query_id, query_text, query_start_time, total_time_ms
FROM system.query.history 
WHERE query_start_time >= current_timestamp() - INTERVAL 1 DAY
  AND total_time_ms > 30000  -- 30ç§’ä»¥ä¸Šã®ã‚¯ã‚¨ãƒª
ORDER BY total_time_ms DESC
LIMIT 10;
```

#### ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ–¹æ³•

```python
# æ–¹æ³•1: Databricks UI
# Data â†’ Create Table â†’ Upload File â†’ JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—

# æ–¹æ³•2: dbutils
dbutils.fs.cp("file:/local/path/profiler.json", "dbfs:/FileStore/profiler.json")

# æ–¹æ³•3: Databricks CLI
# databricks fs cp profiler.json dbfs:/FileStore/profiler.json
```

## ğŸ“Š å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°

### ğŸ“„ extracted_metrics_YYYYMMDD-HHMISS.json

```json
{
  "query_info": {
    "query_id": "01f0565c-48f6-1283-a782-14ed6494eee0",
    "status": "FINISHED", 
    "user": "user@company.com",
    "query_text": "SELECT customer_id, SUM(amount)..."
  },
  "overall_metrics": {
    "total_time_ms": 84224,
    "compilation_time_ms": 876,
    "execution_time_ms": 83278,
    "read_bytes": 123926013605,
    "photon_enabled": true,
    "photon_utilization_ratio": 0.85
  },
  "bottleneck_indicators": {
    "compilation_ratio": 0.010,
    "cache_hit_ratio": 0.003,
    "data_selectivity": 0.000022,
    "has_spill": true,
    "spill_bytes": 1073741824,
    "shuffle_operations_count": 3,
    "has_shuffle_bottleneck": true
  },
  "liquid_clustering_analysis": {
    "recommended_tables": {
      "customer": {
        "clustering_columns": ["customer_id", "region"],
        "scan_performance": {
          "rows_scanned": 1000000,
          "scan_duration_ms": 15000,
          "efficiency_score": 66.67
        }
      }
    }
  }
}
```

### ğŸ“„ bottleneck_analysis_YYYYMMDD-HHMISS.txt

```text
ğŸ” Databricks SQL ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆ†æçµæœ

ğŸ“Š ã€ã‚¯ã‚¨ãƒªåŸºæœ¬æƒ…å ±ã€‘
ğŸ†” ã‚¯ã‚¨ãƒªID: 01f0565c-48f6-1283-a782-14ed6494eee0
â±ï¸ å®Ÿè¡Œæ™‚é–“: 84,224 ms (84.2ç§’)
ğŸ’¾ èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿: 115.4 GB
ğŸ“ˆ å‡ºåŠ›è¡Œæ•°: 2,753 è¡Œ

ğŸš¨ ã€ç‰¹å®šã•ã‚ŒãŸãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã€‘

1. ğŸ”¥ **å¤§é‡ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ (HIGH PRIORITY)**
   - ã‚¹ãƒ”ãƒ«é‡: 1.0 GB
   - åŸå› : ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ã‚ˆã‚‹ä¸­é–“çµæœã®ãƒ‡ã‚£ã‚¹ã‚¯æ›¸ãè¾¼ã¿
   - å½±éŸ¿: å®Ÿè¡Œæ™‚é–“ã®30-50%å¢—åŠ 

2. âš¡ **ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œãƒœãƒˆãƒ«ãƒãƒƒã‚¯ (MEDIUM PRIORITY)**  
   - ã‚·ãƒ£ãƒƒãƒ•ãƒ«å›æ•°: 3å›
   - æœ€å¤§ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ™‚é–“: 15,234 ms
   - å½±éŸ¿: å…¨ä½“å®Ÿè¡Œæ™‚é–“ã®18%

3. ğŸ“Š **ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§ã®å•é¡Œ (MEDIUM PRIORITY)**
   - é¸æŠæ€§: 0.0022% (éå¸¸ã«ä½ã„)
   - èª­ã¿è¾¼ã¿115.4GB â†’ å‡ºåŠ›2,753è¡Œ
   - æ”¹å–„ä½™åœ°: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã®æœ€é©åŒ–

ğŸš€ ã€æ¨å¥¨æ”¹å–„ç­–ã€‘

1. **ãƒ¡ãƒ¢ãƒªè¨­å®šã®æœ€é©åŒ–**
   - spark.sql.adaptive.coalescePartitions.enabled = true
   - ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¡ãƒ¢ãƒªå¢—å¼· (32GB â†’ 64GBæ¨å¥¨)

2. **Liquid Clusteringã®é©ç”¨**
   - customer ãƒ†ãƒ¼ãƒ–ãƒ«: customer_id, region ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
   - æœŸå¾…åŠ¹æœ: ã‚¹ã‚­ãƒ£ãƒ³æ™‚é–“50-70%å‰Šæ¸›

3. **ã‚¯ã‚¨ãƒªæ§‹é€ ã®æœ€é©åŒ–**
   - WHEREå¥ã‚’JOINã‚ˆã‚Šå‰ã«é…ç½®
   - ä¸è¦ãªã‚«ãƒ©ãƒ ã®é™¤å¤–
   - ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å‰ªå®šã®æ´»ç”¨

ğŸ“ˆ ã€æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æœã€‘
- å®Ÿè¡Œæ™‚é–“: 84.2ç§’ â†’ 35-45ç§’ (ç´„50%å‰Šæ¸›)
- ã‚³ã‚¹ãƒˆå‰Šæ¸›: ç´„60%
- ã‚¹ãƒ”ãƒ«è§£æ¶ˆ: 100%å‰Šæ¸›è¦‹è¾¼ã¿
```

### ğŸ“„ optimized_query_YYYYMMDD-HHMISS.sql

```sql
-- æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒª
-- å…ƒã‚¯ã‚¨ãƒªID: 01f0565c-48f6-1283-a782-14ed6494eee0
-- æœ€é©åŒ–æ—¥æ™‚: 2024-01-15 14:30:22
-- ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒª: original_query_20240115-143022.sql

-- PHOTONã‚¨ãƒ³ã‚¸ãƒ³æœ€é©åŒ–ã¨Liquid Clusteringå¯¾å¿œ
WITH customer_filtered AS (
  SELECT customer_id, region, signup_date
  FROM customer 
  WHERE region IN ('US', 'EU')  -- æ—©æœŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    AND signup_date >= '2023-01-01'
),
orders_summary AS (
  SELECT 
    customer_id,
    SUM(amount) as total_amount,
    COUNT(*) as order_count
  FROM orders 
  WHERE order_date >= '2023-01-01'  -- Liquid Clusteringæ´»ç”¨
  GROUP BY customer_id
)
SELECT /*+ BROADCAST(c) */
  c.customer_id,
  c.region,
  COALESCE(o.total_amount, 0) as total_amount,
  COALESCE(o.order_count, 0) as order_count
FROM customer_filtered c
LEFT JOIN orders_summary o ON c.customer_id = o.customer_id
ORDER BY total_amount DESC
LIMIT 100;
```

## ğŸ” é«˜åº¦ãªæ©Ÿèƒ½

### ğŸ“Š ã‚«ã‚¹ã‚¿ãƒ åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

```python
# analyze_bottlenecks_with_llmé–¢æ•°ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
custom_prompt = f"""
ã‚ãªãŸã¯é‡‘èæ¥­ç•Œã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®è¦³ç‚¹ã§åˆ†æã—ã¦ãã ã•ã„ï¼š
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å–å¼•å‡¦ç†ã¸ã®å½±éŸ¿
- è¦åˆ¶è¦ä»¶ã¸ã®é©åˆæ€§
- ç½å®³å¾©æ—§æ™‚ã®æ€§èƒ½
- ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ç›£æŸ»ã¸ã®å¯¾å¿œ

{standard_analysis_content}

æ¥­ç•Œç‰¹æœ‰ã®æ¨å¥¨äº‹é …:
- SOXæ³•å¯¾å¿œã®ãŸã‚ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶
- Basel IIIè¦åˆ¶ä¸‹ã§ã®ãƒªã‚¹ã‚¯è¨ˆç®—æœ€é©åŒ–
- GDPRå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹æœ€é©åŒ–
"""
```

### ğŸ”„ ä¸€æ‹¬åˆ†æãƒ»æ¯”è¼ƒ

```python
# è¤‡æ•°ã‚¯ã‚¨ãƒªã®æ€§èƒ½æ¯”è¼ƒ
profiler_files = [
    '/path/to/query1_profile.json',
    '/path/to/query2_profile.json', 
    '/path/to/query3_profile.json'
]

comparison_results = []
for file_path in profiler_files:
    profiler_data = load_profiler_json(file_path)
    metrics = extract_performance_metrics(profiler_data)
    comparison_results.append({
        'query_id': metrics['query_info']['query_id'],
        'execution_time': metrics['overall_metrics']['total_time_ms'],
        'data_processed_gb': metrics['overall_metrics']['read_bytes'] / 1024**3,
        'spill_detected': metrics['bottleneck_indicators']['has_spill'],
        'cache_efficiency': metrics['bottleneck_indicators']['cache_hit_ratio']
    })

# DataFrameåŒ–ã—ã¦æ¯”è¼ƒ
comparison_df = spark.createDataFrame(comparison_results)
comparison_df.show()
```

### ğŸ“ˆ è‡ªå‹•ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ

```python
# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã®è‡ªå‹•æ¤œå‡º
def monitor_query_performance():
    """å®šæœŸå®Ÿè¡Œã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–"""
    recent_queries = spark.sql("""
        SELECT query_id, query_text, total_time_ms, read_bytes
        FROM system.query.history 
        WHERE query_start_time >= current_timestamp() - INTERVAL 1 HOUR
        AND total_time_ms > 60000  -- 1åˆ†ä»¥ä¸Šã®ã‚¯ã‚¨ãƒª
    """)
    
    for row in recent_queries.collect():
        # é–¾å€¤ãƒã‚§ãƒƒã‚¯
        if row.total_time_ms > 300000:  # 5åˆ†ä»¥ä¸Š
            send_performance_alert(row.query_id, "Long execution time detected")
        
        if row.read_bytes > 100 * 1024**3:  # 100GBä»¥ä¸Š
            send_performance_alert(row.query_id, "Large data scan detected")

# Slack/Teamsé€šçŸ¥
def send_performance_alert(query_id, message):
    webhook_url = "YOUR_SLACK_WEBHOOK_URL"
    payload = {
        "text": f"ğŸš¨ Performance Alert: {message}\nQuery ID: {query_id}"
    }
    requests.post(webhook_url, json=payload)
```

## ğŸ› ï¸ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### âŒ ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨è§£æ±ºæ–¹æ³•

#### 1. LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼

```bash
# ã‚¨ãƒ©ãƒ¼ä¾‹
APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ 404
HTTP 404: Model serving endpoint 'databricks-claude-3-7-sonnet' not found

# è§£æ±ºæ–¹æ³•
1. ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå­˜åœ¨ç¢ºèª:
   databricks serving-endpoints list

2. ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆçŠ¶æ…‹ç¢ºèª: 
   databricks serving-endpoints get databricks-claude-3-7-sonnet

3. æ¨©é™ç¢ºèª:
   - Model Servingã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™
   - Personal Access Tokenã®æœ‰åŠ¹æ€§
```

#### 2. ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼

```python
# ã‚¨ãƒ©ãƒ¼ä¾‹  
OutOfMemoryError: Java heap space

# è§£æ±ºæ–¹æ³•
# 1. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¡ãƒ¢ãƒªå¢—å¼·
spark.conf.set("spark.driver.memory", "16g")
spark.conf.set("spark.executor.memory", "32g")

# 2. å¤§ããªJSONãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†å‰²å‡¦ç†
def process_large_json(file_path, chunk_size=1000000):
    """å¤§ããªJSONãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†å‰²å‡¦ç†"""
    with open(file_path, 'r') as f:
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã§éƒ¨åˆ†çš„ã«èª­ã¿è¾¼ã¿
        pass

# 3. ä¸è¦ãªãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
def extract_essential_metrics_only(profiler_data):
    """å¿…è¦æœ€å°é™ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã¿æŠ½å‡º"""
    essential_data = {
        'query': profiler_data.get('query', {}),
        'graphs': profiler_data.get('graphs', [])[:1]  # æœ€åˆã®ã‚°ãƒ©ãƒ•ã®ã¿
    }
    return essential_data
```

#### 3. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼

```python
# ã‚¨ãƒ©ãƒ¼ä¾‹
FileNotFoundError: [Errno 2] No such file or directory

# è§£æ±ºæ–¹æ³•ã¨ãƒ‡ãƒãƒƒã‚°
# 1. ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
def debug_file_access(file_path):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã®ãƒ‡ãƒãƒƒã‚°"""
    try:
        # DBFSç¢ºèª
        if file_path.startswith('/dbfs/') or file_path.startswith('dbfs:/'):
            dbfs_files = dbutils.fs.ls(file_path.replace('/dbfs', '').replace('dbfs:', ''))
            print(f"DBFS files found: {len(dbfs_files)}")
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ç¢ºèª
        import os
        if os.path.exists(file_path):
            size = os.path.getsize(file_path)
            print(f"Local file found, size: {size} bytes")
        else:
            print(f"File not found: {file_path}")
            
    except Exception as e:
        print(f"Debug error: {e}")

# ä½¿ç”¨ä¾‹
debug_file_access('/dbfs/FileStore/shared_uploads/user/profiler.json')
```

### ğŸ”§ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

```python
# å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®æœ€é©åŒ–
def optimize_large_scale_analysis():
    """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿åˆ†æã®æœ€é©åŒ–è¨­å®š"""
    
    # Sparkè¨­å®šã®æœ€é©åŒ–
    spark.conf.set("spark.sql.adaptive.enabled", "true")
    spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true") 
    spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
    spark.conf.set("spark.sql.adaptive.localShuffleReader.enabled", "true")
    
    # ãƒ¡ãƒ¢ãƒªè¨­å®š
    spark.conf.set("spark.driver.maxResultSize", "8g")
    spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")
    
    # ä¸¦åˆ—å‡¦ç†ã®æœ€é©åŒ–
    num_cores = spark.sparkContext.defaultParallelism
    optimal_partitions = num_cores * 2
    spark.conf.set("spark.sql.shuffle.partitions", str(optimal_partitions))
    
    print(f"âœ… æœ€é©åŒ–è¨­å®šå®Œäº†: {num_cores} cores, {optimal_partitions} partitions")

# å®Ÿè¡Œå‰ã«æœ€é©åŒ–è¨­å®šã‚’é©ç”¨
optimize_large_scale_analysis()
```

## ğŸ“š ã‚¢ãƒ‰ãƒãƒ³ã‚¹æ´»ç”¨

### ğŸ¯ æ¥­ç•Œç‰¹åŒ–ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

```python
# é‡‘èæ¥­ç•Œå‘ã‘ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºä¾‹
class FinancialSQLAnalyzer:
    def __init__(self):
        self.regulatory_keywords = ['risk', 'basel', 'var', 'stress_test']
        self.performance_thresholds = {
            'trading_queries': 1000,  # 1ç§’ä»¥å†…
            'risk_calculations': 30000,  # 30ç§’ä»¥å†…
            'reporting_queries': 300000  # 5åˆ†ä»¥å†…
        }
    
    def analyze_regulatory_compliance(self, metrics):
        """è¦åˆ¶è¦ä»¶ã¸ã®é©åˆæ€§åˆ†æ"""
        query_text = metrics['query_info']['query_text'].lower()
        execution_time = metrics['overall_metrics']['total_time_ms']
        
        # ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
        query_type = 'general'
        for keyword in self.regulatory_keywords:
            if keyword in query_text:
                if keyword in ['risk', 'var']:
                    query_type = 'risk_calculations'
                elif keyword == 'stress_test':
                    query_type = 'trading_queries'
                break
        
        # æ€§èƒ½è¦ä»¶ãƒã‚§ãƒƒã‚¯
        threshold = self.performance_thresholds.get(query_type, 60000)
        compliance_status = "COMPLIANT" if execution_time <= threshold else "NON_COMPLIANT"
        
        return {
            'query_type': query_type,
            'compliance_status': compliance_status,
            'threshold_ms': threshold,
            'actual_ms': execution_time,
            'deviation_pct': ((execution_time - threshold) / threshold) * 100
        }

# ä½¿ç”¨ä¾‹
financial_analyzer = FinancialSQLAnalyzer()
compliance_result = financial_analyzer.analyze_regulatory_compliance(extracted_metrics)
print(f"Compliance Status: {compliance_result['compliance_status']}")
```

### ğŸ”„ CI/CDçµ±åˆ

```yaml
# .github/workflows/sql-performance-check.yml
name: SQL Performance Analysis

on:
  pull_request:
    paths:
      - 'sql/**'
      - 'queries/**'

jobs:
  performance-analysis:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Databricks CLI
        run: |
          pip install databricks-cli
          echo "${{ secrets.DATABRICKS_TOKEN }}" | databricks configure --token
          
      - name: Run SQL Performance Analysis
        run: |
          # å¤‰æ›´ã•ã‚ŒãŸSQLãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å®Ÿè¡Œ
          for sql_file in $(git diff --name-only ${{ github.event.before }} HEAD | grep '\.sql$'); do
            echo "Analyzing $sql_file"
            
            # SQLã‚’å®Ÿè¡Œã—ã¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
            databricks sql-exec --file $sql_file --profile-output profile_$sql_file.json
            
            # åˆ†æãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
            python sql_profiler_analysis.py --input profile_$sql_file.json --output analysis_$sql_file.txt
            
            # æ€§èƒ½åŠ£åŒ–ãƒã‚§ãƒƒã‚¯
            if [ $(grep "CRITICAL" analysis_$sql_file.txt | wc -l) -gt 0 ]; then
              echo "âŒ Performance issues detected in $sql_file"
              exit 1
            fi
          done
          
      - name: Comment PR
        uses: actions/github-script@v6
        if: failure()
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'ğŸš¨ SQL performance issues detected. Please review the analysis results.'
            })
```

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒ»å…è²¬äº‹é …

### ğŸ“œ åˆ©ç”¨æ¡ä»¶

- **ç”¨é€”**: æ•™è‚²ãƒ»ç ”ç©¶ãƒ»å†…éƒ¨åˆ†æç›®çš„ã§ã®ä½¿ç”¨ã‚’æƒ³å®š
- **ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒ**: äº‹å‰ã®ååˆ†ãªãƒ†ã‚¹ãƒˆã¨æ¤œè¨¼ãŒå¿…è¦
- **ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼**: æ©Ÿå¯†æ€§ã®é«˜ã„ã‚¯ã‚¨ãƒªãƒ­ã‚°ã®å–ã‚Šæ‰±ã„ã«æ³¨æ„
- **ã‚³ã‚¹ãƒˆç®¡ç†**: LLM APIä½¿ç”¨æ–™ã¨ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆãƒªã‚½ãƒ¼ã‚¹åˆ©ç”¨æ–™ã«æ³¨æ„

### âš ï¸ å…è²¬äº‹é …

- æœ¬ãƒ„ãƒ¼ãƒ«ã®åˆ†æçµæœã¯å‚è€ƒæƒ…å ±ã§ã‚ã‚Šã€å®Ÿéš›ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã‚’ä¿è¨¼ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“
- AIç”Ÿæˆã®æœ€é©åŒ–SQLã¯æœ¬ç•ªç’°å¢ƒã§ã®å®Ÿè¡Œå‰ã«å¿…ãšæ¤œè¨¼ã—ã¦ãã ã•ã„
- å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚„è¤‡é›‘ãªã‚¯ã‚¨ãƒªã®åˆ†æã«ã¯å®Ÿè¡Œæ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒå¢—å¤§ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
- Databricks Claude 3.7 Sonnetã®åˆ©ç”¨ã«ã¯é©åˆ‡ãªãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¨æ¨©é™ãŒå¿…è¦ã§ã™

## ğŸ¤ ã‚µãƒãƒ¼ãƒˆãƒ»ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£

### ğŸ“ æŠ€è¡“ã‚µãƒãƒ¼ãƒˆ

**å•é¡Œå ±å‘Šæ™‚ã®æƒ…å ±:**
```
1. ç’°å¢ƒæƒ…å ±
   - Databricks Runtime version: __________
   - Cluster configuration: _______________
   - Python version: _____________________

2. ã‚¨ãƒ©ãƒ¼è©³ç´°
   - ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: ___________________
   - ç™ºç”Ÿã‚»ãƒ«ç•ªå·: _______________________
   - ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹: __________________

3. ä½¿ç”¨çŠ¶æ³
   - JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: ________________
   - LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: ___________________
   - ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå†…å®¹: __________________
```

### ğŸš€ æ©Ÿèƒ½ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ»æ”¹å–„ææ¡ˆ

**æ­“è¿ã™ã‚‹è²¢çŒ®:**
- æ–°ã—ã„LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å¯¾å¿œ
- è¿½åŠ ã®åˆ†æãƒ¡ãƒˆãƒªã‚¯ã‚¹
- æ¥­ç•Œç‰¹åŒ–ã®åˆ†æãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
- å¯è¦–åŒ–æ©Ÿèƒ½ã®å¼·åŒ–
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### ğŸ“ˆ ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

**è¿‘æ—¥å®Ÿè£…äºˆå®š:**
- ğŸ”„ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†ææ©Ÿèƒ½
- ğŸ“Š Databricks SQLãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰çµ±åˆ
- ğŸ¤– è‡ªå‹•SQLæœ€é©åŒ–ã®ç²¾åº¦å‘ä¸Š
- ğŸ¯ A/Bãƒ†ã‚¹ãƒˆæ©Ÿèƒ½
- ğŸ“± ãƒ¢ãƒã‚¤ãƒ«å¯¾å¿œãƒ¬ãƒãƒ¼ãƒˆ

---

**ğŸ‰ ã“ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®æ—…ã‚’å§‹ã‚ã¾ã—ã‚‡ã†ï¼**

ğŸ“§ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ»è³ªå•: [GitHub Issues](https://github.com/your-repo/databricks-sql-profiler-analysis/issues)  
ğŸ“– è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: [Wiki](https://github.com/your-repo/databricks-sql-profiler-analysis/wiki)  
ğŸ’¬ ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£: [Discussions](https://github.com/your-repo/databricks-sql-profiler-analysis/discussions)
