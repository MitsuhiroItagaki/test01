# Databricks SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆ†æãƒ„ãƒ¼ãƒ«

Databricksã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šã¨æ”¹å–„æ¡ˆã®æç¤ºã«å¿…è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºãƒ»åˆ†æã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚

## æ©Ÿèƒ½æ¦‚è¦

1. **SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿**
   - Databricksã§å‡ºåŠ›ã•ã‚ŒãŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ­ã‚°ã®è§£æ
   - `graphs`ã‚­ãƒ¼ã«æ ¼ç´ã•ã‚ŒãŸå®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º

2. **é‡è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º**
   - ã‚¯ã‚¨ãƒªåŸºæœ¬æƒ…å ±ï¼ˆIDã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã€å®Ÿè¡Œæ™‚é–“ãªã©ï¼‰
   - å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆå®Ÿè¡Œæ™‚é–“ã€ãƒ‡ãƒ¼ã‚¿é‡ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ãªã©ï¼‰
   - ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ»ãƒãƒ¼ãƒ‰è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹
   - ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è¨ˆç®—

3. **AI ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ**
   - Databricks Claude 3.7 Sonnetã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨
   - æŠ½å‡ºãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
   - å…·ä½“çš„ãªæ”¹å–„æ¡ˆã®æç¤º

## ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

- `databricks_sql_profiler_analysis_script.py` - å®Œå…¨ç‰ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆä¸€æ‹¬å®Ÿè¡Œç”¨ï¼‰
- `databricks_notebook_cells.py` - **Notebookç‰ˆï¼ˆæ¨å¥¨ï¼‰**ï¼šã‚»ãƒ«åˆ†å‰²ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰
- `simple0.json` - ã‚µãƒ³ãƒ—ãƒ«ã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«
- `extracted_metrics.json` - æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆå‡ºåŠ›ï¼‰
- `bottleneck_analysis_result.txt` - AIåˆ†æçµæœï¼ˆå‡ºåŠ›ï¼‰
- `README.md` - ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«

## ğŸ“š ä½¿ç”¨æ–¹æ³•

### ğŸ”¥ æ¨å¥¨: Databricks Notebookç‰ˆ

#### ã‚¹ãƒ†ãƒƒãƒ— 1: Notebookã®ä½œæˆ

1. Databricks ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã§æ–°ã—ã„Notebookã‚’ä½œæˆ
2. è¨€èªã‚’ã€ŒPythonã€ã«è¨­å®š

#### ã‚¹ãƒ†ãƒƒãƒ— 2: ã‚»ãƒ«ã®ã‚³ãƒ”ãƒ¼ãƒšãƒ¼ã‚¹ãƒˆ

`databricks_notebook_cells.py` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ãã€å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä»¥ä¸‹ã®æ‰‹é †ã§ã‚³ãƒ”ãƒ¼ãƒšãƒ¼ã‚¹ãƒˆã—ã¾ã™ï¼š

```python
# === ã‚»ãƒ« 1: ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚»ãƒ« ===
# ã‚»ãƒ«ã‚¿ã‚¤ãƒ—ã‚’ã€ŒMarkdownã€ã«å¤‰æ›´ã—ã¦ä»¥ä¸‹ã‚’ã‚³ãƒ”ãƒ¼
```

**å…·ä½“çš„ãªæ‰‹é †:**

1. **ã‚»ãƒ« 1 (Markdown)**: ã‚»ãƒ«ã‚¿ã‚¤ãƒ—ã‚’ã€ŒMarkdownã€ã«å¤‰æ›´ã—ã¦ã€æ©Ÿèƒ½èª¬æ˜ã‚’ã‚³ãƒ”ãƒ¼
2. **ã‚»ãƒ« 2 (Python)**: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ
3. **ã‚»ãƒ« 3 (Python)**: JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°
4. **ã‚»ãƒ« 4 (Python)**: ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºé–¢æ•°
5. **ã‚»ãƒ« 5 (Python)**: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™è¨ˆç®—é–¢æ•°
6. **ã‚»ãƒ« 6 (Python)**: Claude 3.7 Sonnetåˆ†æé–¢æ•°
7. **ã‚»ãƒ« 7 (Markdown)**: ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®èª¬æ˜
8. **ã‚»ãƒ« 8 (Python)**: è¨­å®šã¨ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
9. **ã‚»ãƒ« 9 (Python)**: ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºã¨è¡¨ç¤º
10. **ã‚»ãƒ« 10 (Python)**: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜ã¨DataFrameè¡¨ç¤º
11. **ã‚»ãƒ« 11 (Python)**: AIåˆ†æå®Ÿè¡Œ
12. **ã‚»ãƒ« 12 (Python)**: åˆ†æçµæœè¡¨ç¤ºã¨ä¿å­˜
13. **ã‚»ãƒ« 13 (Markdown)**: è¿½åŠ ã®ä½¿ç”¨æ–¹æ³•ã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

#### ã‚¹ãƒ†ãƒƒãƒ— 3: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¨­å®š

ã‚»ãƒ« 8 ã§ `JSON_FILE_PATH` ã‚’å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›´ï¼š

```python
# ä¾‹: DBFSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
JSON_FILE_PATH = '/dbfs/FileStore/shared_uploads/your_username/profiler_log.json'

# ä¾‹: dbfs:// å½¢å¼ã®å ´åˆ
JSON_FILE_PATH = 'dbfs:/FileStore/shared_uploads/your_username/profiler_log.json'
```

#### ã‚¹ãƒ†ãƒƒãƒ— 4: å®Ÿè¡Œ

ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œï¼ˆShift+Enter ã¾ãŸã¯ ã€Œâ–¶ Run Allã€ï¼‰

### ğŸ–¥ï¸ ä»£æ›¿: Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆç‰ˆ

```python
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®Ÿè¡Œ
%run /FileStore/databricks_sql_profiler_analysis_script.py
```

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

### 1. Databricksç’°å¢ƒã®æº–å‚™

#### Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¨­å®š

1. **Databricks ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã«ãƒ­ã‚°ã‚¤ãƒ³**

2. **Claude 3.7 Sonnetã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ä½œæˆ**
   
   **UI ã§ã®ä½œæˆ:**
   - Databricks UI > Serving > Model Serving
   - "Create Serving Endpoint" ã‚’ã‚¯ãƒªãƒƒã‚¯
   - ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå: `databricks-claude-3-7-sonnet`
   - ãƒ¢ãƒ‡ãƒ«: Claude 3.7 Sonnet
   - ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚µã‚¤ã‚º: Small ã¾ãŸã¯ Medium

   **CLI ã§ã®ä½œæˆ:**
   ```bash
   databricks serving-endpoints create \
     --name "databricks-claude-3-7-sonnet" \
     --config '{
       "served_entities": [{
         "entity_name": "databricks-claude-3-7-sonnet",
         "entity_version": "1",
         "workload_type": "GPU_MEDIUM",
         "workload_size": "Small"
       }]
     }'
   ```

3. **ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ç¢ºèª**
   - Databricks UI > Serving > Model Serving
   - `databricks-claude-3-7-sonnet` ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒã€ŒReadyã€çŠ¶æ…‹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª

#### æ¨©é™è¨­å®š

1. **Personal Access Token ã®ä½œæˆ**
   - Settings > Developer > Access tokens
   - æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆï¼ˆModel Servingã®æ¨©é™ãŒå¿…è¦ï¼‰

2. **ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹æ¨©é™**
   - Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™
   - ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ã®å®Ÿè¡Œæ¨©é™

### 2. å¿…è¦ãªä¾å­˜é–¢ä¿‚

Databricksç’°å¢ƒã«ã¯ä»¥ä¸‹ãŒæ¨™æº–è£…å‚™ã•ã‚Œã¦ã„ã¾ã™ï¼š

```python
# æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆé€šå¸¸ã¯æ—¢ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ï¼‰
import json
import pandas as pd
from typing import Dict, List, Any
import requests

# Sparké–¢é€£ï¼ˆDatabricksã«æ¨™æº–è£…å‚™ï¼‰
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
```

### 3. SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å–å¾—ã¨é…ç½®

#### Databricks UI ã‹ã‚‰å–å¾—

1. **SQL Editor ã¾ãŸã¯ Notebook ã§ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ**

2. **Query History ã‹ã‚‰è©²å½“ã‚¯ã‚¨ãƒªã‚’é¸æŠ**
   - Compute > SQL Warehouses > Query History
   - ã¾ãŸã¯ç›´æ¥ SQL Editor ã®å±¥æ­´ã‹ã‚‰

3. **Query Profile ã‚’é–‹ã**
   - ã‚¯ã‚¨ãƒªè©³ç´°ãƒšãƒ¼ã‚¸ã® "Query Profile" ã‚¿ãƒ–

4. **JSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ**
   - "Download Profile JSON" ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
   - ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜

#### ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ–¹æ³•

**æ–¹æ³• 1: Databricks UI ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**
1. Data > Create Table
2. "Upload File" ã‚’é¸æŠ
3. JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
4. ãƒ‘ã‚¹ã‚’ãƒ¡ãƒ¢ï¼ˆä¾‹: `/FileStore/shared_uploads/user@company.com/profile.json`ï¼‰

**æ–¹æ³• 2: dbutils ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**
```python
# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’DBFSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
dbutils.fs.cp("file:/local/path/profiler.json", "dbfs:/FileStore/profiler.json")

# å¤–éƒ¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰ã®ã‚³ãƒ”ãƒ¼
dbutils.fs.cp("s3a://bucket/profiler.json", "dbfs:/FileStore/profiler.json")
```

**æ–¹æ³• 3: Databricks CLI**
```bash
# ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰DBFSã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
databricks fs cp profiler.json dbfs:/FileStore/profiler.json
```

#### SQL ã§ã®å–å¾—

```sql
-- ã‚¯ã‚¨ãƒªå±¥æ­´ã‹ã‚‰ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å–å¾—
SELECT query_id, query_text, profile_json 
FROM system.query.history 
WHERE query_start_time >= current_timestamp() - INTERVAL 1 DAY
ORDER BY query_start_time DESC
LIMIT 10;
```

## ğŸ“Š å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«

### extracted_metrics.json

```json
{
  "query_info": {
    "query_id": "01f0565c-48f6-1283-a782-14ed6494eee0",
    "status": "FINISHED",
    "user": "mitsuhiro.itagaki@databricks.com"
  },
  "overall_metrics": {
    "total_time_ms": 84224,
    "compilation_time_ms": 876,
    "execution_time_ms": 83278,
    "read_bytes": 123926013605,
    "cache_hit_ratio": 0.003
  },
  "bottleneck_indicators": {
    "compilation_ratio": 0.010,
    "cache_hit_ratio": 0.003,
    "data_selectivity": 0.000022,
    "slowest_stage_id": "229"
  }
}
```

### bottleneck_analysis_result.txt

AI ã«ã‚ˆã‚‹è©³ç´°ãªåˆ†æçµæœï¼ˆæ—¥æœ¬èªï¼‰ï¼š
- ä¸»è¦ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ç‰¹å®š
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®å„ªå…ˆé †ä½
- å…·ä½“çš„ãªæœ€é©åŒ–æ¡ˆ
- äºˆæƒ³ã•ã‚Œã‚‹æ”¹å–„åŠ¹æœ

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

1. **Claude 3.7 Sonnetã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼**
   ```
   APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ 404
   ```
   - ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆåã‚’ç¢ºèª: `databricks-claude-3-7-sonnet`
   - Model Servingã§ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒç¨¼åƒä¸­ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
   - ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®çŠ¶æ…‹ãŒã€ŒReadyã€ã«ãªã£ã¦ã„ã‚‹ã‹ç¢ºèª

2. **èªè¨¼ã‚¨ãƒ©ãƒ¼**
   ```
   åˆ†æã‚¨ãƒ©ãƒ¼: 'dbutils' is not defined
   ```
   - Databricksã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä¸Šã§å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
   - Personal Access Tokenã®æ¨©é™ã‚’ç¢ºèª

3. **JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼**
   ```
   ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: [Errno 2] No such file or directory
   ```
   - ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç¢ºèª
   - ãƒ•ã‚¡ã‚¤ãƒ«ãŒDBFS ã¾ãŸã¯ ãƒ­ãƒ¼ã‚«ãƒ«ã«æ­£ã—ãã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª

### ãƒ‡ãƒãƒƒã‚°æ–¹æ³•

1. **ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ**
   ```python
   # å„æ®µéšã§ã®ç¢ºèª
   print("1. JSONèª­ã¿è¾¼ã¿:", bool(profiler_data))
   print("2. ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º:", len(extracted_metrics.get('node_metrics', [])))
   print("3. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™:", extracted_metrics.get('bottleneck_indicators', {}))
   ```

2. **ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæ¥ç¶šãƒ†ã‚¹ãƒˆ**
   ```python
   # Claude ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®æ¥ç¶šç¢ºèª
   try:
       token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
       workspace_url = spark.conf.get("spark.databricks.workspaceUrl")
       print(f"Workspace URL: {workspace_url}")
       print(f"Token length: {len(token)}")
   except Exception as e:
       print(f"èªè¨¼è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
   ```

3. **ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª**
   ```python
   # DBFSãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
   dbutils.fs.ls("/FileStore/shared_uploads/")
   
   # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
   import os
   print(f"Current directory: {os.getcwd()}")
   print(f"Files: {os.listdir('.')}")
   ```

## âš™ï¸ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

```python
# extract_performance_metrics é–¢æ•°å†…ã®é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å¤‰æ›´
important_keywords = [
    'TIME', 'DURATION', 'MEMORY', 'ROWS', 'BYTES', 'SPILL',
    'PEAK', 'CUMULATIVE', 'EXCLUSIVE', 'WAIT', 'CPU',
    'NETWORK', 'DISK', 'SHUFFLE'  # è¿½åŠ ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
]
```

### åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

```python
# analyze_bottlenecks_with_claude é–¢æ•°å†…ã® analysis_prompt ã‚’å¤‰æ›´
analysis_prompt = f"""
ã‚ãªãŸã¯ç‰¹å®šã®æ¥­ç•Œã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å°‚é–€å®¶ã§ã™...
è¿½åŠ ã®åˆ†æè¦³ç‚¹:
- ETLå‡¦ç†ã®æœ€é©åŒ–
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æã¸ã®é©ç”¨æ€§
- ã‚³ã‚¹ãƒˆåŠ¹ç‡æ€§ã®è©•ä¾¡
...
"""
```

### å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

```python
# HTML ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¾‹
def generate_html_report(metrics, analysis_result):
    html_content = f"""
    <html>
    <head>
        <title>SQL Performance Analysis Report</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 40px; }}
            .metric {{ background: #f5f5f5; padding: 10px; margin: 5px 0; }}
            .bottleneck {{ color: red; font-weight: bold; }}
        </style>
    </head>
    <body>
        <h1>ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æãƒ¬ãƒãƒ¼ãƒˆ</h1>
        <div class="metric">
            <h3>ã‚¯ã‚¨ãƒªåŸºæœ¬æƒ…å ±</h3>
            <p>ID: {metrics['query_info']['query_id']}</p>
            <p>å®Ÿè¡Œæ™‚é–“: {metrics['overall_metrics']['total_time_ms']:,} ms</p>
        </div>
        <div class="analysis">
            <h3>AIåˆ†æçµæœ</h3>
            <pre>{analysis_result}</pre>
        </div>
    </body>
    </html>
    """
    with open('analysis_report.html', 'w', encoding='utf-8') as f:
        f.write(html_content)
    print("HTML ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: analysis_report.html")

# ä½¿ç”¨ä¾‹
generate_html_report(extracted_metrics, analysis_result)
```

### SparkDataFrame ã§ã®è©³ç´°åˆ†æ

```python
# ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è©³ç´°åˆ†æ
node_data = []
for node in extracted_metrics['node_metrics']:
    node_data.append({
        'node_id': node['node_id'],
        'name': node['name'],
        'tag': node['tag'],
        'rows_num': node['key_metrics'].get('rowsNum', 0),
        'duration_ms': node['key_metrics'].get('durationMs', 0),
        'peak_memory_bytes': node['key_metrics'].get('peakMemoryBytes', 0)
    })

node_df = spark.createDataFrame(node_data)
node_df.createOrReplaceTempView("node_metrics")

# SQL ã§ã®åˆ†æ
result = spark.sql("""
    SELECT 
        name,
        duration_ms,
        peak_memory_bytes / 1024 / 1024 as peak_memory_mb,
        rows_num,
        CASE 
            WHEN duration_ms > 10000 THEN 'HIGH'
            WHEN duration_ms > 1000 THEN 'MEDIUM'
            ELSE 'LOW'
        END as duration_category
    FROM node_metrics
    WHERE rows_num > 0
    ORDER BY duration_ms DESC
""")

result.show()
```

## ğŸš€ é«˜åº¦ãªä½¿ç”¨ä¾‹

### è¤‡æ•°ã‚¯ã‚¨ãƒªã®ä¸€æ‹¬åˆ†æ

```python
# è¤‡æ•°ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬å‡¦ç†
profiler_files = [
    'dbfs:/FileStore/profiles/query1.json',
    'dbfs:/FileStore/profiles/query2.json',
    'dbfs:/FileStore/profiles/query3.json'
]

all_results = []
for file_path in profiler_files:
    profiler_data = load_profiler_json(file_path)
    if profiler_data:
        metrics = extract_performance_metrics(profiler_data)
        all_results.append(metrics)

# ä¸€æ‹¬åˆ†æçµæœã®æ¯”è¼ƒ
comparison_df = spark.createDataFrame([
    {
        'query_id': result['query_info']['query_id'],
        'total_time_ms': result['overall_metrics']['total_time_ms'],
        'read_gb': result['overall_metrics']['read_bytes'] / 1024 / 1024 / 1024,
        'cache_hit_ratio': result['bottleneck_indicators'].get('cache_hit_ratio', 0)
    }
    for result in all_results
])

comparison_df.show()
```

### è‡ªå‹•ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†æ

```python
# Databricks Jobs ã§ã®å®šæœŸå®Ÿè¡Œè¨­å®šä¾‹
def scheduled_analysis():
    """å®šæœŸå®Ÿè¡Œç”¨ã®åˆ†æé–¢æ•°"""
    # æœ€æ–°ã®ã‚¯ã‚¨ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    latest_profiles = spark.sql("""
        SELECT query_id, profile_json
        FROM system.query.history 
        WHERE query_start_time >= current_timestamp() - INTERVAL 1 HOUR
        AND total_time_ms > 30000  -- 30ç§’ä»¥ä¸Šã®ã‚¯ã‚¨ãƒªã®ã¿
        ORDER BY total_time_ms DESC
        LIMIT 5
    """)
    
    for row in latest_profiles.collect():
        profile_data = json.loads(row.profile_json)
        metrics = extract_performance_metrics(profile_data)
        analysis = analyze_bottlenecks_with_claude(metrics)
        
        # Slacké€šçŸ¥ã‚„ãƒ¡ãƒ¼ãƒ«é€ä¿¡ãªã©ã®å‡¦ç†
        send_alert_if_bottleneck_detected(metrics, analysis)

# ä½¿ç”¨ä¾‹ï¼ˆJobsã§å®šæœŸå®Ÿè¡Œï¼‰
scheduled_analysis()
```

## ğŸ“ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒ»æ³¨æ„äº‹é …

- ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ã‚µãƒ³ãƒ—ãƒ«ãƒ»æ•™è‚²ç›®çš„ã§ã®ä½¿ç”¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™
- ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã®ä½¿ç”¨å‰ã«ã¯ååˆ†ãªãƒ†ã‚¹ãƒˆã‚’è¡Œã£ã¦ãã ã•ã„
- Databricks Claude 3.7 Sonnetã®åˆ©ç”¨ã«ã¯é©åˆ‡ãªãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¨æ¨©é™ãŒå¿…è¦ã§ã™
- å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚„ã‚¯ã‚¨ãƒªã®åˆ†æã«ã¯å®Ÿè¡Œæ™‚é–“ã¨ã‚³ã‚¹ãƒˆã«æ³¨æ„ã—ã¦ãã ã•ã„
- æ©Ÿå¯†æ€§ã®é«˜ã„ã‚¯ã‚¨ãƒªãƒ­ã‚°ã®å–ã‚Šæ‰±ã„ã«ã¯ååˆ†æ³¨æ„ã—ã¦ãã ã•ã„

## ğŸ“ ã‚µãƒãƒ¼ãƒˆãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯

å•é¡Œã‚„æ”¹å–„ææ¡ˆãŒã‚ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®è¦³ç‚¹ã§æƒ…å ±ã‚’æ•´ç†ã—ã¦ãã ã•ã„ï¼š

1. **Databricksç’°å¢ƒã®è©³ç´°**
   - Runtime version
   - Cluster configuration
   - Spark version

2. **ã‚¨ãƒ©ãƒ¼æƒ…å ±**
   - ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å…¨æ–‡
   - å®Ÿè¡Œæ™‚ã®ãƒ­ã‚°
   - ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ

3. **ä½¿ç”¨çŠ¶æ³**
   - ä½¿ç”¨ã—ãŸJSONãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚ºã¨æ§‹é€ æ¦‚è¦
   - æœŸå¾…ã™ã‚‹å‹•ä½œã¨å®Ÿéš›ã®å‹•ä½œã®å·®ç•°
   - ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå†…å®¹

4. **ç’°å¢ƒè¨­å®š**
   - Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¨­å®šçŠ¶æ³
   - Personal Access Tokenã®æ¨©é™
   - ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ–¹æ³•

ã“ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦SQLã‚¯ã‚¨ãƒªã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã«å½¹ç«‹ã¦ã¦ã„ãŸã ã‘ã‚Œã°å¹¸ã„ã§ã™ã€‚
