# Databricks notebook source
# MAGIC %md
# MAGIC # Databricks SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆ†æãƒ„ãƒ¼ãƒ«
# MAGIC
# MAGIC ã“ã®notebookã¯ã€Databricksã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šã¨æ”¹å–„æ¡ˆã®æç¤ºã«å¿…è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã¦åˆ†æã‚’è¡Œã„ã¾ã™ã€‚
# MAGIC
# MAGIC ## æ©Ÿèƒ½æ¦‚è¦
# MAGIC
# MAGIC 1. **SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿**
# MAGIC    - Databricksã§å‡ºåŠ›ã•ã‚ŒãŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ­ã‚°ã®è§£æ
# MAGIC    - `graphs`ã‚­ãƒ¼ã«æ ¼ç´ã•ã‚ŒãŸå®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º
# MAGIC
# MAGIC 2. **é‡è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º**
# MAGIC    - ã‚¯ã‚¨ãƒªåŸºæœ¬æƒ…å ±ï¼ˆIDã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã€å®Ÿè¡Œæ™‚é–“ãªã©ï¼‰
# MAGIC    - å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆå®Ÿè¡Œæ™‚é–“ã€ãƒ‡ãƒ¼ã‚¿é‡ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ãªã©ï¼‰
# MAGIC    - ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ»ãƒãƒ¼ãƒ‰è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹
# MAGIC    - ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è¨ˆç®—
# MAGIC
# MAGIC 3. **AI ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ**
# MAGIC    - è¨­å®šå¯èƒ½ãªLLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ (Databricks, OpenAI, Azure OpenAI, Anthropic)
# MAGIC    - æŠ½å‡ºãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
# MAGIC    - å…·ä½“çš„ãªæ”¹å–„æ¡ˆã®æç¤º
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC **äº‹å‰æº–å‚™:**
# MAGIC - LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¨­å®šï¼ˆDatabricks Model Serving ã¾ãŸã¯ å¤–éƒ¨APIï¼‰
# MAGIC - å¿…è¦ãªAPIã‚­ãƒ¼ã®è¨­å®š
# MAGIC - SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®æº–å‚™ï¼ˆDBFS ã¾ãŸã¯ FileStoreï¼‰
# MAGIC
# MAGIC ---

# COMMAND ----------

# MAGIC %md
# MAGIC # ğŸ”§ è¨­å®šãƒ»æº–å‚™ã‚»ã‚¯ã‚·ãƒ§ãƒ³
# MAGIC
# MAGIC **ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ãƒ„ãƒ¼ãƒ«ã®åŸºæœ¬è¨­å®šã‚’è¡Œã„ã¾ã™**
# MAGIC
# MAGIC ğŸ“‹ **è¨­å®šå†…å®¹:**
# MAGIC - åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®æŒ‡å®š
# MAGIC - LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¨­å®š
# MAGIC - åˆ†æé–¢æ•°ã®å®šç¾©
# MAGIC
# MAGIC âš ï¸ **é‡è¦:** ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹å‰ã«ã€ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã™ã¹ã¦ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“ åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
# MAGIC
# MAGIC **æœ€åˆã«ã€åˆ†æå¯¾è±¡ã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚**
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®è¨­å®šã‚’è¡Œã„ã¾ã™ï¼š
# MAGIC - ğŸ“‚ SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹è¨­å®š
# MAGIC - ğŸ“‹ å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å½¢å¼ã®ä¾‹
# MAGIC - âš™ï¸ åŸºæœ¬çš„ãªç’°å¢ƒè¨­å®š

# COMMAND ----------

# ğŸ“ SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹è¨­å®š
# 
# ä»¥ä¸‹ã®JSON_FILE_PATHã‚’å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›´ã—ã¦ãã ã•ã„ï¼š

JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/POC1.json'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

# ğŸŒ å‡ºåŠ›è¨€èªè¨­å®šï¼ˆOUTPUT_LANGUAGE: 'ja' = æ—¥æœ¬èª, 'en' = è‹±èªï¼‰
OUTPUT_LANGUAGE = 'ja'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æ—¥æœ¬èª

# ğŸ’¡ ä½¿ç”¨ä¾‹:
# OUTPUT_LANGUAGE = 'ja'  # æ—¥æœ¬èªã§ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
# OUTPUT_LANGUAGE = 'en'  # è‹±èªã§ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›

# ğŸŒ å¤šè¨€èªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¾æ›¸
MESSAGES = {
    'ja': {
        'bottleneck_title': 'Databricks SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ',
        'query_id': 'ã‚¯ã‚¨ãƒªID',
        'analysis_time': 'åˆ†ææ—¥æ™‚',
        'execution_time': 'å®Ÿè¡Œæ™‚é–“',
        'sql_optimization_report': 'SQLæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ',
        'optimization_time': 'æœ€é©åŒ–æ—¥æ™‚',
        'original_file': 'ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ•ã‚¡ã‚¤ãƒ«',
        'optimized_file': 'æœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ«',
        'optimization_analysis': 'æœ€é©åŒ–åˆ†æçµæœ',
        'performance_metrics': 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹å‚è€ƒæƒ…å ±',
        'read_data': 'èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿',
        'spill': 'ã‚¹ãƒ”ãƒ«',
        'top10_processes': 'æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10'
    },
    'en': {
        'bottleneck_title': 'Databricks SQL Profiler Bottleneck Analysis Results',
        'query_id': 'Query ID',
        'analysis_time': 'Analysis Time',
        'execution_time': 'Execution Time',
        'sql_optimization_report': 'SQL Optimization Report',
        'optimization_time': 'Optimization Time',
        'original_file': 'Original File',
        'optimized_file': 'Optimized File',
        'optimization_analysis': 'Optimization Analysis Results',
        'performance_metrics': 'Performance Metrics Reference',
        'read_data': 'Data Read',
        'spill': 'Spill',
        'top10_processes': 'Top 10 Most Time-Consuming Processes'
    }
}

def get_message(key: str) -> str:
    """å¤šè¨€èªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—"""
    return MESSAGES.get(OUTPUT_LANGUAGE, MESSAGES['ja']).get(key, key)

# ğŸ“‹ å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å½¢å¼ã®ä¾‹:
# Unity Catalog Volumes:
# JSON_FILE_PATH = '/Volumes/catalog/schema/volume/profiler.json'
# 
# FileStore (æ¨å¥¨):
# JSON_FILE_PATH = '/FileStore/shared_uploads/your_username/profiler_log.json'
# 
# DBFS:
# JSON_FILE_PATH = '/dbfs/FileStore/shared_uploads/your_username/profiler_log.json'
# 
# DBFS URI:
# JSON_FILE_PATH = 'dbfs:/FileStore/shared_uploads/your_username/profiler_log.json'

print("ğŸ“ ã€åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šå®Œäº†ã€‘")
print("=" * 50)
print(f"ğŸ“„ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {JSON_FILE_PATH}")
print("=" * 50)

# âš™ï¸ åŸºæœ¬çš„ãªç’°å¢ƒè¨­å®š
import json
try:
    import pandas as pd
except ImportError:
    print("Warning: pandas is not installed, some features may not work")
    pd = None
from typing import Dict, List, Any
from datetime import datetime

print("âœ… åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†")
print("ğŸš€ æ¬¡ã®ã‚»ãƒ«ã«é€²ã‚“ã§ãã ã•ã„")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¤– LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®š
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®è¨­å®šã‚’è¡Œã„ã¾ã™ï¼š
# MAGIC - LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®é¸æŠï¼ˆDatabricks/OpenAI/Azure/Anthropicï¼‰
# MAGIC - å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æ¥ç¶šè¨­å®š
# MAGIC - å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# COMMAND ----------

# ğŸ¤– LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®š
LLM_CONFIG = {
    # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚¿ã‚¤ãƒ—: 'databricks', 'openai', 'azure_openai', 'anthropic'
    "provider": "databricks",
    
    # Databricks Model Servingè¨­å®šï¼ˆé«˜é€Ÿå®Ÿè¡Œå„ªå…ˆï¼‰
    "databricks": {
        "endpoint_name": "databricks-claude-3-7-sonnet",  # Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå
        "max_tokens": 131072,  # 128K tokensï¼ˆClaude 3.7 Sonnetã®æœ€å¤§åˆ¶é™ï¼‰
        "temperature": 0.0,    # æ±ºå®šçš„ãªå‡ºåŠ›ã®ãŸã‚ï¼ˆ0.1â†’0.0ï¼‰
        "thinking_enabled": False,  # æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç„¡åŠ¹ - é«˜é€Ÿå®Ÿè¡Œå„ªå…ˆï¼‰
        "thinking_budget_tokens": 65536  # æ€è€ƒç”¨ãƒˆãƒ¼ã‚¯ãƒ³äºˆç®— 64K tokensï¼ˆæœ‰åŠ¹æ™‚ã®ã¿ä½¿ç”¨ï¼‰
    },
    
    # OpenAIè¨­å®šï¼ˆå®Œå…¨ãªSQLç”Ÿæˆç”¨ã«æœ€é©åŒ–ï¼‰
    "openai": {
        "api_key": "",  # OpenAI APIã‚­ãƒ¼ (ç’°å¢ƒå¤‰æ•°OPENAI_API_KEYã§ã‚‚å¯)
        "model": "gpt-4o",  # gpt-4o, gpt-4-turbo, gpt-3.5-turbo
        "max_tokens": 16000,  # OpenAIã®åˆ¶é™å†…æœ€å¤§
        "temperature": 0.0    # æ±ºå®šçš„ãªå‡ºåŠ›ã®ãŸã‚ï¼ˆ0.1â†’0.0ï¼‰
    },
    
    # Azure OpenAIè¨­å®šï¼ˆå®Œå…¨ãªSQLç”Ÿæˆç”¨ã«æœ€é©åŒ–ï¼‰
    "azure_openai": {
        "api_key": "",  # Azure OpenAI APIã‚­ãƒ¼ (ç’°å¢ƒå¤‰æ•°AZURE_OPENAI_API_KEYã§ã‚‚å¯)
        "endpoint": "",  # https://your-resource.openai.azure.com/
        "deployment_name": "",  # ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå
        "api_version": "2024-02-01",
        "max_tokens": 16000,  # Azure OpenAIã®åˆ¶é™å†…æœ€å¤§
        "temperature": 0.0    # æ±ºå®šçš„ãªå‡ºåŠ›ã®ãŸã‚ï¼ˆ0.1â†’0.0ï¼‰
    },
    
    # Anthropicè¨­å®šï¼ˆå®Œå…¨ãªSQLç”Ÿæˆç”¨ã«æœ€é©åŒ–ï¼‰
    "anthropic": {
        "api_key": "",  # Anthropic APIã‚­ãƒ¼ (ç’°å¢ƒå¤‰æ•°ANTHROPIC_API_KEYã§ã‚‚å¯)
        "model": "claude-3-5-sonnet-20241022",  # claude-3-5-sonnet-20241022, claude-3-opus-20240229
        "max_tokens": 16000,  # Anthropicã®åˆ¶é™å†…æœ€å¤§
        "temperature": 0.0    # æ±ºå®šçš„ãªå‡ºåŠ›ã®ãŸã‚ï¼ˆ0.1â†’0.0ï¼‰
    }
}

print("ğŸ¤– LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®šå®Œäº†")
print(f"ğŸ¤– LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {LLM_CONFIG['provider']}")

if LLM_CONFIG['provider'] == 'databricks':
    print(f"ğŸ”— Databricksã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: {LLM_CONFIG['databricks']['endpoint_name']}")
    thinking_status = "æœ‰åŠ¹" if LLM_CONFIG['databricks'].get('thinking_enabled', False) else "ç„¡åŠ¹"
    thinking_budget = LLM_CONFIG['databricks'].get('thinking_budget_tokens', 65536)
    max_tokens = LLM_CONFIG['databricks'].get('max_tokens', 131072)
    print(f"ğŸ§  æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰: {thinking_status} (äºˆç®—: {thinking_budget:,} tokens)")
    print(f"ğŸ“Š æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {max_tokens:,} tokens ({max_tokens//1024}K)")
    if not LLM_CONFIG['databricks'].get('thinking_enabled', False):
        print("âš¡ é«˜é€Ÿå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’çœç•¥ã—ã¦è¿…é€Ÿãªçµæœç”Ÿæˆ")
elif LLM_CONFIG['provider'] == 'openai':
    print(f"ğŸ”— OpenAIãƒ¢ãƒ‡ãƒ«: {LLM_CONFIG['openai']['model']}")
elif LLM_CONFIG['provider'] == 'azure_openai':
    print(f"ğŸ”— Azure OpenAIãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ: {LLM_CONFIG['azure_openai']['deployment_name']}")
elif LLM_CONFIG['provider'] == 'anthropic':
    print(f"ğŸ”— Anthropicãƒ¢ãƒ‡ãƒ«: {LLM_CONFIG['anthropic']['model']}")

print()
print("ğŸ’¡ LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ‡ã‚Šæ›¿ãˆä¾‹:")
print('   LLM_CONFIG["provider"] = "openai"      # OpenAI GPT-4ã«åˆ‡ã‚Šæ›¿ãˆ')
print('   LLM_CONFIG["provider"] = "anthropic"   # Anthropic Claudeã«åˆ‡ã‚Šæ›¿ãˆ')
print('   LLM_CONFIG["provider"] = "azure_openai" # Azure OpenAIã«åˆ‡ã‚Šæ›¿ãˆ')
print()
print("ğŸ§  Databricksæ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰è¨­å®šä¾‹:")
print('   LLM_CONFIG["databricks"]["thinking_enabled"] = False  # æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰ç„¡åŠ¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ»é«˜é€Ÿå®Ÿè¡Œï¼‰')
print('   LLM_CONFIG["databricks"]["thinking_enabled"] = True   # æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹ï¼ˆè©³ç´°åˆ†ææ™‚ã®ã¿ï¼‰')
print('   LLM_CONFIG["databricks"]["thinking_budget_tokens"] = 65536  # æ€è€ƒç”¨ãƒˆãƒ¼ã‚¯ãƒ³äºˆç®—(64K)')
print('   LLM_CONFIG["databricks"]["max_tokens"] = 131072  # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°(128K)')
print()

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import requests
except ImportError:
    print("Warning: requests is not installed, some features may not work")
    requests = None
import os
try:
    from pyspark.sql import SparkSession
except ImportError:
    print("Warning: pyspark is not installed")
    SparkSession = None
    print("âœ… Spark Version: Not available")

# Databricks Runtimeæƒ…å ±ã‚’å®‰å…¨ã«å–å¾—
try:
    if spark is not None:
        runtime_version = spark.conf.get('spark.databricks.clusterUsageTags.sparkVersion')
    print(f"âœ… Databricks Runtime: {runtime_version}")
except Exception:
    try:
        # ä»£æ›¿æ‰‹æ®µã§DBRæƒ…å ±ã‚’å–å¾—
        dbr_version = spark.conf.get('spark.databricks.clusterUsageTags.clusterName', 'Unknown')
        print(f"âœ… Databricks Cluster: {dbr_version}")
    except Exception:
        print("âœ… Databricks Environment: è¨­å®šæƒ…å ±ã®å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“‚ SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
# MAGIC - DBFS/FileStore/ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®è‡ªå‹•åˆ¤åˆ¥
# MAGIC - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã®è¡¨ç¤º

# COMMAND ----------

def load_profiler_json(file_path: str) -> Dict[str, Any]:
    """
    SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        file_path: JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆDBFS ã¾ãŸã¯ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ï¼‰
        
    Returns:
        Dict: ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸJSONãƒ‡ãƒ¼ã‚¿
    """
    try:
        # DBFSãƒ‘ã‚¹ã®å ´åˆã¯é©åˆ‡ã«å‡¦ç†
        if file_path.startswith('/dbfs/'):
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        elif file_path.startswith('dbfs:/'):
            # dbfs: ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’/dbfs/ã«å¤‰æ›
            local_path = file_path.replace('dbfs:', '/dbfs')
            with open(local_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        elif file_path.startswith('/FileStore/'):
            # FileStore ãƒ‘ã‚¹ã‚’ /dbfs/FileStore/ ã«å¤‰æ›
            local_path = '/dbfs' + file_path
            with open(local_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        
        print(f"âœ… JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {file_path}")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(str(data)):,} characters")
        return data
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {}

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: load_profiler_json")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºé–¢æ•°
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º
# MAGIC - ã‚¯ã‚¨ãƒªåŸºæœ¬æƒ…å ±ã®å–å¾—
# MAGIC - å…¨ä½“/ã‚¹ãƒ†ãƒ¼ã‚¸/ãƒãƒ¼ãƒ‰åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®è¨ˆç®—
# MAGIC - ã‚¹ãƒ”ãƒ«æ¤œå‡ºã¨ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®åˆ†æ

# COMMAND ----------

def detect_data_format(profiler_data: Dict[str, Any]) -> str:
    """
    JSONãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ã‚’æ¤œå‡º
    """
    # SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼å½¢å¼ã®æ¤œå‡º
    if 'graphs' in profiler_data and isinstance(profiler_data['graphs'], list):
        if len(profiler_data['graphs']) > 0:
            return 'sql_profiler'
    
    # SQLã‚¯ã‚¨ãƒªã‚µãƒãƒªãƒ¼å½¢å¼ã®æ¤œå‡ºï¼ˆtest2.jsonå½¢å¼ï¼‰
    if 'query' in profiler_data and 'planMetadatas' in profiler_data:
        query_data = profiler_data.get('query', {})
        if 'metrics' in query_data:
            return 'sql_query_summary'
    
    return 'unknown'

def extract_performance_metrics_from_query_summary(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Databricks SQLã‚¯ã‚¨ãƒªã‚µãƒãƒªãƒ¼å½¢å¼ã®JSONã‹ã‚‰åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
    (test2.jsonå½¢å¼ã«å¯¾å¿œ)
    """
    try:
        query_data = profiler_data.get('query', {})
        metrics_data = query_data.get('metrics', {})
        
        if not metrics_data:
            print("âš ï¸ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {}
        
        print(f"âœ… SQLã‚¯ã‚¨ãƒªã‚µãƒãƒªãƒ¼å½¢å¼ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
        print(f"   - å®Ÿè¡Œæ™‚é–“: {metrics_data.get('totalTimeMs', 0):,} ms")
        print(f"   - èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿: {metrics_data.get('readBytes', 0) / 1024 / 1024 / 1024:.2f} GB")
        print(f"   - å‡¦ç†è¡Œæ•°: {metrics_data.get('rowsReadCount', 0):,} è¡Œ")
        
        # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º
        overall_metrics = {
            'total_time_ms': metrics_data.get('totalTimeMs', 0),
            'execution_time_ms': metrics_data.get('executionTimeMs', 0),
            'compilation_time_ms': metrics_data.get('compilationTimeMs', 0),
            'read_bytes': metrics_data.get('readBytes', 0),
            'read_remote_bytes': metrics_data.get('readRemoteBytes', 0),
            'read_cache_bytes': metrics_data.get('readCacheBytes', 0),
            'spill_to_disk_bytes': metrics_data.get('spillToDiskBytes', 0),
            'rows_produced_count': metrics_data.get('rowsProducedCount', 0),
            'rows_read_count': metrics_data.get('rowsReadCount', 0),
            'read_files_count': metrics_data.get('readFilesCount', 0),
            'read_partitions_count': metrics_data.get('readPartitionsCount', 0),
            'photon_total_time_ms': metrics_data.get('photonTotalTimeMs', 0),
            'task_total_time_ms': metrics_data.get('taskTotalTimeMs', 0),
            'network_sent_bytes': metrics_data.get('networkSentBytes', 0),
            'photon_enabled': metrics_data.get('photonTotalTimeMs', 0) > 0,
            'photon_utilization_ratio': 0
        }
        
        # Photonåˆ©ç”¨ç‡ã®è¨ˆç®—
        if overall_metrics['task_total_time_ms'] > 0:
            overall_metrics['photon_utilization_ratio'] = min(
                overall_metrics['photon_total_time_ms'] / overall_metrics['task_total_time_ms'], 1.0
            )
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã®è¨ˆç®—
        cache_hit_ratio = 0
        if overall_metrics['read_bytes'] > 0:
            cache_hit_ratio = overall_metrics['read_cache_bytes'] / overall_metrics['read_bytes']
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è¨ˆç®—
        bottleneck_indicators = {
            'spill_bytes': overall_metrics['spill_to_disk_bytes'],
            'has_spill': overall_metrics['spill_to_disk_bytes'] > 0,
            'cache_hit_ratio': cache_hit_ratio,
            'has_cache_miss': cache_hit_ratio < 0.8,
            'photon_efficiency': overall_metrics['photon_utilization_ratio'],
            'has_shuffle_bottleneck': False,  # è©³ç´°æƒ…å ±ãŒãªã„ãŸã‚åˆ¤å®šä¸å¯
            'remote_read_ratio': 0,
            'has_memory_pressure': overall_metrics['spill_to_disk_bytes'] > 0,
            'max_task_duration_ratio': 1.0,  # ä¸æ˜
            'has_data_skew': False  # è©³ç´°æƒ…å ±ãŒãªã„ãŸã‚åˆ¤å®šä¸å¯
        }
        
        # ãƒªãƒ¢ãƒ¼ãƒˆèª­ã¿è¾¼ã¿æ¯”ç‡ã®è¨ˆç®—
        if overall_metrics['read_bytes'] > 0:
            bottleneck_indicators['remote_read_ratio'] = overall_metrics['read_remote_bytes'] / overall_metrics['read_bytes']
        
        # ã‚¯ã‚¨ãƒªæƒ…å ±ã®æŠ½å‡º
        query_info = {
            'query_id': query_data.get('id', ''),
            'query_text': query_data.get('queryText', '')[:300] + "..." if len(query_data.get('queryText', '')) > 300 else query_data.get('queryText', ''),
            'status': query_data.get('status', ''),
            'query_start_time': query_data.get('queryStartTimeMs', 0),
            'query_end_time': query_data.get('queryEndTimeMs', 0),
            'spark_ui_url': query_data.get('sparkUiUrl', ''),
            'endpoint_id': query_data.get('endpointId', ''),
            'user': query_data.get('user', {}).get('displayName', ''),
            'statement_type': query_data.get('statementType', ''),
            'plans_state': query_data.get('plansState', '')
        }
        
        # è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ´å¯Ÿã‚’è¨ˆç®—
        performance_insights = calculate_performance_insights_from_metrics(overall_metrics)
        
        # æ“¬ä¼¼çš„ãªãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆã‚µãƒãƒªãƒ¼æƒ…å ±ã‹ã‚‰ç”Ÿæˆï¼‰
        summary_node = {
            'node_id': 'summary_node',
            'name': f'Query Execution Summary ({query_data.get("statementType", "SQL")})',
            'tag': 'QUERY_SUMMARY',
            'key_metrics': {
                'durationMs': overall_metrics['total_time_ms'],
                'rowsNum': overall_metrics['rows_read_count'],
                'peakMemoryBytes': 0,  # ä¸æ˜
                'throughputMBps': performance_insights['parallelization']['throughput_mb_per_second'],
                'dataSelectivity': performance_insights['data_efficiency']['data_selectivity'],
                'cacheHitRatio': performance_insights['cache_efficiency']['cache_hit_ratio']
            },
            'detailed_metrics': {
                'Total Time': {'value': overall_metrics['total_time_ms'], 'display_name': 'Total Time'},
                'Read Bytes': {'value': overall_metrics['read_bytes'], 'display_name': 'Read Bytes'},
                'Spill Bytes': {'value': overall_metrics['spill_to_disk_bytes'], 'display_name': 'Spill to Disk'},
                'Photon Time': {'value': overall_metrics['photon_total_time_ms'], 'display_name': 'Photon Time'},
                'Rows Read': {'value': overall_metrics['rows_read_count'], 'display_name': 'Rows Read Count'},
                'Cache Hit Ratio': {'value': performance_insights['cache_efficiency']['cache_hit_ratio'], 'display_name': 'Cache Hit Ratio'},
                'Data Selectivity': {'value': performance_insights['data_efficiency']['data_selectivity'], 'display_name': 'Data Selectivity'},
                'Throughput': {'value': performance_insights['parallelization']['throughput_mb_per_second'], 'display_name': 'Throughput (MB/s)'}
            },
            'graph_index': 0,
            'performance_insights': performance_insights
        }
        
        return {
            'data_format': 'sql_query_summary',
            'query_info': query_info,
            'overall_metrics': overall_metrics,
            'bottleneck_indicators': bottleneck_indicators,
            'node_metrics': [summary_node],
            'stage_metrics': [],  # è©³ç´°ã‚¹ãƒ†ãƒ¼ã‚¸æƒ…å ±ãªã—
            'liquid_clustering_analysis': {},  # å¾Œã§è¿½åŠ 
            'raw_profiler_data': profiler_data,
            'performance_insights': performance_insights,  # è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ´å¯Ÿã‚’è¿½åŠ 
            'analysis_capabilities': [
                'ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ã€ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§ã€PhotonåŠ¹ç‡ï¼‰',
                'ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³åˆ†æï¼ˆã‚¹ãƒ”ãƒ«ã€ä¸¦åˆ—åŒ–åŠ¹ç‡ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼‰',
                'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™è¨ˆç®—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åŠ¹ç‡ã€ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åŠ¹ç‡ï¼‰',
                'ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ï¼‰'
            ],
            'analysis_limitations': [
                'è©³ç´°ãªå®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ï¼ˆãƒãƒ¼ãƒ‰ã€ã‚¨ãƒƒã‚¸ï¼‰ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“',
                'ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“', 
                'BROADCASTåˆ†æã¯åŸºæœ¬çš„ãªæ¨å®šã®ã¿å¯èƒ½',
                'Liquid Clusteringåˆ†æã¯ä¸€èˆ¬çš„ãªæ¨å¥¨ã®ã¿å¯èƒ½',
                'ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã¯å¹³å‡å€¤ãƒ™ãƒ¼ã‚¹ã®æ¨å®šã®ã¿',
                'ã‚¯ã‚¨ãƒªæ§‹é€ ã®è©³ç´°è§£æã¯è¡Œã„ã¾ã›ã‚“ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹é‡è¦–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰'
            ]
        }
        
    except Exception as e:
        print(f"âš ï¸ SQLã‚¯ã‚¨ãƒªã‚µãƒãƒªãƒ¼å½¢å¼ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {}

def extract_performance_metrics(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã«å¿…è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°å½¢å¼å¯¾å¿œï¼‰
    """
    # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’æ¤œå‡º
    data_format = detect_data_format(profiler_data)
    
    print(f"ğŸ” æ¤œå‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å½¢å¼: {data_format}")
    
    if data_format == 'sql_query_summary':
        print("ğŸ“Š Databricks SQLã‚¯ã‚¨ãƒªã‚µãƒãƒªãƒ¼å½¢å¼ã¨ã—ã¦å‡¦ç†ä¸­...")
        result = extract_performance_metrics_from_query_summary(profiler_data)
        if result:
            # Liquid Clusteringåˆ†æã‚’è¿½åŠ ï¼ˆåˆ¶é™ä»˜ãï¼‰
            try:
                result["liquid_clustering_analysis"] = analyze_liquid_clustering_opportunities(profiler_data, result)
            except Exception as e:
                print(f"âš ï¸ Liquid Clusteringåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—: {str(e)}")
                result["liquid_clustering_analysis"] = {}
        return result
    elif data_format == 'sql_profiler':
        print("ğŸ“Š SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼è©³ç´°å½¢å¼ã¨ã—ã¦å‡¦ç†ä¸­...")
        # æ—¢å­˜ã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼å½¢å¼ã®å‡¦ç†ã‚’ç¶™ç¶š
        pass
    else:
        print(f"âš ï¸ æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã§ã™: {data_format}")
        return {}
    
    # æ—¢å­˜ã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼å½¢å¼ã®å‡¦ç†
    metrics = {
        "query_info": {},
        "overall_metrics": {},
        "stage_metrics": [],
        "node_metrics": [],
        "bottleneck_indicators": {},
        "liquid_clustering_analysis": {},
        "raw_profiler_data": profiler_data  # ãƒ—ãƒ©ãƒ³åˆ†æã®ãŸã‚ã«ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    }
    
    # åŸºæœ¬çš„ãªã‚¯ã‚¨ãƒªæƒ…å ±
    if 'query' in profiler_data:
        query = profiler_data['query']
        metrics["query_info"] = {
            "query_id": query.get('id', ''),
            "status": query.get('status', ''),
            "query_start_time": query.get('queryStartTimeMs', 0),
            "query_end_time": query.get('queryEndTimeMs', 0),
            "user": query.get('user', {}).get('displayName', ''),
            "query_text": query.get('queryText', '')[:300] + "..." if len(query.get('queryText', '')) > 300 else query.get('queryText', '')
        }
        
        # å…¨ä½“çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if 'metrics' in query:
            query_metrics = query['metrics']
            metrics["overall_metrics"] = {
                "total_time_ms": query_metrics.get('totalTimeMs', 0),
                "compilation_time_ms": query_metrics.get('compilationTimeMs', 0),
                "execution_time_ms": query_metrics.get('executionTimeMs', 0),
                "read_bytes": query_metrics.get('readBytes', 0),
                "read_remote_bytes": query_metrics.get('readRemoteBytes', 0),
                "read_cache_bytes": query_metrics.get('readCacheBytes', 0),
                "rows_produced_count": query_metrics.get('rowsProducedCount', 0),
                "rows_read_count": query_metrics.get('rowsReadCount', 0),
                "spill_to_disk_bytes": query_metrics.get('spillToDiskBytes', 0),
                "read_files_count": query_metrics.get('readFilesCount', 0),
                "task_total_time_ms": query_metrics.get('taskTotalTimeMs', 0),
                "photon_total_time_ms": query_metrics.get('photonTotalTimeMs', 0),
                # Photonåˆ©ç”¨çŠ¶æ³ã®åˆ†æï¼ˆPhotonå®Ÿè¡Œæ™‚é–“/ã‚¿ã‚¹ã‚¯åˆè¨ˆæ™‚é–“ï¼‰
                "photon_enabled": query_metrics.get('photonTotalTimeMs', 0) > 0,
                "photon_utilization_ratio": min(query_metrics.get('photonTotalTimeMs', 0) / max(query_metrics.get('taskTotalTimeMs', 1), 1), 1.0)
            }
    
    # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¹ãƒ†ãƒ¼ã‚¸ã¨ãƒãƒ¼ãƒ‰ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ã‚°ãƒ©ãƒ•å¯¾å¿œï¼‰
    if 'graphs' in profiler_data and profiler_data['graphs']:
        # ã™ã¹ã¦ã®ã‚°ãƒ©ãƒ•ã‚’åˆ†æ
        for graph_index, graph in enumerate(profiler_data['graphs']):
            print(f"ğŸ” ã‚°ãƒ©ãƒ•{graph_index}ã‚’åˆ†æä¸­...")
            
            # ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿
            if 'stageData' in graph:
                for stage in graph['stageData']:
                    stage_metric = {
                        "stage_id": stage.get('stageId', ''),
                        "status": stage.get('status', ''),
                        "duration_ms": stage.get('keyMetrics', {}).get('durationMs', 0),
                        "num_tasks": stage.get('numTasks', 0),
                        "num_failed_tasks": stage.get('numFailedTasks', 0),
                        "num_complete_tasks": stage.get('numCompleteTasks', 0),
                        "start_time_ms": stage.get('startTimeMs', 0),
                        "end_time_ms": stage.get('endTimeMs', 0),
                        "graph_index": graph_index  # ã©ã®ã‚°ãƒ©ãƒ•ç”±æ¥ã‹ã‚’è¨˜éŒ²
                    }
                    metrics["stage_metrics"].append(stage_metric)
            
            # ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ï¼ˆé‡è¦ãªã‚‚ã®ã®ã¿ï¼‰
            if 'nodes' in graph:
                for node in graph['nodes']:
                    if not node.get('hidden', False):
                        node_metric = {
                            "node_id": node.get('id', ''),
                            "name": node.get('name', ''),
                            "tag": node.get('tag', ''),
                            "key_metrics": node.get('keyMetrics', {}),
                            "metrics": node.get('metrics', []),  # å…ƒã®metricsé…åˆ—ã‚’ä¿æŒ
                            "graph_index": graph_index  # ã©ã®ã‚°ãƒ©ãƒ•ç”±æ¥ã‹ã‚’è¨˜éŒ²
                        }
                        
                        # é‡è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã¿è©³ç´°æŠ½å‡ºï¼ˆã‚¹ãƒ”ãƒ«é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿½åŠ ãƒ»labelå¯¾å¿œï¼‰
                        detailed_metrics = {}
                        for metric in node.get('metrics', []):
                            metric_key = metric.get('key', '')
                            metric_label = metric.get('label', '')
                            
                            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’keyã¨labelã®ä¸¡æ–¹ã§ç¢ºèª
                            key_keywords = ['TIME', 'MEMORY', 'ROWS', 'BYTES', 'DURATION', 'PEAK', 'CUMULATIVE', 'EXCLUSIVE', 
                                           'SPILL', 'DISK', 'PRESSURE', 'SINK']
                            
                            # metric_keyã¾ãŸã¯metric_labelã«é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹å ´åˆã«æŠ½å‡º
                            is_important_metric = (
                                any(keyword in metric_key.upper() for keyword in key_keywords) or
                                any(keyword in metric_label.upper() for keyword in key_keywords)
                            )
                            
                            if is_important_metric:
                                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã¨ã—ã¦ã€labelãŒæœ‰åŠ¹ãªå ´åˆã¯labelã‚’ä½¿ç”¨ã€ãã†ã§ãªã‘ã‚Œã°keyã‚’ä½¿ç”¨
                                metric_name = metric_label if metric_label and metric_label != 'UNKNOWN_KEY' else metric_key
                                detailed_metrics[metric_name] = {
                                    'value': metric.get('value', 0),
                                    'label': metric_label,
                                    'type': metric.get('metricType', ''),
                                    'original_key': metric_key,  # å…ƒã®ã‚­ãƒ¼åã‚’ä¿å­˜
                                    'display_name': metric_name  # è¡¨ç¤ºç”¨ã®åå‰
                                }
                        node_metric['detailed_metrics'] = detailed_metrics
                        metrics["node_metrics"].append(node_metric)
    
    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è¨ˆç®—
    metrics["bottleneck_indicators"] = calculate_bottleneck_indicators(metrics)
    
    # Liquid Clusteringåˆ†æ
    metrics["liquid_clustering_analysis"] = analyze_liquid_clustering_opportunities(profiler_data, metrics)
    
    return metrics

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: extract_performance_metrics")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ·ï¸ ãƒãƒ¼ãƒ‰åè§£æãƒ»æ”¹å–„é–¢æ•°
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - æ±ç”¨çš„ãªãƒãƒ¼ãƒ‰åï¼ˆWhole Stage Codegenç­‰ï¼‰ã®å…·ä½“åŒ–
# MAGIC - é–¢é€£ãƒãƒ¼ãƒ‰ã®æ¤œç´¢ã¨æœ€é©ãªå‡¦ç†åã®é¸æŠ
# MAGIC - Photonæƒ…å ±ã‚„ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®ä»˜åŠ 
# MAGIC - å‡¦ç†åã®æ„å‘³çš„ãªæ”¹å–„

# COMMAND ----------

def get_meaningful_node_name(node: Dict[str, Any], extracted_metrics: Dict[str, Any]) -> str:
    """
    ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹ãƒãƒ¼ãƒ‰åã‚’å–å¾—ã™ã‚‹é–¢æ•°
    æ±ç”¨çš„ãªåå‰ï¼ˆWhole Stage Codegenãªã©ï¼‰ã‚’å…·ä½“çš„ãªå‡¦ç†åã«å¤‰æ›
    """
    original_name = node.get('name', '')
    node_id = node.get('node_id', node.get('id', ''))
    node_tag = node.get('tag', '')
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—
    metadata = node.get('metadata', [])
    metadata_info = {}
    for meta in metadata:
        key = meta.get('key', '')
        value = meta.get('value', '')
        label = meta.get('label', '')
        if value:
            metadata_info[key] = value
    
    # 1. æ±ç”¨çš„ãªåå‰ã‚’å…·ä½“çš„ãªåå‰ã«ç½®ãæ›ãˆ
    if 'whole stage codegen' in original_name.lower():
        # ã‚ˆã‚Šå…·ä½“çš„ãªå‡¦ç†åã‚’æ¨æ¸¬ã™ã‚‹ãŸã‚ã®ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯
        
        # ãƒãƒ¼ãƒ‰IDãƒ™ãƒ¼ã‚¹ã§ã®é–¢é€£æ€§ã‚’æ¨æ¸¬ï¼ˆéš£æ¥IDï¼‰
        node_id_num = None
        try:
            node_id_num = int(node_id) if node_id else None
        except:
            pass
        
        if node_id_num:
            # åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®è¿‘ã„IDã®å…·ä½“çš„ãªå‡¦ç†ã‚’æ¢ã™
            all_nodes = extracted_metrics.get('node_metrics', [])
            nearby_specific_nodes = []
            
            for other_node in all_nodes:
                other_id = other_node.get('node_id', '')
                other_name = other_node.get('name', '')
                
                try:
                    other_id_num = int(other_id) if other_id else None
                    if other_id_num and abs(other_id_num - node_id_num) <= 10:  # è¿‘éš£10å€‹ä»¥å†…
                        if is_specific_process_name(other_name):
                            nearby_specific_nodes.append(other_name)
                except:
                    continue
            
            # æœ€ã‚‚å…·ä½“çš„ãªå‡¦ç†åã‚’é¸æŠ
            if nearby_specific_nodes:
                specific_name = get_most_specific_process_name_from_list(nearby_specific_nodes)
                if specific_name and specific_name != original_name:
                    return f"{specific_name} (Whole Stage Codegen)"
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: tagã‹ã‚‰ã‚ˆã‚Šå…·ä½“çš„ãªæƒ…å ±ã‚’æŠ½å‡º
        if 'CODEGEN' in node_tag:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­ã‚¿ã‚°æƒ…å ±ã‚’ç¢ºèª
            child_tag = metadata_info.get('CHILD_TAG', '')
            if child_tag and child_tag != 'Child':
                return f"Whole Stage Codegen ({child_tag})"
    
    # 2. ã‚ˆã‚Šå…·ä½“çš„ãªã‚¿ã‚°æƒ…å ±ã‚’ãƒãƒ¼ãƒ‰åã«åæ˜ 
    tag_to_name_mapping = {
        'PHOTON_SHUFFLE_EXCHANGE_SINK_EXEC': 'Photon Shuffle Exchange',
        'PHOTON_GROUPING_AGG_EXEC': 'Photon Grouping Aggregate', 
        'UNKNOWN_DATA_SOURCE_SCAN_EXEC': 'Data Source Scan',
        'HASH_AGGREGATE_EXEC': 'Hash Aggregate',
        'WHOLE_STAGE_CODEGEN_EXEC': 'Whole Stage Codegen'
    }
    
    if node_tag in tag_to_name_mapping:
        mapped_name = tag_to_name_mapping[node_tag]
        if mapped_name != original_name and mapped_name != 'Whole Stage Codegen':
            # ã‚¿ã‚°ã®æ–¹ãŒã‚ˆã‚Šå…·ä½“çš„ãªå ´åˆã¯ä½¿ç”¨
            enhanced_name = mapped_name
        else:
            enhanced_name = original_name
    else:
        enhanced_name = original_name
    
    # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‡¦ç†ã®è©³ç´°ã‚’è¿½åŠ 
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’è¿½åŠ ï¼ˆå¼·åŒ–ç‰ˆï¼‰
    table_name = None
    
    # è¤‡æ•°ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡ºï¼ˆãƒ•ãƒ«ãƒ‘ã‚¹å„ªå…ˆï¼‰
    for key_candidate in ['SCAN_TABLE', 'SCAN_IDENTIFIER', 'TABLE_NAME', 'RELATION', 'SCAN_RELATION']:
        if key_candidate in metadata_info:
            extracted_table = metadata_info[key_candidate]
            # ãƒ•ãƒ«ãƒ‘ã‚¹ï¼ˆcatalog.schema.tableï¼‰ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
            if isinstance(extracted_table, str) and extracted_table.count('.') >= 2:
                table_name = extracted_table
                break
            elif isinstance(extracted_table, str) and extracted_table.count('.') == 1:
                # schema.tableå½¢å¼ã®å ´åˆã‚‚ãã®ã¾ã¾ä½¿ç”¨
                table_name = extracted_table
                break
            elif not table_name:  # ã¾ã ãƒ†ãƒ¼ãƒ–ãƒ«åãŒè¦‹ã¤ã‹ã£ã¦ã„ãªã„å ´åˆã®ã¿è¨­å®š
                table_name = extracted_table
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡ºã§ããªã„å ´åˆã€ãƒãƒ¼ãƒ‰åã‹ã‚‰æ¨æ¸¬
    if not table_name and ('scan' in enhanced_name.lower() or 'data source' in enhanced_name.lower()):
        # ãƒãƒ¼ãƒ‰åã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æ¨æ¸¬
        import re
        
        # "Scan tpcds.tpcds_sf1000_delta_lc.customer" ã®ã‚ˆã†ãªå½¢å¼
        table_patterns = [
            r'[Ss]can\s+([a-zA-Z_][a-zA-Z0-9_.]*[a-zA-Z0-9_])',
            r'[Tt]able\s+([a-zA-Z_][a-zA-Z0-9_.]*[a-zA-Z0-9_])',
            r'([a-zA-Z_][a-zA-Z0-9_]*\.)+([a-zA-Z_][a-zA-Z0-9_]*)',
        ]
        
        for pattern in table_patterns:
            match = re.search(pattern, original_name)
            if match:
                if '.' in match.group(0):
                    # ãƒ•ãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«åï¼ˆcatalog.schema.tableï¼‰ã®å ´åˆã¯ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’ä½¿ç”¨
                    table_name = match.group(0)
                else:
                    table_name = match.group(1) if match.lastindex and match.lastindex >= 1 else match.group(0)
                break
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®valuesãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰ã‚‚ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æ¤œç´¢
    if not table_name:
        for meta in metadata:
            values = meta.get('values', [])
            if values:
                for value in values:
                    if isinstance(value, str) and '.' in value and len(value.split('.')) >= 2:
                        # "catalog.schema.table" å½¢å¼ã®å ´åˆ
                        parts = value.split('.')
                        if len(parts) >= 2 and not any(part.isdigit() for part in parts[-2:]):
                            # ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’ä½¿ç”¨ï¼ˆcatalog.schema.tableï¼‰
                            if len(parts) >= 3:
                                table_name = '.'.join(parts)  # ãƒ•ãƒ«ãƒ‘ã‚¹
                            else:
                                table_name = value  # ãã®ã¾ã¾ä½¿ç”¨
                            break
                if table_name:
                    break
    
    # Data Source Scanã®å ´åˆã«ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’è¡¨ç¤º
    if table_name and ('scan' in enhanced_name.lower() or 'data source' in enhanced_name.lower()):
        # ãƒ•ãƒ«ãƒ‘ã‚¹è¡¨ç¤ºã®ãŸã‚ã«åˆ¶é™ã‚’ç·©å’Œï¼ˆ60æ–‡å­—ã¾ã§ï¼‰
        if len(table_name) > 60:
            # ã‚«ã‚¿ãƒ­ã‚°.ã‚¹ã‚­ãƒ¼ãƒ.ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã®å ´åˆã¯ä¸­é–“ã‚’çœç•¥
            parts = table_name.split('.')
            if len(parts) >= 3:
                table_name = f"{parts[0]}.*.{parts[-1]}"
            else:
                table_name = table_name[:57] + "..."
        enhanced_name = f"Data Source Scan ({table_name})"
    elif 'scan' in enhanced_name.lower() and 'data source' in enhanced_name.lower():
        # ãƒ†ãƒ¼ãƒ–ãƒ«åãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã§ã‚‚ã€ã‚ˆã‚Šæ˜ç¢ºãªåå‰ã«
        enhanced_name = "Data Source Scan"
    
    # Photonæƒ…å ±ã‚’è¿½åŠ 
    if 'IS_PHOTON' in metadata_info and metadata_info['IS_PHOTON'] == 'true':
        if not enhanced_name.startswith('Photon'):
            enhanced_name = f"Photon {enhanced_name}"
    
    return enhanced_name

def find_related_specific_nodes(target_node_id: str, nodes: list, edges: list) -> list:
    """æŒ‡å®šãƒãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹å…·ä½“çš„ãªå‡¦ç†ãƒãƒ¼ãƒ‰ã‚’æ¤œç´¢"""
    
    # ã‚¨ãƒƒã‚¸ã‹ã‚‰é–¢é€£ãƒãƒ¼ãƒ‰ã‚’ç‰¹å®š
    related_node_ids = set()
    
    # ç›´æ¥æ¥ç¶šã•ã‚Œã¦ã„ã‚‹ãƒãƒ¼ãƒ‰
    for edge in edges:
        from_id = edge.get('fromId', '')
        to_id = edge.get('toId', '')
        
        if from_id == target_node_id:
            related_node_ids.add(to_id)
        elif to_id == target_node_id:
            related_node_ids.add(from_id)
    
    # é–¢é€£ãƒãƒ¼ãƒ‰ã®è©³ç´°ã‚’å–å¾—
    related_nodes = []
    for node in nodes:
        node_id = node.get('id', '')
        if node_id in related_node_ids:
            node_name = node.get('name', '')
            # å…·ä½“çš„ãªå‡¦ç†åã‚’æŒã¤ãƒãƒ¼ãƒ‰ã®ã¿é¸æŠ
            if is_specific_process_name(node_name):
                related_nodes.append(node)
    
    return related_nodes

def is_specific_process_name(name: str) -> bool:
    """å…·ä½“çš„ãªå‡¦ç†åã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    specific_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project', 'join',
        'aggregate', 'sort', 'exchange', 'broadcast', 'scan', 'union'
    ]
    
    generic_keywords = [
        'whole stage codegen', 'stage', 'query', 'result'
    ]
    
    name_lower = name.lower()
    
    # å…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€å ´åˆ
    for keyword in specific_keywords:
        if keyword in name_lower:
            return True
    
    # æ±ç”¨çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã®å ´åˆã¯é™¤å¤–
    for keyword in generic_keywords:
        if keyword in name_lower and len(name_lower.split()) <= 3:
            return False
    
    return True

def get_most_specific_process_name(nodes: list) -> str:
    """æœ€ã‚‚å…·ä½“çš„ãªå‡¦ç†åã‚’é¸æŠ"""
    if not nodes:
        return ""
    
    # å„ªå…ˆé †ä½: ã‚ˆã‚Šå…·ä½“çš„ã§æ„å‘³ã®ã‚ã‚‹å‡¦ç†å
    priority_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project',
        'hash join', 'broadcast join', 'sort merge join',
        'hash aggregate', 'sort aggregate', 'grouping aggregate'
    ]
    
    for keyword in priority_keywords:
        for node in nodes:
            node_name = node.get('name', '').lower()
            if keyword in node_name:
                return node.get('name', '')
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®å…·ä½“çš„ãªãƒãƒ¼ãƒ‰å
    for node in nodes:
        node_name = node.get('name', '')
        if is_specific_process_name(node_name):
            return node_name
    
    return ""

def get_most_specific_process_name_from_list(node_names: list) -> str:
    """ãƒãƒ¼ãƒ‰åã®ãƒªã‚¹ãƒˆã‹ã‚‰æœ€ã‚‚å…·ä½“çš„ãªå‡¦ç†åã‚’é¸æŠ"""
    if not node_names:
        return ""
    
    # å„ªå…ˆé †ä½: ã‚ˆã‚Šå…·ä½“çš„ã§æ„å‘³ã®ã‚ã‚‹å‡¦ç†å
    priority_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project',
        'hash join', 'broadcast join', 'sort merge join',
        'hash aggregate', 'sort aggregate', 'grouping aggregate'
    ]
    
    for keyword in priority_keywords:
        for name in node_names:
            if keyword in name.lower():
                return name
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®å…·ä½“çš„ãªãƒãƒ¼ãƒ‰å
    for name in node_names:
        if is_specific_process_name(name):
            return name
    
    return ""

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: get_meaningful_node_name")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¯ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™è¨ˆç®—é–¢æ•°
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - å®Ÿè¡Œæ™‚é–“ã¨ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“ã®æ¯”ç‡åˆ†æ
# MAGIC - ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ã¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹ç‡ã®è¨ˆç®—
# MAGIC - Photonåˆ©ç”¨ç‡ã®åˆ†æ
# MAGIC - ã‚¹ãƒ”ãƒ«æ¤œå‡ºã¨ã‚·ãƒ£ãƒƒãƒ•ãƒ«/ä¸¦åˆ—åº¦ã®å•é¡Œç‰¹å®š

# COMMAND ----------

def calculate_bottleneck_indicators(metrics: Dict[str, Any]) -> Dict[str, Any]:
    """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã‚’è¨ˆç®—"""
    indicators = {}
    
    overall = metrics.get('overall_metrics', {})
    total_time = overall.get('total_time_ms', 0)
    execution_time = overall.get('execution_time_ms', 0)
    compilation_time = overall.get('compilation_time_ms', 0)
    
    if total_time > 0:
        indicators['compilation_ratio'] = compilation_time / total_time
        indicators['execution_ratio'] = execution_time / total_time
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡
    read_bytes = overall.get('read_bytes', 0)
    cache_bytes = overall.get('read_cache_bytes', 0)
    if read_bytes > 0:
        indicators['cache_hit_ratio'] = cache_bytes / read_bytes
    
    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹ç‡
    rows_read = overall.get('rows_read_count', 0)
    rows_produced = overall.get('rows_produced_count', 0)
    if rows_read > 0:
        indicators['data_selectivity'] = rows_produced / rows_read
    
    # Photonä½¿ç”¨ç‡ï¼ˆã‚¿ã‚¹ã‚¯å®Ÿè¡Œæ™‚é–“ã«å¯¾ã™ã‚‹å‰²åˆï¼‰
    task_time = overall.get('task_total_time_ms', 0)
    photon_time = overall.get('photon_total_time_ms', 0)
    if task_time > 0:
        indicators['photon_ratio'] = min(photon_time / task_time, 1.0)  # æœ€å¤§100%ã«åˆ¶é™
    else:
        indicators['photon_ratio'] = 0.0
    
    # ã‚¹ãƒ”ãƒ«æ¤œå‡ºï¼ˆè©³ç´°ç‰ˆï¼šSink - Num bytes spilled to disk due to memory pressure ãƒ™ãƒ¼ã‚¹ï¼‰
    spill_detected = False
    total_spill_bytes = 0
    spill_details = []
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹åï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
    target_spill_metrics = [
        "Sink - Num bytes spilled to disk due to memory pressure",
        "Num bytes spilled to disk due to memory pressure"
    ]
    
    # å„ãƒãƒ¼ãƒ‰ã§ã‚¹ãƒ”ãƒ«æ¤œå‡ºã‚’å®Ÿè¡Œ
    for node in metrics.get('node_metrics', []):
        node_spill_found = False
        
        # 1. detailed_metricsã‹ã‚‰æ¤œç´¢
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            metric_value = metric_info.get('value', 0)
            metric_label = metric_info.get('label', '')
            
            # è¤‡æ•°ã®ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’ãƒã‚§ãƒƒã‚¯
            if ((metric_key in target_spill_metrics or 
                 metric_label in target_spill_metrics) and metric_value > 0):
                spill_detected = True
                node_spill_found = True
                total_spill_bytes += metric_value
                spill_details.append({
                    'node_id': node.get('node_id', ''),
                    'node_name': node.get('name', ''),
                    'spill_bytes': metric_value,
                    'spill_metric': metric_key if metric_key in target_spill_metrics else metric_label,
                    'source': 'detailed_metrics'
                })
                break
        
        # 2. raw_metricsã‹ã‚‰æ¤œç´¢ï¼ˆã“ã®ãƒãƒ¼ãƒ‰ã§ã¾ã è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆï¼‰
        if not node_spill_found:
            raw_metrics = node.get('metrics', [])
            for metric in raw_metrics:
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                # è¤‡æ•°ã®ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’ãƒã‚§ãƒƒã‚¯
                if ((metric_key in target_spill_metrics or 
                     metric_label in target_spill_metrics) and metric_value > 0):
                    spill_detected = True
                    node_spill_found = True
                    total_spill_bytes += metric_value
                    spill_details.append({
                        'node_id': node.get('node_id', ''),
                        'node_name': node.get('name', ''),
                        'spill_bytes': metric_value,
                        'spill_metric': metric_key if metric_key in target_spill_metrics else metric_label,
                        'source': 'raw_metrics'
                    })
                    break
        
        # 3. key_metricsã‹ã‚‰æ¤œç´¢ï¼ˆæœ€å¾Œã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not node_spill_found:
            key_metrics = node.get('key_metrics', {})
            for key_metric_name, key_metric_value in key_metrics.items():
                # è¤‡æ•°ã®ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’ãƒã‚§ãƒƒã‚¯
                if key_metric_name in target_spill_metrics and key_metric_value > 0:
                    spill_detected = True
                    node_spill_found = True
                    total_spill_bytes += key_metric_value
                    spill_details.append({
                        'node_id': node.get('node_id', ''),
                        'node_name': node.get('name', ''),
                        'spill_bytes': key_metric_value,
                        'spill_metric': key_metric_name,
                        'source': 'key_metrics'
                    })
                    break
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: overall_metricsã‹ã‚‰ã®ç°¡æ˜“æ¤œå‡º
    if not spill_detected:
        fallback_spill_bytes = overall.get('spill_to_disk_bytes', 0)
        if fallback_spill_bytes > 0:
            spill_detected = True
            total_spill_bytes = fallback_spill_bytes
            spill_details.append({
                'node_id': 'overall',
                'node_name': 'Overall Metrics',
                'spill_bytes': fallback_spill_bytes,
                'source': 'overall_metrics'
            })
    
    indicators['has_spill'] = spill_detected
    indicators['spill_bytes'] = total_spill_bytes
    indicators['spill_details'] = spill_details
    indicators['spill_nodes_count'] = len(spill_details)
    
    # æœ€ã‚‚æ™‚é–“ã®ã‹ã‹ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸
    stage_durations = [(s['stage_id'], s['duration_ms']) for s in metrics.get('stage_metrics', []) if s['duration_ms'] > 0]
    if stage_durations:
        slowest_stage = max(stage_durations, key=lambda x: x[1])
        indicators['slowest_stage_id'] = slowest_stage[0]
        indicators['slowest_stage_duration'] = slowest_stage[1]
    
    # æœ€ã‚‚ãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨ã™ã‚‹ãƒãƒ¼ãƒ‰
    memory_usage = []
    for node in metrics.get('node_metrics', []):
        peak_memory = node.get('key_metrics', {}).get('peakMemoryBytes', 0)
        if peak_memory > 0:
            memory_usage.append((node['node_id'], node['name'], peak_memory))
    
    if memory_usage:
        highest_memory_node = max(memory_usage, key=lambda x: x[2])
        indicators['highest_memory_node_id'] = highest_memory_node[0]
        indicators['highest_memory_node_name'] = highest_memory_node[1]
        indicators['highest_memory_bytes'] = highest_memory_node[2]
    
    # ä¸¦åˆ—åº¦ã¨ã‚·ãƒ£ãƒƒãƒ•ãƒ«å•é¡Œã®æ¤œå‡º
    shuffle_nodes = []
    low_parallelism_stages = []
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰ã®ç‰¹å®š
    for node in metrics.get('node_metrics', []):
        node_name = node.get('name', '').upper()
        if any(keyword in node_name for keyword in ['SHUFFLE', 'EXCHANGE']):
            shuffle_nodes.append({
                'node_id': node['node_id'],
                'name': node['name'],
                'duration_ms': node.get('key_metrics', {}).get('durationMs', 0),
                'rows': node.get('key_metrics', {}).get('rowsNum', 0)
            })
    
    # ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸ã®æ¤œå‡º
    for stage in metrics.get('stage_metrics', []):
        num_tasks = stage.get('num_tasks', 0)
        duration_ms = stage.get('duration_ms', 0)
        
        # ä¸¦åˆ—åº¦ãŒä½ã„ï¼ˆã‚¿ã‚¹ã‚¯æ•°ãŒå°‘ãªã„ï¼‰ã‹ã¤å®Ÿè¡Œæ™‚é–“ãŒé•·ã„ã‚¹ãƒ†ãƒ¼ã‚¸
        if num_tasks > 0 and num_tasks < 10 and duration_ms > 5000:  # 10ã‚¿ã‚¹ã‚¯æœªæº€ã€5ç§’ä»¥ä¸Š
            low_parallelism_stages.append({
                'stage_id': stage['stage_id'],
                'num_tasks': num_tasks,
                'duration_ms': duration_ms,
                'avg_task_duration': duration_ms / max(num_tasks, 1)
            })
    
    indicators['shuffle_operations_count'] = len(shuffle_nodes)
    indicators['low_parallelism_stages_count'] = len(low_parallelism_stages)
    indicators['has_shuffle_bottleneck'] = len(shuffle_nodes) > 0 and any(s['duration_ms'] > 10000 for s in shuffle_nodes)
    indicators['has_low_parallelism'] = len(low_parallelism_stages) > 0
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã®è©³ç´°æƒ…å ±
    if shuffle_nodes:
        total_shuffle_time = sum(s['duration_ms'] for s in shuffle_nodes)
        indicators['total_shuffle_time_ms'] = total_shuffle_time
        indicators['shuffle_time_ratio'] = total_shuffle_time / max(total_time, 1)
        
        # æœ€ã‚‚æ™‚é–“ã®ã‹ã‹ã‚‹ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ
        slowest_shuffle = max(shuffle_nodes, key=lambda x: x['duration_ms'])
        indicators['slowest_shuffle_duration_ms'] = slowest_shuffle['duration_ms']
        indicators['slowest_shuffle_node'] = slowest_shuffle['name']
    
    # ä½ä¸¦åˆ—åº¦ã®è©³ç´°æƒ…å ±
    if low_parallelism_stages:
        indicators['low_parallelism_details'] = low_parallelism_stages
        avg_parallelism = sum(s['num_tasks'] for s in low_parallelism_stages) / len(low_parallelism_stages)
        indicators['average_low_parallelism'] = avg_parallelism
    
    return indicators

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: calculate_bottleneck_indicators")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ§¬ Liquid Clusteringåˆ†æé–¢æ•°
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ã‚«ãƒ©ãƒ æƒ…å ±æŠ½å‡º
# MAGIC - ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€JOINã€GROUP BYæ¡ä»¶ã®åˆ†æ
# MAGIC - ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å½±éŸ¿ã®è©•ä¾¡
# MAGIC - ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¨å¥¨ã‚«ãƒ©ãƒ ã®ç‰¹å®š

# COMMAND ----------

def calculate_performance_insights_from_metrics(overall_metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ã®ã¿ã‹ã‚‰è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ´å¯Ÿã‚’è¨ˆç®—
    """
    insights = {}
    
    # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿
    total_time_ms = overall_metrics.get('total_time_ms', 0)
    read_bytes = overall_metrics.get('read_bytes', 0)
    read_cache_bytes = overall_metrics.get('read_cache_bytes', 0)
    read_remote_bytes = overall_metrics.get('read_remote_bytes', 0)
    rows_read = overall_metrics.get('rows_read_count', 0)
    rows_produced = overall_metrics.get('rows_produced_count', 0)
    read_files = overall_metrics.get('read_files_count', 0)
    read_partitions = overall_metrics.get('read_partitions_count', 0)
    photon_time = overall_metrics.get('photon_total_time_ms', 0)
    task_time = overall_metrics.get('task_total_time_ms', 0)
    spill_bytes = overall_metrics.get('spill_to_disk_bytes', 0)
    
    # 1. ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡åˆ†æ
    insights['data_efficiency'] = {
        'data_selectivity': rows_produced / max(rows_read, 1),
        'avg_bytes_per_file': read_bytes / max(read_files, 1),
        'avg_bytes_per_partition': read_bytes / max(read_partitions, 1),
        'avg_rows_per_file': rows_read / max(read_files, 1),
        'avg_rows_per_partition': rows_read / max(read_partitions, 1)
    }
    
    # 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡åˆ†æ
    cache_hit_ratio = read_cache_bytes / max(read_bytes, 1)
    insights['cache_efficiency'] = {
        'cache_hit_ratio': cache_hit_ratio,
        'cache_hit_percentage': cache_hit_ratio * 100,
        'remote_read_ratio': read_remote_bytes / max(read_bytes, 1),
        'cache_effectiveness': 'high' if cache_hit_ratio > 0.8 else 'medium' if cache_hit_ratio > 0.5 else 'low'
    }
    
    # 3. ä¸¦åˆ—åŒ–åŠ¹ç‡åˆ†æ
    insights['parallelization'] = {
        'files_per_second': read_files / max(total_time_ms / 1000, 1),
        'partitions_per_second': read_partitions / max(total_time_ms / 1000, 1),
        'throughput_mb_per_second': (read_bytes / 1024 / 1024) / max(total_time_ms / 1000, 1),
        'rows_per_second': rows_read / max(total_time_ms / 1000, 1)
    }
    
    # 4. PhotonåŠ¹ç‡åˆ†æ
    photon_efficiency = photon_time / max(task_time, 1)
    insights['photon_analysis'] = {
        'photon_enabled': photon_time > 0,
        'photon_efficiency': photon_efficiency,
        'photon_utilization_percentage': photon_efficiency * 100,
        'photon_effectiveness': 'high' if photon_efficiency > 0.8 else 'medium' if photon_efficiency > 0.5 else 'low'
    }
    
    # 5. ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³
    insights['resource_usage'] = {
        'memory_pressure': spill_bytes > 0,
        'spill_gb': spill_bytes / 1024 / 1024 / 1024,
        'data_processed_gb': read_bytes / 1024 / 1024 / 1024,
        'data_reduction_ratio': 1 - (rows_produced / max(rows_read, 1))
    }
    
    # 6. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™
    bottlenecks = []
    if cache_hit_ratio < 0.3:
        bottlenecks.append('ä½ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡')
    if read_remote_bytes / max(read_bytes, 1) > 0.8:
        bottlenecks.append('é«˜ãƒªãƒ¢ãƒ¼ãƒˆèª­ã¿è¾¼ã¿æ¯”ç‡')
    if photon_efficiency < 0.5 and photon_time > 0:
        bottlenecks.append('ä½PhotonåŠ¹ç‡')
    if spill_bytes > 0:
        bottlenecks.append('ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«ç™ºç”Ÿ')
    if insights['data_efficiency']['data_selectivity'] < 0.1:
        bottlenecks.append('ä½ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§')
    
    insights['potential_bottlenecks'] = bottlenecks
    
    return insights

def extract_liquid_clustering_data(profiler_data: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Liquid Clusteringåˆ†æã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆLLMåˆ†æç”¨ï¼‰
    """
    extracted_data = {
        "filter_columns": [],
        "join_columns": [],
        "groupby_columns": [],
        "aggregate_columns": [],
        "table_info": {},
        "scan_nodes": [],
        "join_nodes": [],
        "filter_nodes": [],
        "metadata_summary": {}
    }
    
    print(f"ğŸ” Liquid Clusteringåˆ†æç”¨ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–‹å§‹")
    
    # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’ç¢ºèª
    data_format = metrics.get('data_format', '')
    if data_format == 'sql_query_summary':
        print("ğŸ“Š SQLã‚¯ã‚¨ãƒªã‚µãƒãƒªãƒ¼å½¢å¼: åˆ¶é™ä»˜ãã®Liquid Clusteringåˆ†æ")
        # test2.jsonå½¢å¼ã®å ´åˆã¯åˆ¶é™ä»˜ãã®åˆ†æã‚’è¡Œã†
        query_info = metrics.get('query_info', {})
        query_text = query_info.get('query_text', '')
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ã®ã¿ã‹ã‚‰åŸºæœ¬çš„ãªãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’ç”Ÿæˆ
        # test2.jsonå½¢å¼ã§ã¯ planMetadatas ãŒç©ºã®ãŸã‚ã€graphs metadata ã¯åˆ©ç”¨ä¸å¯
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é‡è¦–ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’è¡Œã†
        
        extracted_data["table_info"]["metrics_summary"] = {
            "node_name": "Metrics-Based Analysis",
            "node_tag": "QUERY_SUMMARY", 
            "node_id": "summary",
            "files_count": overall_metrics.get('read_files_count', 0),
            "partitions_count": overall_metrics.get('read_partitions_count', 0),
            "data_size_gb": overall_metrics.get('read_bytes', 0) / 1024 / 1024 / 1024,
            "rows_read": overall_metrics.get('rows_read_count', 0),
            "rows_produced": overall_metrics.get('rows_produced_count', 0),
            "data_selectivity": overall_metrics.get('rows_produced_count', 0) / max(overall_metrics.get('rows_read_count', 1), 1),
            "avg_file_size_mb": (overall_metrics.get('read_bytes', 0) / 1024 / 1024) / max(overall_metrics.get('read_files_count', 1), 1),
            "avg_partition_size_mb": (overall_metrics.get('read_bytes', 0) / 1024 / 1024) / max(overall_metrics.get('read_partitions_count', 1), 1),
            "note": "è©³ç´°ãªãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã¯SQLã‚¯ã‚¨ãƒªã‚µãƒãƒªãƒ¼å½¢å¼ã§ã¯åˆ©ç”¨ä¸å¯ã€‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹åˆ†æã‚’å®Ÿè¡Œã€‚"
        }
        
        # ã‚µãƒãƒªãƒ¼ãƒãƒ¼ãƒ‰ã®æƒ…å ±ã‚’ä½¿ç”¨
        for node in metrics.get('node_metrics', []):
            node_name = node.get('name', '')
            extracted_data["scan_nodes"].append({
                "name": node_name,
                "type": node.get('tag', ''),
                "rows": node.get('key_metrics', {}).get('rowsNum', 0),
                "duration_ms": node.get('key_metrics', {}).get('durationMs', 0),
                "node_id": node.get('node_id', '')
            })
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ï¼ˆåˆ¶é™ä»˜ãï¼‰
        view_count = sum(1 for table in extracted_data["table_info"].values() if table.get('is_view', False))
        actual_table_count = sum(len(table.get('underlying_tables', [])) for table in extracted_data["table_info"].values())
        
        extracted_data["metadata_summary"] = {
            "total_nodes": len(metrics.get('node_metrics', [])),
            "total_graphs": 0,
            "filter_expressions_count": 0,
            "join_expressions_count": 0,
            "groupby_expressions_count": 0,
            "aggregate_expressions_count": 0,
            "tables_identified": len(extracted_data["table_info"]),
            "views_identified": view_count,
            "underlying_tables_estimated": actual_table_count,
            "scan_nodes_count": len(extracted_data["scan_nodes"]),
            "join_nodes_count": 0,
            "filter_nodes_count": 0,
            "analysis_limitation": "SQLã‚¯ã‚¨ãƒªã‚µãƒãƒªãƒ¼å½¢å¼ã®ãŸã‚è©³ç´°åˆ†æãŒåˆ¶é™ã•ã‚Œã¦ã„ã¾ã™"
        }
        
        print(f"âœ… åˆ¶é™ä»˜ããƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†: {extracted_data['metadata_summary']}")
        
        # ãƒ“ãƒ¥ãƒ¼æƒ…å ±ã®è©³ç´°è¡¨ç¤º
        if view_count > 0:
            print(f"ğŸ” ãƒ“ãƒ¥ãƒ¼æƒ…å ±ã®è©³ç´°:")
            for table_name, table_info in extracted_data["table_info"].items():
                if table_info.get('is_view', False):
                    print(f"  ğŸ“Š ãƒ“ãƒ¥ãƒ¼: {table_name}")
                    print(f"     ã‚¨ã‚¤ãƒªã‚¢ã‚¹: {table_info.get('alias', 'ãªã—')}")
                    print(f"     ãƒ†ãƒ¼ãƒ–ãƒ«ç¨®åˆ¥: {table_info.get('table_type', 'unknown')}")
                    
                    underlying_tables = table_info.get('underlying_tables', [])
                    if underlying_tables:
                        print(f"     æ¨å®šå®Ÿãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(underlying_tables)}")
                        for i, underlying_table in enumerate(underlying_tables[:3]):  # æœ€å¤§3å€‹è¡¨ç¤º
                            print(f"       - {underlying_table}")
                        if len(underlying_tables) > 3:
                            print(f"       ... ãŠã‚ˆã³ {len(underlying_tables) - 3} å€‹ã®è¿½åŠ ãƒ†ãƒ¼ãƒ–ãƒ«")
                    print()
        
        return extracted_data
    
    # é€šå¸¸ã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼å½¢å¼ã®å‡¦ç†
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿè¡Œã‚°ãƒ©ãƒ•æƒ…å ±ã‚’å–å¾—ï¼ˆè¤‡æ•°ã‚°ãƒ©ãƒ•å¯¾å¿œï¼‰
    graphs = profiler_data.get('graphs', [])
    if not graphs:
        print("âš ï¸ ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return extracted_data

    # ã™ã¹ã¦ã®ã‚°ãƒ©ãƒ•ã‹ã‚‰ãƒãƒ¼ãƒ‰ã‚’åé›†
    all_nodes = []
    for graph_index, graph in enumerate(graphs):
        nodes = graph.get('nodes', [])
        for node in nodes:
            node['graph_index'] = graph_index
            all_nodes.append(node)
    
    print(f"ğŸ” {len(graphs)}å€‹ã®ã‚°ãƒ©ãƒ•ã‹ã‚‰{len(all_nodes)}å€‹ã®ãƒãƒ¼ãƒ‰ã‚’å‡¦ç†ä¸­")

    # ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã‚’æŠ½å‡º
    for node in all_nodes:
        node_name = node.get('name', '')
        node_tag = node.get('tag', '')
        node_metadata = node.get('metadata', [])
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é‡è¦ãªæƒ…å ±ã‚’æŠ½å‡º
        for metadata_item in node_metadata:
            key = metadata_item.get('key', '')
            values = metadata_item.get('values', [])
            value = metadata_item.get('value', '')
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã®æŠ½å‡º
            if key == 'FILTERS' and values:
                for filter_expr in values:
                    extracted_data["filter_columns"].append({
                        "expression": filter_expr,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # GROUP BYå¼ã®æŠ½å‡º
            elif key == 'GROUPING_EXPRESSIONS' and values:
                for group_expr in values:
                    extracted_data["groupby_columns"].append({
                        "expression": group_expr,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # JOINæ¡ä»¶ã®æŠ½å‡º
            elif key in ['LEFT_KEYS', 'RIGHT_KEYS'] and values:
                for join_key in values:
                    extracted_data["join_columns"].append({
                        "expression": join_key,
                        "key_type": key,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # é›†ç´„é–¢æ•°ã®æŠ½å‡º
            elif key == 'AGGREGATE_EXPRESSIONS' and values:
                for agg_expr in values:
                    extracted_data["aggregate_columns"].append({
                        "expression": agg_expr,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®æŠ½å‡º
            elif key == 'SCAN_IDENTIFIER':
                table_name = value
                extracted_data["table_info"][table_name] = {
                    "node_name": node_name,
                    "node_tag": node_tag,
                    "node_id": node.get('id', '')
                }

    # ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—åˆ¥ã®åˆ†é¡
    node_metrics = metrics.get('node_metrics', [])
    for node in node_metrics:
        node_name = node.get('name', '')
        node_type = node.get('tag', '')
        key_metrics = node.get('key_metrics', {})
        
        if any(keyword in node_name.upper() for keyword in ['SCAN', 'FILESCAN', 'PARQUET', 'DELTA']):
            extracted_data["scan_nodes"].append({
                "name": node_name,
                "type": node_type,
                "rows": key_metrics.get('rowsNum', 0),
                "duration_ms": key_metrics.get('durationMs', 0),
                "node_id": node.get('node_id', '')
            })
        elif any(keyword in node_name.upper() for keyword in ['JOIN', 'HASH']):
            extracted_data["join_nodes"].append({
                "name": node_name,
                "type": node_type,
                "duration_ms": key_metrics.get('durationMs', 0),
                "node_id": node.get('node_id', '')
            })
        elif any(keyword in node_name.upper() for keyword in ['FILTER']):
            extracted_data["filter_nodes"].append({
                "name": node_name,
                "type": node_type,
                "duration_ms": key_metrics.get('durationMs', 0),
                "node_id": node.get('node_id', '')
            })

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼
    extracted_data["metadata_summary"] = {
        "total_nodes": len(all_nodes),
        "total_graphs": len(graphs),
        "filter_expressions_count": len(extracted_data["filter_columns"]),
        "join_expressions_count": len(extracted_data["join_columns"]),
        "groupby_expressions_count": len(extracted_data["groupby_columns"]),
        "aggregate_expressions_count": len(extracted_data["aggregate_columns"]),
        "tables_identified": len(extracted_data["table_info"]),
        "scan_nodes_count": len(extracted_data["scan_nodes"]),
        "join_nodes_count": len(extracted_data["join_nodes"]),
        "filter_nodes_count": len(extracted_data["filter_nodes"])
    }
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†: {extracted_data['metadata_summary']}")
    return extracted_data

def analyze_liquid_clustering_opportunities(profiler_data: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    LLMã‚’ä½¿ç”¨ã—ã¦Liquid Clusteringã®åˆ†æã¨æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ
    """
    print(f"ğŸ¤– LLMã«ã‚ˆã‚‹Liquid Clusteringåˆ†æã‚’é–‹å§‹")
    
    # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
    extracted_data = extract_liquid_clustering_data(profiler_data, metrics)
    
    # LLMåˆ†æç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
    overall_metrics = metrics.get('overall_metrics', {})
    bottleneck_indicators = metrics.get('bottleneck_indicators', {})
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦
    total_time_sec = overall_metrics.get('total_time_ms', 0) / 1000
    read_gb = overall_metrics.get('read_bytes', 0) / 1024 / 1024 / 1024
    rows_produced = overall_metrics.get('rows_produced_count', 0)
    rows_read = overall_metrics.get('rows_read_count', 0)
    
    # æŠ½å‡ºã—ãŸã‚«ãƒ©ãƒ æƒ…å ±ã®ã‚µãƒãƒªãƒ¼ä½œæˆï¼ˆä¸Šä½5å€‹ã¾ã§ï¼‰
    filter_summary = []
    for i, item in enumerate(extracted_data["filter_columns"][:5]):
        filter_summary.append(f"  {i+1}. {item['expression']} (ãƒãƒ¼ãƒ‰: {item['node_name']})")
    
    join_summary = []
    for i, item in enumerate(extracted_data["join_columns"][:5]):
        join_summary.append(f"  {i+1}. {item['expression']} (ã‚¿ã‚¤ãƒ—: {item['key_type']}, ãƒãƒ¼ãƒ‰: {item['node_name']})")
    
    groupby_summary = []
    for i, item in enumerate(extracted_data["groupby_columns"][:5]):
        groupby_summary.append(f"  {i+1}. {item['expression']} (ãƒãƒ¼ãƒ‰: {item['node_name']})")
    
    aggregate_summary = []
    for i, item in enumerate(extracted_data["aggregate_columns"][:5]):
        aggregate_summary.append(f"  {i+1}. {item['expression']} (ãƒãƒ¼ãƒ‰: {item['node_name']})")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®ã‚µãƒãƒªãƒ¼
    table_summary = []
    for table_name, table_info in extracted_data["table_info"].items():
        table_summary.append(f"  - {table_name} (ãƒãƒ¼ãƒ‰: {table_info['node_name']})")
    
    # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
    scan_performance = []
    for scan in extracted_data["scan_nodes"]:
        efficiency = scan['rows'] / max(scan['duration_ms'], 1)
        scan_performance.append(f"  - {scan['name']}: {scan['rows']:,}è¡Œ, {scan['duration_ms']:,}ms, åŠ¹ç‡={efficiency:.1f}è¡Œ/ms")

    clustering_prompt = f"""
ã‚ãªãŸã¯Databricksã®Liquid Clusteringå°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€æœ€é©ãªLiquid Clusteringã®æ¨å¥¨äº‹é …ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚

ã€ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦ã€‘
- å®Ÿè¡Œæ™‚é–“: {total_time_sec:.1f}ç§’
- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {read_gb:.2f}GB
- å‡ºåŠ›è¡Œæ•°: {rows_produced:,}è¡Œ
- èª­ã¿è¾¼ã¿è¡Œæ•°: {rows_read:,}è¡Œ
- ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§: {(rows_produced/max(rows_read,1)):.4f}

ã€æŠ½å‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‘

ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ ({len(extracted_data["filter_columns"])}å€‹):
{chr(10).join(filter_summary)}

ğŸ”— JOINæ¡ä»¶ ({len(extracted_data["join_columns"])}å€‹):
{chr(10).join(join_summary)}

ğŸ“Š GROUP BY ({len(extracted_data["groupby_columns"])}å€‹):
{chr(10).join(groupby_summary)}

ğŸ“ˆ é›†ç´„é–¢æ•° ({len(extracted_data["aggregate_columns"])}å€‹):
{chr(10).join(aggregate_summary)}

ã€ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã€‘
ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(extracted_data["table_info"])}å€‹
{chr(10).join(table_summary)}

ã€ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‘
{chr(10).join(scan_performance)}

ã€ç¾åœ¨ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã€‘
- ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ: {'ã‚ã‚Š' if bottleneck_indicators.get('has_spill', False) else 'ãªã—'}
- ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ: {bottleneck_indicators.get('shuffle_operations_count', 0)}å›
- ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸: {bottleneck_indicators.get('low_parallelism_stages_count', 0)}å€‹

ã€åˆ†æè¦æ±‚ã€‘
1. å„ãƒ†ãƒ¼ãƒ–ãƒ«ã«å¯¾ã™ã‚‹æœ€é©ãªLiquid Clusteringã‚«ãƒ©ãƒ ã®æ¨å¥¨ï¼ˆæœ€å¤§4ã‚«ãƒ©ãƒ ï¼‰
2. ã‚«ãƒ©ãƒ é¸å®šã®æ ¹æ‹ ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€JOINã€GROUP BYã§ã®ä½¿ç”¨é »åº¦ã¨é‡è¦åº¦ï¼‰
3. å®Ÿè£…å„ªå…ˆé †ä½ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸ŠåŠ¹æœé †ï¼‰
4. å…·ä½“çš„ãªSQLå®Ÿè£…ä¾‹ï¼ˆæ­£ã—ã„Databricks SQLæ§‹æ–‡ï¼‰
5. æœŸå¾…ã•ã‚Œã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„åŠ¹æœï¼ˆæ•°å€¤ã§ï¼‰

ã€åˆ¶ç´„äº‹é …ã€‘
- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚„ZORDERã¯ææ¡ˆã—ãªã„ï¼ˆLiquid Clusteringã®ã¿ï¼‰
- æ­£ã—ã„Databricks SQLæ§‹æ–‡ã‚’ä½¿ç”¨ï¼š
  * æ–°è¦ãƒ†ãƒ¼ãƒ–ãƒ«: CREATE TABLE ... CLUSTER BY (col1, col2, ...)
  * æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«: ALTER TABLE table_name CLUSTER BY (col1, col2, ...)
- æœ€å¤§4ã‚«ãƒ©ãƒ ã¾ã§ã®æ¨å¥¨
- ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ã‚„ä¸¦åˆ—åº¦ã®å•é¡Œã‚‚è€ƒæ…®

ç°¡æ½”ã§å®Ÿè·µçš„ãªåˆ†æçµæœã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚
"""

    try:
        # LLMåˆ†æã®å®Ÿè¡Œ
        provider = LLM_CONFIG["provider"]
        print(f"ğŸ¤– {provider}ã‚’ä½¿ç”¨ã—ã¦Liquid Clusteringåˆ†æä¸­...")
        
        if provider == "databricks":
            llm_analysis = _call_databricks_llm(clustering_prompt)
        elif provider == "openai":
            llm_analysis = _call_openai_llm(clustering_prompt)
        elif provider == "azure_openai":
            llm_analysis = _call_azure_openai_llm(clustering_prompt)
        elif provider == "anthropic":
            llm_analysis = _call_anthropic_llm(clustering_prompt)
        else:
            llm_analysis = f"âŒ ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}"
        
        # åˆ†æçµæœã®æ§‹é€ åŒ–
        clustering_analysis = {
            "llm_analysis": llm_analysis,
            "extracted_data": extracted_data,
            "performance_context": {
                "total_time_sec": total_time_sec,
                "read_gb": read_gb,
                "rows_produced": rows_produced,
                "rows_read": rows_read,
                "data_selectivity": rows_produced/max(rows_read,1)
            },
            "summary": {
                "analysis_method": "LLM-based",
                "tables_identified": len(extracted_data["table_info"]),
                "total_filter_columns": len(extracted_data["filter_columns"]),
                "total_join_columns": len(extracted_data["join_columns"]),
                "total_groupby_columns": len(extracted_data["groupby_columns"]),
                "total_aggregate_columns": len(extracted_data["aggregate_columns"]),
                "scan_nodes_count": len(extracted_data["scan_nodes"]),
                "llm_provider": provider
            }
        }
        
        print("âœ… LLM Liquid Clusteringåˆ†æå®Œäº†")
        return clustering_analysis
        
    except Exception as e:
        error_msg = f"LLMåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
        print(f"âŒ {error_msg}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªæŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’è¿”ã™
        return {
            "llm_analysis": f"âŒ LLMåˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ: {error_msg}",
            "extracted_data": extracted_data,
            "summary": {
                "analysis_method": "extraction-only",
                "tables_identified": len(extracted_data["table_info"]),
                "total_filter_columns": len(extracted_data["filter_columns"]),
                "error": error_msg
            }
        }

def save_liquid_clustering_analysis(clustering_analysis: Dict[str, Any], output_dir: str = "/tmp") -> Dict[str, str]:
    """
    Liquid Clusteringåˆ†æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
    """
    import os
    import json
    from datetime import datetime
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    json_path = f"{output_dir}/liquid_clustering_analysis_{timestamp}.json"
    markdown_path = f"{output_dir}/liquid_clustering_analysis_{timestamp}.md"
    sql_path = f"{output_dir}/liquid_clustering_implementation_{timestamp}.sql"
    
    file_paths = {}
    
    try:
        # 1. JSONå½¢å¼ã§ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        # setå‹ã‚’listå‹ã«å¤‰æ›ã—ã¦JSON serializable ã«ã™ã‚‹
        json_data = convert_sets_to_lists(clustering_analysis)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        file_paths['json'] = json_path
        print(f"âœ… JSONå½¢å¼ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {json_path}")
        
        # 2. Markdownå½¢å¼ã§ã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        markdown_content = generate_liquid_clustering_markdown_report(clustering_analysis)
        
        with open(markdown_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        file_paths['markdown'] = markdown_path
        print(f"âœ… Markdownå½¢å¼ã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {markdown_path}")
        
        # 3. SQLå®Ÿè£…ä¾‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆ
        sql_content = generate_liquid_clustering_sql_implementations(clustering_analysis)
        
        with open(sql_path, 'w', encoding='utf-8') as f:
            f.write(sql_content)
        
        file_paths['sql'] = sql_path
        print(f"âœ… SQLå®Ÿè£…ä¾‹ã‚’ä¿å­˜: {sql_path}")
        
        return file_paths
        
    except Exception as e:
        error_msg = f"ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}"
        print(f"âŒ {error_msg}")
        return {"error": error_msg}

def generate_liquid_clustering_markdown_report(clustering_analysis: Dict[str, Any]) -> str:
    """
    Liquid Clusteringåˆ†æçµæœã®Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    """
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # åŸºæœ¬æƒ…å ±ã®å–å¾—
    summary = clustering_analysis.get('summary', {})
    performance_context = clustering_analysis.get('performance_context', {})
    extracted_data = clustering_analysis.get('extracted_data', {})
    llm_analysis = clustering_analysis.get('llm_analysis', '')
    
    markdown_content = f"""# Liquid Clustering åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {timestamp}  
**åˆ†ææ–¹æ³•**: {summary.get('analysis_method', 'Unknown')}  
**LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼**: {summary.get('llm_provider', 'Unknown')}

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦

| é …ç›® | å€¤ |
|------|-----|
| å®Ÿè¡Œæ™‚é–“ | {performance_context.get('total_time_sec', 0):.1f}ç§’ |
| ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ | {performance_context.get('read_gb', 0):.2f}GB |
| å‡ºåŠ›è¡Œæ•° | {performance_context.get('rows_produced', 0):,}è¡Œ |
| èª­ã¿è¾¼ã¿è¡Œæ•° | {performance_context.get('rows_read', 0):,}è¡Œ |
| ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§ | {performance_context.get('data_selectivity', 0):.4f} |

## ğŸ” æŠ½å‡ºã•ã‚ŒãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿

### ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ ({summary.get('total_filter_columns', 0)}å€‹)
"""
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã®è©³ç´°
    filter_columns = extracted_data.get('filter_columns', [])
    for i, filter_item in enumerate(filter_columns[:10], 1):  # æœ€å¤§10å€‹ã¾ã§è¡¨ç¤º
        markdown_content += f"{i}. `{filter_item.get('expression', '')}` (ãƒãƒ¼ãƒ‰: {filter_item.get('node_name', '')})\n"
    
    if len(filter_columns) > 10:
        markdown_content += f"... ä»– {len(filter_columns) - 10}å€‹\n"
    
    markdown_content += f"""
### JOINæ¡ä»¶ ({summary.get('total_join_columns', 0)}å€‹)
"""
    
    # JOINæ¡ä»¶ã®è©³ç´°
    join_columns = extracted_data.get('join_columns', [])
    for i, join_item in enumerate(join_columns[:10], 1):
        markdown_content += f"{i}. `{join_item.get('expression', '')}` ({join_item.get('key_type', '')})\n"
    
    if len(join_columns) > 10:
        markdown_content += f"... ä»– {len(join_columns) - 10}å€‹\n"
    
    markdown_content += f"""
### GROUP BYæ¡ä»¶ ({summary.get('total_groupby_columns', 0)}å€‹)
"""
    
    # GROUP BYæ¡ä»¶ã®è©³ç´°
    groupby_columns = extracted_data.get('groupby_columns', [])
    for i, groupby_item in enumerate(groupby_columns[:10], 1):
        markdown_content += f"{i}. `{groupby_item.get('expression', '')}` (ãƒãƒ¼ãƒ‰: {groupby_item.get('node_name', '')})\n"
    
    if len(groupby_columns) > 10:
        markdown_content += f"... ä»– {len(groupby_columns) - 10}å€‹\n"
    
    markdown_content += f"""
### é›†ç´„é–¢æ•° ({summary.get('total_aggregate_columns', 0)}å€‹)
"""
    
    # é›†ç´„é–¢æ•°ã®è©³ç´°
    aggregate_columns = extracted_data.get('aggregate_columns', [])
    for i, agg_item in enumerate(aggregate_columns[:10], 1):
        markdown_content += f"{i}. `{agg_item.get('expression', '')}` (ãƒãƒ¼ãƒ‰: {agg_item.get('node_name', '')})\n"
    
    if len(aggregate_columns) > 10:
        markdown_content += f"... ä»– {len(aggregate_columns) - 10}å€‹\n"
    
    markdown_content += f"""
## ğŸ·ï¸ è­˜åˆ¥ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ« ({summary.get('tables_identified', 0)}å€‹)

"""
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®è©³ç´°
    table_info = extracted_data.get('table_info', {})
    for table_name, table_details in table_info.items():
        markdown_content += f"- **{table_name}** (ãƒãƒ¼ãƒ‰: {table_details.get('node_name', '')})\n"
    
    markdown_content += f"""
## ğŸ” ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰åˆ†æ ({summary.get('scan_nodes_count', 0)}å€‹)

"""
    
    # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®è©³ç´°
    scan_nodes = extracted_data.get('scan_nodes', [])
    for scan in scan_nodes:
        efficiency = scan.get('rows', 0) / max(scan.get('duration_ms', 1), 1)
        markdown_content += f"- **{scan.get('name', '')}**: {scan.get('rows', 0):,}è¡Œ, {scan.get('duration_ms', 0):,}ms, åŠ¹ç‡={efficiency:.1f}è¡Œ/ms\n"
    
    markdown_content += f"""
## ğŸ¤– LLMåˆ†æçµæœ

{llm_analysis}

## ğŸ“‹ åˆ†æã‚µãƒãƒªãƒ¼

- **åˆ†æå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°**: {summary.get('tables_identified', 0)}
- **ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶æ•°**: {summary.get('total_filter_columns', 0)}
- **JOINæ¡ä»¶æ•°**: {summary.get('total_join_columns', 0)}
- **GROUP BYæ¡ä»¶æ•°**: {summary.get('total_groupby_columns', 0)}
- **é›†ç´„é–¢æ•°æ•°**: {summary.get('total_aggregate_columns', 0)}
- **ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰æ•°**: {summary.get('scan_nodes_count', 0)}

---
*ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ™‚åˆ»: {timestamp}*
"""
    
    return markdown_content

def generate_liquid_clustering_sql_implementations(clustering_analysis: Dict[str, Any]) -> str:
    """
    Liquid Clusteringå®Ÿè£…ç”¨ã®SQLä¾‹ã‚’ç”Ÿæˆ
    """
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # åŸºæœ¬æƒ…å ±ã®å–å¾—
    extracted_data = clustering_analysis.get('extracted_data', {})
    table_info = extracted_data.get('table_info', {})
    
    sql_content = f"""-- =====================================================
-- Liquid Clustering å®Ÿè£…SQLä¾‹
-- ç”Ÿæˆæ—¥æ™‚: {timestamp}
-- =====================================================

-- ã€é‡è¦ã€‘
-- ä»¥ä¸‹ã®SQLä¾‹ã¯åˆ†æçµæœã«åŸºã¥ãæ¨å¥¨äº‹é …ã§ã™ã€‚
-- å®Ÿéš›ã®å®Ÿè£…å‰ã«ã€ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‚„ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

"""
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã®SQLå®Ÿè£…ä¾‹ã‚’ç”Ÿæˆ
    for table_name, table_details in table_info.items():
        sql_content += f"""
-- =====================================================
-- ãƒ†ãƒ¼ãƒ–ãƒ«: {table_name}
-- =====================================================

-- æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«Liquid Clusteringã‚’é©ç”¨ã™ã‚‹å ´åˆ:
-- ALTER TABLE {table_name} CLUSTER BY (column1, column2, column3, column4);

-- æ–°è¦ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆæ™‚ã«Liquid Clusteringã‚’è¨­å®šã™ã‚‹å ´åˆ:
-- CREATE TABLE {table_name}_clustered
-- CLUSTER BY (column1, column2, column3, column4)
-- AS SELECT * FROM {table_name};

-- Delta Live Tablesã§ã®è¨­å®šä¾‹:
-- @dlt.table(
--   cluster_by=["column1", "column2", "column3", "column4"]
-- )
-- def {table_name.split('.')[-1]}_clustered():
--   return spark.table("{table_name}")

-- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çŠ¶æ³ã®ç¢ºèª:
-- DESCRIBE DETAIL {table_name};

-- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµ±è¨ˆã®ç¢ºèª:
-- ANALYZE TABLE {table_name} COMPUTE STATISTICS FOR ALL COLUMNS;

"""
    
    sql_content += f"""
-- =====================================================
-- ä¸€èˆ¬çš„ãªLiquid Clusteringå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³
-- =====================================================

-- ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é »åº¦ã®é«˜ã„ã‚«ãƒ©ãƒ ã‚’å„ªå…ˆ
-- æ¨å¥¨é †åº: 1) ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚«ãƒ©ãƒ  2) JOINæ¡ä»¶ã‚«ãƒ©ãƒ  3) GROUP BYã‚«ãƒ©ãƒ 

-- ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã‚’è€ƒæ…®ã—ãŸé †åº
-- ä½ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ â†’ é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã®é †ã§é…ç½®

-- ãƒ‘ã‚¿ãƒ¼ãƒ³3: ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãé…ç½®
-- ã‚ˆãä¸€ç·’ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ ã‚’è¿‘ã„ä½ç½®ã«é…ç½®

-- =====================================================
-- å®Ÿè£…å¾Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼SQL
-- =====================================================

-- 1. ã‚¯ã‚¨ãƒªå®Ÿè¡Œè¨ˆç”»ã®ç¢ºèª
-- EXPLAIN SELECT ... FROM table_name WHERE ...;

-- 2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒƒãƒ—çµ±è¨ˆã®ç¢ºèª
-- SELECT * FROM table_name WHERE filter_column = 'value';
-- -- SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒƒãƒ—æ•°ã‚’ç¢ºèª

-- 3. ãƒ‡ãƒ¼ã‚¿é…ç½®ã®ç¢ºèª
-- SELECT 
--   file_path,
--   count(*) as row_count,
--   min(cluster_column1) as min_val,
--   max(cluster_column1) as max_val
-- FROM table_name
-- GROUP BY file_path
-- ORDER BY file_path;

-- =====================================================
-- æ³¨æ„äº‹é …
-- =====================================================

-- 1. Liquid Clusteringã¯æœ€å¤§4ã‚«ãƒ©ãƒ ã¾ã§æŒ‡å®šå¯èƒ½
-- 2. ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã¨ã¯ä½µç”¨ä¸å¯
-- 3. æ—¢å­˜ã®ZORDER BYã¯è‡ªå‹•çš„ã«ç„¡åŠ¹åŒ–ã•ã‚Œã‚‹
-- 4. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®åŠ¹æœã¯æ™‚é–“ã¨ã¨ã‚‚ã«å‘ä¸Šã™ã‚‹ï¼ˆOPTIMIZEå®Ÿè¡Œã§æœ€é©åŒ–ï¼‰
-- 5. å®šæœŸçš„ãªOPTIMIZEå®Ÿè¡Œã‚’æ¨å¥¨

-- OPTIMIZEå®Ÿè¡Œä¾‹:
-- OPTIMIZE table_name;

-- =====================================================
-- ç”Ÿæˆæƒ…å ±
-- =====================================================
-- ç”Ÿæˆæ—¥æ™‚: {timestamp}
-- åˆ†æå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(table_info)}
-- åŸºã¥ã„ãŸåˆ†æ: LLMã«ã‚ˆã‚‹Liquid Clusteringåˆ†æ
"""
    
    return sql_content

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: analyze_liquid_clustering_opportunities, save_liquid_clustering_analysis")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¤– LLMã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æé–¢æ•°
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®LLMåˆ†æç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
# MAGIC - è¤‡æ•°LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å¯¾å¿œï¼ˆDatabricks/OpenAI/Azure/Anthropicï¼‰
# MAGIC - æ—¥æœ¬èªã§ã®è©³ç´°ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
# MAGIC - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æ

# COMMAND ----------

def analyze_bottlenecks_with_llm(metrics: Dict[str, Any]) -> str:
    """
    è¨­å®šã•ã‚ŒãŸLLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’è¡Œã†
    """
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¦ç´„ã®æº–å‚™ï¼ˆç°¡æ½”ç‰ˆï¼‰
    # ä¸»è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã¿ã‚’æŠ½å‡ºã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
    total_time_sec = metrics['overall_metrics'].get('total_time_ms', 0) / 1000
    read_gb = metrics['overall_metrics'].get('read_bytes', 0) / 1024 / 1024 / 1024
    cache_ratio = metrics['bottleneck_indicators'].get('cache_hit_ratio', 0) * 100
    data_selectivity = metrics['bottleneck_indicators'].get('data_selectivity', 0) * 100
    
    # Liquid Clusteringåˆ†ææƒ…å ±ã®å–å¾—ï¼ˆLLMãƒ™ãƒ¼ã‚¹å¯¾å¿œï¼‰
    liquid_analysis = metrics.get('liquid_clustering_analysis', {})
    extracted_data = liquid_analysis.get('extracted_data', {})
    metadata_summary = extracted_data.get('metadata_summary', {})
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®ç°¡æ½”ç‰ˆ
    table_info = extracted_data.get('table_info', {})
    table_recommendations = []
    if table_info:
        for i, (table_name, table_details) in enumerate(list(table_info.items())[:3]):
            table_recommendations.append(f"- {table_name}: LLMåˆ†æã«ã‚ˆã‚‹æ¨å¥¨")
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ»JOINãƒ»GROUP BYã‚«ãƒ©ãƒ æƒ…å ±
    filter_columns = extracted_data.get('filter_columns', [])
    join_columns = extracted_data.get('join_columns', [])
    groupby_columns = extracted_data.get('groupby_columns', [])
    
    high_impact_summary = []
    if filter_columns:
        high_impact_summary.append(f"- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶: {len(filter_columns)}å€‹ã®ã‚«ãƒ©ãƒ æŠ½å‡º")
    if join_columns:
        high_impact_summary.append(f"- JOINæ¡ä»¶: {len(join_columns)}å€‹ã®ã‚«ãƒ©ãƒ æŠ½å‡º")
    if groupby_columns:
        high_impact_summary.append(f"- GROUP BYæ¡ä»¶: {len(groupby_columns)}å€‹ã®ã‚«ãƒ©ãƒ æŠ½å‡º")
    
    # Photonã¨ä¸¦åˆ—åº¦ã®æƒ…å ±ã‚’è¿½åŠ 
    photon_enabled = metrics['overall_metrics'].get('photon_enabled', False)
    photon_utilization_ratio = metrics['overall_metrics'].get('photon_utilization_ratio', 0)
    photon_utilization = min(photon_utilization_ratio * 100, 100.0)  # æœ€å¤§100%ã«åˆ¶é™
    shuffle_count = metrics['bottleneck_indicators'].get('shuffle_operations_count', 0)
    has_shuffle_bottleneck = metrics['bottleneck_indicators'].get('has_shuffle_bottleneck', False)
    has_low_parallelism = metrics['bottleneck_indicators'].get('has_low_parallelism', False)
    low_parallelism_count = metrics['bottleneck_indicators'].get('low_parallelism_stages_count', 0)
    
    analysis_prompt = f"""
ã‚ãªãŸã¯Databricksã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åˆ†æã—ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®šã—ã¦æ”¹å–„æ¡ˆã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚

ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦ã€‘
- å®Ÿè¡Œæ™‚é–“: {total_time_sec:.1f}ç§’
- èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿: {read_gb:.1f}GB
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡: {cache_ratio:.1f}%
- ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§: {data_selectivity:.1f}%
- ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ: {'ã‚ã‚Š' if metrics['bottleneck_indicators'].get('has_spill', False) else 'ãªã—'}

ã€Photonã‚¨ãƒ³ã‚¸ãƒ³åˆ†æã€‘
- Photonæœ‰åŠ¹: {'ã¯ã„' if photon_enabled else 'ã„ã„ãˆ'}
- Photonåˆ©ç”¨ç‡: {photon_utilization:.1f}%
- Photonæ¨å¥¨: {'æ—¢ã«æœ€é©åŒ–æ¸ˆã¿' if photon_utilization > 80 else 'Photonæœ‰åŠ¹åŒ–ã‚’æ¨å¥¨' if not photon_enabled else 'Photonåˆ©ç”¨ç‡å‘ä¸ŠãŒå¿…è¦'}

ã€ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«åˆ†æã€‘
- ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ: {shuffle_count}å›
- ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯: {'ã‚ã‚Š' if has_shuffle_bottleneck else 'ãªã—'}
- ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸: {low_parallelism_count}å€‹
- ä¸¦åˆ—åº¦å•é¡Œ: {'ã‚ã‚Š' if has_low_parallelism else 'ãªã—'}

ã€Liquid Clusteringæ¨å¥¨ã€‘
ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {metadata_summary.get('tables_identified', 0)}å€‹
æ¨å¥¨ã‚«ãƒ©ãƒ :
{chr(10).join(table_recommendations) if table_recommendations else "LLMåˆ†æçµæœã‚’å‚ç…§"}

ã‚«ãƒ©ãƒ æŠ½å‡ºçŠ¶æ³:
{chr(10).join(high_impact_summary) if high_impact_summary else "åˆ†æãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™"}

ã€é‡è¦æŒ‡æ¨™ã€‘
- æœ€é…ã‚¹ãƒ†ãƒ¼ã‚¸: {metrics['bottleneck_indicators'].get('slowest_stage_id', 'N/A')}
- æœ€é«˜ãƒ¡ãƒ¢ãƒª: {metrics['bottleneck_indicators'].get('highest_memory_bytes', 0)/1024/1024:.0f}MB
- Photonä½¿ç”¨ç‡: {metrics['bottleneck_indicators'].get('photon_ratio', 0)*100:.0f}%

ã€æ±‚ã‚ã‚‹åˆ†æã€‘
1. ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨åŸå› ï¼ˆPhotonã€ä¸¦åˆ—åº¦ã€ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã«ç„¦ç‚¹ï¼‰
2. Liquid Clusteringå®Ÿè£…ã®å„ªå…ˆé †ä½ã¨æ‰‹é †ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»ZORDERä»¥å¤–ã‚’DatabricksSQLã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹æ­£ã—ã„SQLæ§‹æ–‡ã§æç¤ºï¼‰
3. å„æ¨å¥¨ã‚«ãƒ©ãƒ ã®é¸å®šç†ç”±ã¨åŠ¹æœ
4. Photonã‚¨ãƒ³ã‚¸ãƒ³ã®æœ€é©åŒ–æ¡ˆ
5. ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ€é©åŒ–æ¡ˆ
6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„è¦‹è¾¼ã¿

**é‡è¦**: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚„ZORDERã¯ææ¡ˆã›ãšã€Liquid Clusteringã®ã¿ã‚’æ¨å¥¨ã—ã¦ãã ã•ã„ã€‚
Liquid Clusteringå®Ÿè£…æ™‚ã¯ã€æ­£ã—ã„Databricks SQLæ§‹æ–‡ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼š
- æ–°è¦ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆæ™‚: CREATE TABLE ... CLUSTER BY (column1, column2, ...)
- æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«å¤‰æ›´æ™‚: ALTER TABLE table_name CLUSTER BY (column1, column2, ...)
- Delta Liveãƒ†ãƒ¼ãƒ–ãƒ«: @dlt.table(cluster_by=["column1", "column2"])
ç°¡æ½”ã§å®Ÿè·µçš„ãªæ”¹å–„ææ¡ˆã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚
"""
    
    try:
        # è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«åŸºã¥ã„ã¦åˆ†æå®Ÿè¡Œ
        provider = LLM_CONFIG["provider"]
        print(f"ğŸ¤– {provider}ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«åˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ä¸­...")
        print("â³ å¤§ããªãƒ‡ãƒ¼ã‚¿ã®ãŸã‚å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™...")
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ã®å‡¦ç†
        if provider == "databricks":
            return _call_databricks_llm(analysis_prompt)
        elif provider == "openai":
            return _call_openai_llm(analysis_prompt)
        elif provider == "azure_openai":
            return _call_azure_openai_llm(analysis_prompt)
        elif provider == "anthropic":
            return _call_anthropic_llm(analysis_prompt)
        else:
            return f"âŒ ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}"
            
    except Exception as e:
        error_msg = f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
        print(f"âŒ {error_msg}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªåˆ†æçµæœã‚’æä¾›
        fallback_analysis = f"""
ğŸ”§ **åŸºæœ¬çš„ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ** ({provider} LLMåˆ©ç”¨ä¸å¯ã®ãŸã‚ç°¡æ˜“ç‰ˆ)

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦
- **å®Ÿè¡Œæ™‚é–“**: {total_time_sec:.1f}ç§’
- **èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿é‡**: {read_gb:.1f}GB  
- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡**: {cache_ratio:.1f}%
- **ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§**: {data_selectivity:.1f}%

## âš¡ Photonã‚¨ãƒ³ã‚¸ãƒ³åˆ†æ
- **Photonæœ‰åŠ¹**: {'ã¯ã„' if photon_enabled else 'ã„ã„ãˆ'}
- **Photonåˆ©ç”¨ç‡**: {min(photon_utilization, 100.0):.1f}%
- **æ¨å¥¨**: {'Photonåˆ©ç”¨ç‡å‘ä¸ŠãŒå¿…è¦' if photon_utilization < 80 else 'æœ€é©åŒ–æ¸ˆã¿'}

## ï¿½ ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«åˆ†æ
- **ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ**: {shuffle_count}å›
- **ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: {'ã‚ã‚Š' if has_shuffle_bottleneck else 'ãªã—'}
- **ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸**: {low_parallelism_count}å€‹
- **ä¸¦åˆ—åº¦å•é¡Œ**: {'ã‚ã‚Š' if has_low_parallelism else 'ãªã—'}

## ğŸ—‚ï¸ Liquid Clusteringæ¨å¥¨äº‹é …
**å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°**: {metadata_summary.get('tables_identified', 0)}å€‹

**æ¨å¥¨å®Ÿè£…**:
{chr(10).join(table_recommendations) if table_recommendations else '- æ¨å¥¨ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ'}

## âš ï¸ ä¸»è¦ãªå•é¡Œç‚¹
- {'ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™' if metrics['bottleneck_indicators'].get('has_spill', False) else 'ãƒ¡ãƒ¢ãƒªä½¿ç”¨ã¯æ­£å¸¸ã§ã™'}
- {'ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ãŒä½ä¸‹ã—ã¦ã„ã¾ã™' if cache_ratio < 50 else 'ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ã¯è‰¯å¥½ã§ã™'}
- {'ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§ãŒä½ãã€å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™' if data_selectivity < 10 else 'ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§ã¯é©åˆ‡ã§ã™'}
- {'Photonã‚¨ãƒ³ã‚¸ãƒ³ãŒç„¡åŠ¹ã¾ãŸã¯åˆ©ç”¨ç‡ãŒä½ã„' if not photon_enabled or photon_utilization < 50 else 'Photonåˆ©ç”¨ã¯è‰¯å¥½'}
- {'ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãŒç™ºç”Ÿ' if has_shuffle_bottleneck else 'ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‡¦ç†ã¯æ­£å¸¸'}
- {'ä¸¦åˆ—åº¦ãŒä½ã„ã‚¹ãƒ†ãƒ¼ã‚¸ãŒå­˜åœ¨' if has_low_parallelism else 'ä¸¦åˆ—åº¦ã¯é©åˆ‡'}

## ğŸš€ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
1. **Liquid Clusteringå®Ÿè£…**: ä¸Šè¨˜æ¨å¥¨ã‚«ãƒ©ãƒ ã§ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ­£ã—ã„Databricks SQLæ§‹æ–‡: ALTER TABLE table_name CLUSTER BY (column1, column2, ...)ï¼‰
2. **Photonæœ‰åŠ¹åŒ–**: {'Photonã‚¨ãƒ³ã‚¸ãƒ³ã‚’æœ‰åŠ¹ã«ã™ã‚‹' if not photon_enabled else 'Photonè¨­å®šã‚’æœ€é©åŒ–'}
3. **ä¸¦åˆ—åº¦æœ€é©åŒ–**: {'ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µã‚¤ã‚ºãƒ»ä¸¦åˆ—åº¦è¨­å®šã‚’è¦‹ç›´ã—' if has_low_parallelism else 'ç¾åœ¨ã®ä¸¦åˆ—åº¦ã¯é©åˆ‡'}
4. **ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ€é©åŒ–**: {'JOINé †åºãƒ»GROUP BYæœ€é©åŒ–ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‰Šæ¸›' if has_shuffle_bottleneck else 'ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‡¦ç†ã¯æœ€é©'}
5. **ã‚¯ã‚¨ãƒªæœ€é©åŒ–**: WHEREå¥ã®æ¡ä»¶ã‚’é©åˆ‡ã«è¨­å®š
6. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨**: ã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œè¨

**é‡è¦**: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚„ZORDERã¯ä½¿ç”¨ã›ãšã€æ­£ã—ã„Databricks SQLæ§‹æ–‡ï¼ˆALTER TABLE table_name CLUSTER BY (column1, column2, ...)ï¼‰ã‚’ä½¿ç”¨ã—ã¦Liquid Clusteringã§æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚

**æ³¨æ„**: {provider} LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®æ¥ç¶šã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚è©³ç´°ãªåˆ†æã¯æ‰‹å‹•ã§å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚
        """
        return fallback_analysis

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ”Œ å€‹åˆ¥LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ¥ç¶šé–¢æ•°
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - Databricks Model Serving ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæ¥ç¶š
# MAGIC - OpenAI API æ¥ç¶š
# MAGIC - Azure OpenAI API æ¥ç¶š
# MAGIC - Anthropic API æ¥ç¶š
# MAGIC - å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

# COMMAND ----------

def _call_databricks_llm(prompt: str) -> str:
    """Databricks Model Serving APIã‚’å‘¼ã³å‡ºã™"""
    try:
        # Databricksãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—
        try:
            token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
        except Exception:
            token = os.environ.get('DATABRICKS_TOKEN')
            if not token:
                return "âŒ Databricksãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç’°å¢ƒå¤‰æ•°DATABRICKS_TOKENã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        
        # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹URLã®å–å¾—
        try:
            workspace_url = spark.conf.get("spark.databricks.workspaceUrl")
        except Exception:
            workspace_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get("browserHostName").get()
        
        config = LLM_CONFIG["databricks"]
        endpoint_url = f"https://{workspace_url}/serving-endpoints/{config['endpoint_name']}/invocations"
        
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        # æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ãªå ´åˆã¯è¿½åŠ 
        if config.get("thinking_enabled", False):
            payload["thinking"] = {
                "type": "enabled",
                "budget_tokens": config.get("thinking_budget_tokens", 65536)
            }
        
        # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ï¼ˆSQLæœ€é©åŒ–ç”¨ã«å¢—å¼·ï¼‰
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    print(f"ğŸ”„ ãƒªãƒˆãƒ©ã‚¤ä¸­... (è©¦è¡Œ {attempt + 1}/{max_retries})")
                
                response = requests.post(endpoint_url, headers=headers, json=payload, timeout=300)
                
                if response.status_code == 200:
                    result = response.json()
                    analysis_text = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                    print("âœ… ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
                    return analysis_text
                else:
                    error_msg = f"APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}"
                    if response.status_code == 400:
                        # 400ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯è©³ç´°ãªè§£æ±ºç­–ã‚’æä¾›
                        error_detail = response.text
                        if "maximum tokens" in error_detail.lower():
                            if attempt == max_retries - 1:
                                detailed_error = f"""âŒ {error_msg}

ğŸ”§ ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã‚¨ãƒ©ãƒ¼ã®è§£æ±ºç­–:
1. LLM_CONFIG["databricks"]["max_tokens"] ã‚’ 65536 (64K) ã«å‰Šæ¸›
2. ã‚ˆã‚Šå˜ç´”ãªã‚¯ã‚¨ãƒªã§å†è©¦è¡Œ
3. æ‰‹å‹•ã§SQLæœ€é©åŒ–ã‚’å®Ÿè¡Œ
4. ã‚¯ã‚¨ãƒªã‚’åˆ†å‰²ã—ã¦æ®µéšçš„ã«æœ€é©åŒ–

ğŸ’¡ æ¨å¥¨è¨­å®š:
LLM_CONFIG["databricks"]["max_tokens"] = 65536
LLM_CONFIG["databricks"]["thinking_budget_tokens"] = 32768

è©³ç´°ã‚¨ãƒ©ãƒ¼: {error_detail}"""
                                print(detailed_error)
                                return detailed_error
                            else:
                                print(f"âš ï¸ {error_msg} (ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™) - ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                                continue
                    
                    if attempt == max_retries - 1:
                        print(f"âŒ {error_msg}\nãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text}")
                        return f"{error_msg}\nãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text}"
                    else:
                        print(f"âš ï¸ {error_msg} - ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                        continue
                        
            except requests.exceptions.Timeout:
                if attempt == max_retries - 1:
                    timeout_msg = f"""â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼: Databricksã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®å¿œç­”ãŒ300ç§’ä»¥å†…ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸã€‚

ğŸ”§ è§£æ±ºç­–:
1. LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ç¨¼åƒçŠ¶æ³ã‚’ç¢ºèª
2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
3. ã‚ˆã‚Šé«˜æ€§èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
4. æ‰‹å‹•ã§SQLæœ€é©åŒ–ã‚’å®Ÿè¡Œ

ğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:
- ã‚¯ã‚¨ãƒªã®è¤‡é›‘åº¦ã‚’ç¢ºèª
- Databricks Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
- ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¯ã‚¨ãƒªã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
                    print(f"âŒ {timeout_msg}")
                    return timeout_msg
                else:
                    print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç™ºç”Ÿï¼ˆ300ç§’ï¼‰- ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™... (è©¦è¡Œ {attempt + 1}/{max_retries})")
                    continue
                    
    except Exception as e:
        return f"Databricks APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}"

def _call_openai_llm(prompt: str) -> str:
    """OpenAI APIã‚’å‘¼ã³å‡ºã™"""
    try:
        config = LLM_CONFIG["openai"]
        api_key = config["api_key"] or os.environ.get('OPENAI_API_KEY')
        
        if not api_key:
            return "âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚LLM_CONFIG['openai']['api_key']ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°OPENAI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": config["model"],
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        response = requests.post("https://api.openai.com/v1/chat/completions", 
                               headers=headers, json=payload, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['choices'][0]['message']['content']
            print("âœ… OpenAIåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
            return analysis_text
        else:
            return f"OpenAI APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"OpenAI APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}"

def _call_azure_openai_llm(prompt: str) -> str:
    """Azure OpenAI APIã‚’å‘¼ã³å‡ºã™"""
    try:
        config = LLM_CONFIG["azure_openai"]
        api_key = config["api_key"] or os.environ.get('AZURE_OPENAI_API_KEY')
        
        if not api_key or not config["endpoint"] or not config["deployment_name"]:
            return "âŒ Azure OpenAIè¨­å®šãŒä¸å®Œå…¨ã§ã™ã€‚api_keyã€endpointã€deployment_nameã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        
        endpoint_url = f"{config['endpoint']}/openai/deployments/{config['deployment_name']}/chat/completions?api-version={config['api_version']}"
        
        headers = {
            "api-key": api_key,
            "Content-Type": "application/json"
        }
        
        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        response = requests.post(endpoint_url, headers=headers, json=payload, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['choices'][0]['message']['content']
            print("âœ… Azure OpenAIåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
            return analysis_text
        else:
            return f"Azure OpenAI APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"Azure OpenAI APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}"

def _call_anthropic_llm(prompt: str) -> str:
    """Anthropic APIã‚’å‘¼ã³å‡ºã™"""
    try:
        config = LLM_CONFIG["anthropic"]
        api_key = config["api_key"] or os.environ.get('ANTHROPIC_API_KEY')
        
        if not api_key:
            return "âŒ Anthropic APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚LLM_CONFIG['anthropic']['api_key']ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ANTHROPIC_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        
        headers = {
            "x-api-key": api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        payload = {
            "model": config["model"],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"],
            "messages": [{"role": "user", "content": prompt}]
        }
        
        response = requests.post("https://api.anthropic.com/v1/messages", 
                               headers=headers, json=payload, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['content'][0]['text']
            print("âœ… Anthropicåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
            return analysis_text
        else:
            return f"Anthropic APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"Anthropic APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}"

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: analyze_bottlenecks_with_llm")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“‹ LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Ÿè¡Œã®æº–å‚™
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç¢ºèªã¨è¡¨ç¤º
# MAGIC - åˆ†æé–‹å§‹ã®æº–å‚™ã¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
# MAGIC - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ã«ã‚ˆã‚‹å®‰å®šæ€§å‘ä¸Š

# COMMAND ----------

# LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Ÿè¡Œã®æº–å‚™
provider = LLM_CONFIG["provider"]

print(f"\nğŸ¤– ã€{provider.upper()} LLM ã«ã‚ˆã‚‹ SQLãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™ã€‘")
print("=" * 80)

if provider == "databricks":
    endpoint = LLM_CONFIG["databricks"]["endpoint_name"]
    print(f"ğŸ”— Databricks Model Serving ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: {endpoint}")
    print("âš ï¸  Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒç¨¼åƒä¸­ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
elif provider == "openai":
    model = LLM_CONFIG["openai"]["model"]
    print(f"ğŸ”— OpenAI ãƒ¢ãƒ‡ãƒ«: {model}")
    print("âš ï¸  OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™")
elif provider == "azure_openai":
    deployment = LLM_CONFIG["azure_openai"]["deployment_name"]
    print(f"ğŸ¤– Azure OpenAI ({deployment}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print("âš ï¸  Azure OpenAI APIã‚­ãƒ¼ã¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒå¿…è¦ã§ã™")
elif provider == "anthropic":
    model = LLM_CONFIG["anthropic"]["model"]
    print(f"ğŸ¤– Anthropic ({model}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print("âš ï¸  Anthropic APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™")

print("ğŸ“ åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç°¡æ½”åŒ–ã—ã¦ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒªã‚¹ã‚¯ã‚’è»½æ¸›ã—ã¦ã„ã¾ã™...")
print()

# extracted_metricså¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
try:
    extracted_metrics
    print("âœ… extracted_metricså¤‰æ•°ãŒç¢ºèªã•ã‚Œã¾ã—ãŸ")
    analysis_result = analyze_bottlenecks_with_llm(extracted_metrics)
except NameError:
    print("âŒ extracted_metricså¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("âš ï¸ ã‚»ãƒ«12 (ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º) ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„")
    print("ğŸ“‹ æ­£ã—ã„å®Ÿè¡Œé †åº: ã‚»ãƒ«11 â†’ ã‚»ãƒ«12 â†’ ã‚»ãƒ«15")
    print("ğŸ”„ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®åˆ†æçµæœã‚’è¨­å®šã—ã¾ã™")
    analysis_result = """
ğŸ¤– LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ

âŒ åˆ†æã«å¿…è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚

ğŸ“‹ è§£æ±ºæ–¹æ³•:
1. ã‚»ãƒ«11ã§JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
2. ã‚»ãƒ«12ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã™ã‚‹
3. ã“ã®ã‚»ãƒ«ï¼ˆã‚»ãƒ«15ï¼‰ã‚’å†å®Ÿè¡Œã™ã‚‹

âš ï¸ å…ˆã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºã‚’å®Œäº†ã—ã¦ã‹ã‚‰åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
"""
except Exception as e:
    print(f"âŒ LLMåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    analysis_result = f"LLMåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"

# COMMAND ----------

# MAGIC %md
# MAGIC # ğŸš€ ãƒ¡ã‚¤ãƒ³å‡¦ç†å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³
# MAGIC
# MAGIC **ã“ã“ã‹ã‚‰ãƒ¡ã‚¤ãƒ³ã®åˆ†æå‡¦ç†ãŒé–‹å§‹ã•ã‚Œã¾ã™**
# MAGIC
# MAGIC ğŸ“‹ **å®Ÿè¡Œæ‰‹é †:**
# MAGIC 1. ä¸Šè¨˜ã®ğŸ”§è¨­å®šãƒ»æº–å‚™ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã™ã¹ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„
# MAGIC 2. ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œã—ã¦åˆ†æã‚’è¡Œã„ã¾ã™
# MAGIC 3. ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€è¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„
# MAGIC
# MAGIC âš ï¸ **æ³¨æ„:**
# MAGIC - ğŸ”§è¨­å®šãƒ»æº–å‚™ã‚»ã‚¯ã‚·ãƒ§ãƒ³ â†’ ğŸš€ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚»ã‚¯ã‚·ãƒ§ãƒ³ â†’ ğŸ”§SQLæœ€é©åŒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ã®é †åºã§å®Ÿè¡Œ
# MAGIC - ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¨­å®šã¯å¿…ãšæœ€åˆã®ã‚»ãƒ«ã§è¡Œã£ã¦ãã ã•ã„
# MAGIC - LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸš€ SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Ÿè¡Œ
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - è¨­å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
# MAGIC - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨åŸºæœ¬æƒ…å ±ã®è¡¨ç¤º
# MAGIC - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨å‡¦ç†åœæ­¢åˆ¶å¾¡

# COMMAND ----------

print("=" * 80)
print("ğŸš€ Databricks SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆ†æãƒ„ãƒ¼ãƒ«")
print("=" * 80)
print(f"ğŸ“ åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {JSON_FILE_PATH}")
print()

# SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
profiler_data = load_profiler_json(JSON_FILE_PATH)
if not profiler_data:
    print("âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    print("âš ï¸ å‡¦ç†ã‚’åœæ­¢ã—ã¾ã™ã€‚")
    # dbutils.notebook.exit("File loading failed")  # å®‰å…¨ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    raise RuntimeError("JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºã¨æ¦‚è¦è¡¨ç¤º
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º
# MAGIC - ã‚¯ã‚¨ãƒªåŸºæœ¬æƒ…å ±ã®è¡¨ç¤º
# MAGIC - å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®è¨ˆç®—ã¨è¡¨ç¤º
# MAGIC - Liquid Clusteringã®åˆ†æçµæœè¡¨ç¤º

# COMMAND ----------

# ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º
extracted_metrics = extract_performance_metrics(profiler_data)
print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")

# æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¦‚è¦è¡¨ç¤º
print("\n" + "=" * 50)
print("ğŸ“ˆ æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¦‚è¦")
print("=" * 50)

query_info = extracted_metrics['query_info']
overall_metrics = extracted_metrics['overall_metrics']
bottleneck_indicators = extracted_metrics['bottleneck_indicators']

print(f"ğŸ†” ã‚¯ã‚¨ãƒªID: {query_info['query_id']}")
print(f"ğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {query_info['status']}")
print(f"ğŸ‘¤ å®Ÿè¡Œãƒ¦ãƒ¼ã‚¶ãƒ¼: {query_info['user']}")
print(f"â±ï¸ å®Ÿè¡Œæ™‚é–“: {overall_metrics['total_time_ms']:,} ms ({overall_metrics['total_time_ms']/1000:.2f} sec)")
print(f"ğŸ’¾ èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿: {overall_metrics['read_bytes']/1024/1024/1024:.2f} GB")
print(f"ğŸ“ˆ å‡ºåŠ›è¡Œæ•°: {overall_metrics['rows_produced_count']:,} è¡Œ")
print(f"ğŸ“‰ èª­ã¿è¾¼ã¿è¡Œæ•°: {overall_metrics['rows_read_count']:,} è¡Œ")
print(f"ğŸ¯ ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§: {bottleneck_indicators.get('data_selectivity', 0):.4f} ({bottleneck_indicators.get('data_selectivity', 0)*100:.2f}%)")
print(f"ğŸ”§ ã‚¹ãƒ†ãƒ¼ã‚¸æ•°: {len(extracted_metrics['stage_metrics'])}")
print(f"ğŸ—ï¸ ãƒãƒ¼ãƒ‰æ•°: {len(extracted_metrics['node_metrics'])}")

# Liquid Clusteringåˆ†æçµæœã®è¡¨ç¤º
liquid_analysis = extracted_metrics['liquid_clustering_analysis']
liquid_summary = liquid_analysis.get('summary', {})
print(f"ğŸ—‚ï¸ Liquid Clusteringå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {liquid_summary.get('tables_identified', 0)}")
print(f"ğŸ“Š é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {liquid_summary.get('high_impact_tables', 0)}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ” ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™è©³ç´°
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - Photon ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆ©ç”¨çŠ¶æ³ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
# MAGIC - ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œã¨ä¸¦åˆ—åº¦ã®å•é¡Œæ¤œå‡º
# MAGIC - å„ç¨®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®è©³ç´°è¡¨ç¤º

# COMMAND ----------

# ğŸ“‹ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è©³ç´°è¡¨ç¤º
print("\n" + "=" * 50)
print("ğŸ” ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™è©³ç´°")
print("=" * 50)

# Photoné–¢é€£æŒ‡æ¨™
photon_enabled = overall_metrics.get('photon_enabled', False)
photon_utilization_ratio = overall_metrics.get('photon_utilization_ratio', 0)
photon_utilization = min(photon_utilization_ratio * 100, 100.0)  # æœ€å¤§100%ã«åˆ¶é™
photon_emoji = "âœ…" if photon_enabled and photon_utilization > 80 else "âš ï¸" if photon_enabled else "âŒ"

# åˆ©ç”¨ç‡ã«é–¢ã™ã‚‹è©³ç´°æƒ…å ±
if photon_enabled:
    photon_total_ms = overall_metrics.get('photon_total_time_ms', 0)
    task_total_ms = overall_metrics.get('task_total_time_ms', 0)
    print(f"{photon_emoji} Photonã‚¨ãƒ³ã‚¸ãƒ³: æœ‰åŠ¹ (åˆ©ç”¨ç‡: {photon_utilization:.1f}%)")
    print(f"   ğŸ“Š Photonå®Ÿè¡Œæ™‚é–“: {photon_total_ms:,} ms | ã‚¿ã‚¹ã‚¯åˆè¨ˆæ™‚é–“: {task_total_ms:,} ms")
else:
    print(f"{photon_emoji} Photonã‚¨ãƒ³ã‚¸ãƒ³: ç„¡åŠ¹")

# ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«é–¢é€£æŒ‡æ¨™
shuffle_count = bottleneck_indicators.get('shuffle_operations_count', 0)
has_shuffle_bottleneck = bottleneck_indicators.get('has_shuffle_bottleneck', False)
has_low_parallelism = bottleneck_indicators.get('has_low_parallelism', False)
low_parallelism_count = bottleneck_indicators.get('low_parallelism_stages_count', 0)

shuffle_emoji = "ğŸš¨" if has_shuffle_bottleneck else "âš ï¸" if shuffle_count > 5 else "âœ…"
print(f"{shuffle_emoji} ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ: {shuffle_count}å› ({'ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚ã‚Š' if has_shuffle_bottleneck else 'æ­£å¸¸'})")

parallelism_emoji = "ğŸš¨" if has_low_parallelism else "âœ…"
print(f"{parallelism_emoji} ä¸¦åˆ—åº¦: {'å•é¡Œã‚ã‚Š' if has_low_parallelism else 'é©åˆ‡'} (ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸: {low_parallelism_count}å€‹)")

print()
print("ğŸ“Š ãã®ä»–ã®æŒ‡æ¨™:")

for key, value in bottleneck_indicators.items():
    # æ–°ã—ãè¿½åŠ ã—ãŸæŒ‡æ¨™ã¯ä¸Šè¨˜ã§è¡¨ç¤ºæ¸ˆã¿ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
    if key in ['shuffle_operations_count', 'has_shuffle_bottleneck', 'has_low_parallelism', 
               'low_parallelism_stages_count', 'total_shuffle_time_ms', 'shuffle_time_ratio',
               'slowest_shuffle_duration_ms', 'slowest_shuffle_node', 'low_parallelism_details',
               'average_low_parallelism']:
        continue
        
    if 'ratio' in key:
        emoji = "ğŸ“Š" if value < 0.1 else "âš ï¸" if value < 0.3 else "ğŸš¨"
        print(f"{emoji} {key}: {value:.3f} ({value*100:.1f}%)")
    elif 'bytes' in key and key != 'has_spill':
        if value > 0:
            emoji = "ğŸ’¾" if value < 1024*1024*1024 else "âš ï¸"  # 1GBæœªæº€ã¯æ™®é€šã€ä»¥ä¸Šã¯æ³¨æ„
            print(f"{emoji} {key}: {value:,} bytes ({value/1024/1024:.2f} MB)")
    elif key == 'has_spill':
        emoji = "âŒ" if not value else "âš ï¸"
        print(f"{emoji} {key}: {'ã‚ã‚Š' if value else 'ãªã—'}")
    elif 'duration' in key:
        emoji = "â±ï¸"
        print(f"{emoji} {key}: {value:,} ms ({value/1000:.2f} sec)")
    else:
        emoji = "â„¹ï¸"
        print(f"{emoji} {key}: {value}")

print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ’¾ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜ã¨æ™‚é–“æ¶ˆè²»åˆ†æ
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - æŠ½å‡ºã—ãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®JSONå½¢å¼ã§ã®ä¿å­˜
# MAGIC - setå‹ã‹ã‚‰listå‹ã¸ã®å¤‰æ›å‡¦ç†
# MAGIC - æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10ã®è©³ç´°åˆ†æ
# MAGIC - ç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã‚¹ãƒ”ãƒ«æ¤œå‡ºã¨AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼åˆ†æ
# MAGIC
# MAGIC ğŸ’¿ **ã‚¹ãƒ”ãƒ«æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯**:
# MAGIC - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹: `"Sink - Num bytes spilled to disk due to memory pressure"`
# MAGIC - åˆ¤å®šæ¡ä»¶: ä¸Šè¨˜ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å€¤ > 0 ã®å ´åˆã«ã‚¹ãƒ”ãƒ«ã‚ã‚Šã¨åˆ¤å®š
# MAGIC - æ¤œç´¢å¯¾è±¡: detailed_metrics â†’ raw_metrics â†’ key_metrics ã®é †åºã§æ¤œç´¢
# MAGIC
# MAGIC ğŸ¯ **ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯**:
# MAGIC - `AQEShuffleRead - Number of skewed partitions`: AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º
# MAGIC - åˆ¤å®šæ¡ä»¶: ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤ > 0 ã§ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®š
# MAGIC - é‡è¦åº¦: æ¤œå‡ºå€¤ã«åŸºã¥ã„ãŸåˆ¤å®š
# MAGIC - çµ±è¨ˆãƒ™ãƒ¼ã‚¹åˆ¤å®šã¯éæ¨å¥¨ï¼ˆAQEãƒ™ãƒ¼ã‚¹åˆ¤å®šã‚’æ¨å¥¨ï¼‰
# MAGIC
# MAGIC ğŸ’¡ **ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰**: ã‚¹ãƒ”ãƒ«ãƒ»ã‚¹ã‚­ãƒ¥ãƒ¼ã®åˆ¤å®šæ ¹æ‹ ã‚’è©³ç´°è¡¨ç¤ºã—ãŸã„å ´åˆ
# MAGIC ```python
# MAGIC import os
# MAGIC os.environ['DEBUG_SPILL_ANALYSIS'] = 'true'   # ç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¹ãƒ”ãƒ«åˆ¤å®šã®è©³ç´°è¡¨ç¤º
# MAGIC os.environ['DEBUG_SKEW_ANALYSIS'] = 'true'    # AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®šã®è©³ç´°è¡¨ç¤º
# MAGIC ```

# COMMAND ----------

# ğŸ› ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# 
# **ã‚¹ãƒ”ãƒ«ãƒ»ã‚¹ã‚­ãƒ¥ãƒ¼ã®åˆ¤å®šæ ¹æ‹ ã‚’è©³ç´°è¡¨ç¤ºã—ãŸã„å ´åˆã®ã¿å®Ÿè¡Œã—ã¦ãã ã•ã„**
# 
# ğŸ“‹ è¨­å®šå†…å®¹:
# - DEBUG_SPILL_ANALYSIS=true: ç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¹ãƒ”ãƒ«åˆ¤å®šã®è©³ç´°æ ¹æ‹ ã‚’è¡¨ç¤º
# - DEBUG_SKEW_ANALYSIS=true: AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®šã®è©³ç´°æ ¹æ‹ ã‚’è¡¨ç¤º
# 
# ğŸ’¿ ã‚¹ãƒ”ãƒ«ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºå†…å®¹:
# - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹: "Sink - Num bytes spilled to disk due to memory pressure"
# - å„ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆdetailed_metrics, raw_metrics, key_metricsï¼‰ã§ã®æ¤œç´¢çµæœ
# - ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç™ºè¦‹æ™‚ã®å€¤ã¨åˆ¤å®šçµæœ
# - ãã®ä»–ã®ã‚¹ãƒ”ãƒ«é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¸€è¦§ï¼ˆå‚è€ƒæƒ…å ±ï¼‰
# 
# ğŸ¯ ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºå†…å®¹:
# - AQEShuffleRead - Number of skewed partitions ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤
# - AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã®åˆ¤å®šæ ¹æ‹ 
# - æ¤œå‡ºã•ã‚ŒãŸã‚¹ã‚­ãƒ¥ãƒ¼æ•°ã¨é‡è¦åº¦ãƒ¬ãƒ™ãƒ«
# - çµ±è¨ˆãƒ™ãƒ¼ã‚¹åˆ¤å®šã¯éæ¨å¥¨ï¼ˆAQEãƒ™ãƒ¼ã‚¹åˆ¤å®šã‚’æ¨å¥¨ï¼‰

import os

# ç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¹ãƒ”ãƒ«åˆ†æã®ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºã‚’æœ‰åŠ¹ã«ã™ã‚‹å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’è§£é™¤
# os.environ['DEBUG_SPILL_ANALYSIS'] = 'true'

# AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼åˆ†æã®ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºã‚’æœ‰åŠ¹ã«ã™ã‚‹å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’è§£é™¤  
# os.environ['DEBUG_SKEW_ANALYSIS'] = 'true'

print("ğŸ› ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰è¨­å®š:")
print(f"   ç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¹ãƒ”ãƒ«åˆ†æãƒ‡ãƒãƒƒã‚°: {os.environ.get('DEBUG_SPILL_ANALYSIS', 'false')}")
print(f"   AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼åˆ†æãƒ‡ãƒãƒƒã‚°: {os.environ.get('DEBUG_SKEW_ANALYSIS', 'false')}")
print("   â€» 'true'ã«è¨­å®šã™ã‚‹ã¨åˆ¤å®šæ ¹æ‹ ã®è©³ç´°æƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œã¾ã™")
print()
print("ğŸ’¿ ç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¹ãƒ”ãƒ«æ¤œå‡ºåŸºæº–:")
print('   ğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: "Sink - Num bytes spilled to disk due to memory pressure"')
print("   âœ… åˆ¤å®šæ¡ä»¶: å€¤ > 0")
print()
print("ğŸ¯ AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºåŸºæº–:")
print("   ğŸ“Š AQEShuffleRead - Number of skewed partitions > 0")
print("   ğŸ“Š åˆ¤å®šæ¡ä»¶: ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤ > 0")
print("   ğŸ“Š é‡è¦åº¦: æ¤œå‡ºå€¤ã«åŸºã¥ã")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸŒ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - æŠ½å‡ºã—ãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®JSONå½¢å¼ã§ã®ä¿å­˜
# MAGIC - setå‹ã‹ã‚‰listå‹ã¸ã®å¤‰æ›å‡¦ç†
# MAGIC - æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10ã®è©³ç´°åˆ†æ
# MAGIC - ã‚¹ãƒ”ãƒ«æ¤œå‡ºã¨ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼åˆ†æ
# MAGIC - Sparkã‚¹ãƒ†ãƒ¼ã‚¸å®Ÿè¡Œåˆ†æ

# COMMAND ----------

# ğŸ’¾ æŠ½å‡ºã—ãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã¯é™¤å¤–ï¼ˆä¸è¦ï¼‰
def format_thinking_response(response) -> str:
    """
    thinking_enabled: Trueã®å ´åˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’äººé–“ã«èª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›
    æ€è€ƒéç¨‹ï¼ˆthinkingï¼‰ã¨ã‚·ã‚°ãƒãƒãƒ£ï¼ˆsignatureï¼‰ç­‰ã®ä¸è¦ãªæƒ…å ±ã¯é™¤å¤–ã—ã€æœ€çµ‚çš„ãªçµè«–ã®ã¿ã‚’è¡¨ç¤º
    JSONæ§‹é€ ã‚„ä¸é©åˆ‡ãªæ–‡å­—åˆ—ã®éœ²å‡ºã‚’é˜²æ­¢
    """
    import re  # reãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
    
    if not isinstance(response, list):
        # ãƒªã‚¹ãƒˆã§ãªã„å ´åˆã¯æ–‡å­—åˆ—ã¨ã—ã¦å‡¦ç†ã—ã€ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        cleaned_text = clean_response_text(str(response))
        return cleaned_text
    
    # é™¤å¤–ã™ã¹ãã‚­ãƒ¼ã®ãƒªã‚¹ãƒˆï¼ˆæ‹¡å¼µï¼‰
    excluded_keys = {
        'thinking', 'signature', 'metadata', 'id', 'request_id', 
        'timestamp', 'uuid', 'reasoning', 'type', 'model'
    }
    
    formatted_parts = []
    
    for item in response:
        if isinstance(item, dict):
            # æœ€ã‚‚é©åˆ‡ãªãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            content = extract_best_content_from_dict(item, excluded_keys)
            if content:
                cleaned_content = clean_response_text(content)
                if is_valid_content(cleaned_content):
                    formatted_parts.append(cleaned_content)
        else:
            # è¾æ›¸ã§ãªã„å ´åˆã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            cleaned_content = clean_response_text(str(item))
            if is_valid_content(cleaned_content):
                formatted_parts.append(cleaned_content)
    
    final_result = '\n'.join(formatted_parts)
    
    # æœ€çµ‚çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    final_result = final_quality_check(final_result)
    
    return final_result

def extract_best_content_from_dict(item_dict, excluded_keys):
    """è¾æ›¸ã‹ã‚‰æœ€é©ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º"""
    # å„ªå…ˆé †ä½: text > summary_text > content > message > ãã®ä»–
    priority_keys = ['text', 'summary_text', 'content', 'message', 'response']
    
    for key in priority_keys:
        if key in item_dict and item_dict[key]:
            content = str(item_dict[key])
            # JSONæ§‹é€ ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
            if not looks_like_json_structure(content):
                return content
    
    # å„ªå…ˆã‚­ãƒ¼ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ä»–ã®ã‚­ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆé™¤å¤–ã‚­ãƒ¼ä»¥å¤–ï¼‰
    for key, value in item_dict.items():
        if key not in excluded_keys and value and isinstance(value, str):
            if not looks_like_json_structure(value):
                return value
    
    return None

def looks_like_json_structure(text):
    """ãƒ†ã‚­ã‚¹ãƒˆãŒJSONæ§‹é€ ã‚’å«ã‚“ã§ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    json_indicators = [
        "{'type':", '[{\'type\':', '{"type":', '[{"type":',
        "'text':", '"text":', "'summary_text':", '"summary_text":',
        'reasoning', 'metadata', 'signature'
    ]
    text_lower = text.lower()
    return any(indicator.lower() in text_lower for indicator in json_indicators)

def clean_response_text(text):
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    import re
    
    if not text or not isinstance(text, str):
        return ""
    
    # æ”¹è¡Œã‚³ãƒ¼ãƒ‰ã®æ­£è¦åŒ–
    text = text.replace('\\n', '\n').replace('\\t', '\t')
    
    # JSONæ§‹é€ ã®é™¤å»
    
    # å…¸å‹çš„ãªJSONæ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å»
    json_patterns = [
        r"'type':\s*'[^']*'",
        r'"type":\s*"[^"]*"',
        r"\[?\{'type':[^}]*\}[,\]]?",
        r'\[?\{"type":[^}]*\}[,\]]?',
        r"'reasoning':\s*\[[^\]]*\]",
        r'"reasoning":\s*\[[^\]]*\]',
        r"'signature':\s*'[A-Za-z0-9+/=]{50,}'",
        r'"signature":\s*"[A-Za-z0-9+/=]{50,}"'
    ]
    
    for pattern in json_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)
    
    # ä¸å®Œå…¨ãªJSONãƒ–ãƒ©ã‚±ãƒƒãƒˆã®é™¤å»
    text = re.sub(r'^\s*[\[\{]', '', text)  # å…ˆé ­ã® [ ã‚„ {
    text = re.sub(r'[\]\}]\s*$', '', text)  # æœ«å°¾ã® ] ã‚„ }
    text = re.sub(r'^\s*[,;]\s*', '', text)  # å…ˆé ­ã®ã‚«ãƒ³ãƒã‚„ã‚»ãƒŸã‚³ãƒ­ãƒ³
    
    # é€£ç¶šã™ã‚‹ç©ºç™½ãƒ»æ”¹è¡Œã®æ­£è¦åŒ–
    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)  # 3ã¤ä»¥ä¸Šã®é€£ç¶šæ”¹è¡Œã‚’2ã¤ã«
    text = re.sub(r'[ \t]+', ' ', text)  # é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ãƒ»ã‚¿ãƒ–ã‚’1ã¤ã«
    
    # å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
    text = text.strip()
    
    return text

def is_valid_content(text):
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    import re
    
    if not text or len(text.strip()) < 10:
        return False
    
    # ç„¡åŠ¹ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
    invalid_patterns = [
        r'^[{\[\'"]*$',  # JSONæ§‹é€ ã®ã¿
        r'^[,;:\s]*$',   # åŒºåˆ‡ã‚Šæ–‡å­—ã®ã¿
        r'^\s*reasoning\s*$',  # reasoningã®ã¿
        r'^\s*metadata\s*$',   # metadataã®ã¿
        r'^[A-Za-z0-9+/=]{50,}$',  # Base64ã£ã½ã„é•·ã„æ–‡å­—åˆ—
    ]
    
    for pattern in invalid_patterns:
        if re.match(pattern, text.strip(), re.IGNORECASE):
            return False
    
    return True

def final_quality_check(text):
    """æœ€çµ‚çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    import re  # reãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
    
    if not text:
        return "åˆ†æçµæœã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
    
    # è¨€èªã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå®‰å…¨ãªå¤‰æ•°ã‚¢ã‚¯ã‚»ã‚¹ï¼‰
    try:
        language = globals().get('OUTPUT_LANGUAGE', 'ja')  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ—¥æœ¬èª
    except:
        language = 'ja'
    
    if language == 'ja':
        text = ensure_japanese_consistency(text)
    elif language == 'en':
        text = ensure_english_consistency(text)
    
    # æœ€å°é™ã®é•·ã•ãƒã‚§ãƒƒã‚¯
    if len(text.strip()) < 20:
        if language == 'ja':
            return "åˆ†æçµæœãŒä¸å®Œå…¨ã§ã™ã€‚è©³ç´°ãªåˆ†æã‚’å®Ÿè¡Œä¸­ã§ã™ã€‚"
        else:
            return "Analysis result is incomplete. Detailed analysis in progress."
    
    return text

def ensure_japanese_consistency(text):
    """æ—¥æœ¬èªã®ä¸€è²«æ€§ã‚’ç¢ºä¿"""
    import re
    
    # æ˜ã‚‰ã‹ã«ç ´æã—ã¦ã„ã‚‹éƒ¨åˆ†ã‚’é™¤å»
    # ä¾‹: "æ­£caientify="predicate_liquid_referencet1" ã®ã‚ˆã†ãªç ´ææ–‡å­—åˆ—
    text = re.sub(r'[a-zA-Z0-9_="\']{20,}', '', text)
    
    # ä¸å®Œå…¨ãªãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ä¿®æ­£
    text = re.sub(r'#\s*[^#\n]*["\'>]+[^#\n]*', '', text)  # ç ´æã—ãŸãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼
    
    # æ„å‘³ä¸æ˜ãªæ–‡å­—åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é™¤å»ï¼ˆæ‹¡å¼µï¼‰
    nonsense_patterns = [
        r'addressing_sales_column\d*',
        r'predicate_liquid_reference[a-zA-Z0-9]*',
        r'bottlenars\s+effect',
        r'å®Ÿè£…éä¿å­˜åœ¨',
        r'è£ç¥¨ã®end_by',
        r'riconsistall',
        r'caientify[a-zA-Z0-9="\']*',
        r'iving\s+[a-zA-Z0-9]*',
        r'o\s+Matteré…è³›',
        r'ubsãŒä½ã„åƒ®æ€§',
        r'åˆ°ç”°ãƒ‡ãƒ¼ã‚¿ã®æ–¹åŠ¹æ€§',
        r'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹.*topic.*é …è¡Œã«è€ƒ',
        r'ï¼»[^ï¼½]*ï¼½">[^<]*',  # ç ´æã—ãŸHTML/XMLè¦ç´ 
        r'\]\s*">\s*$'  # æ–‡æœ«ã®ç ´æã—ãŸã‚¿ã‚°
    ]
    
    for pattern in nonsense_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # é€£ç¶šã™ã‚‹è¨˜å·ã®é™¤å»
    text = re.sub(r'["\'>]{2,}', '', text)
    text = re.sub(r'[=\'"]{3,}', '', text)
    
    # ç ´æã—ãŸæ—¥æœ¬èªã®ä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³
    broken_japanese_patterns = [
        (r'ã®æ–¹æ³•å‹•çš„ãŒã‚‰', 'å‹•çš„ãªæ–¹æ³•ã§'),
        (r'æ€è€ƒã«æ²¿ã£ã¦é€²ã‚ã¦ã„ãã¾ã™ã€‚$', 'æ€è€ƒã«æ²¿ã£ã¦åˆ†æã‚’é€²ã‚ã¾ã™ã€‚'),
        (r'ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«æ²¿ã£ãŸæ”¹å–„ã‚’.*ã¾ã§ã—ã¦ã„ã‚‹ã®', 'ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«æ²¿ã£ãŸæ”¹å–„ææ¡ˆ'),
    ]
    
    for broken, fixed in broken_japanese_patterns:
        text = re.sub(broken, fixed, text, flags=re.IGNORECASE)
    
    # ç©ºè¡Œã®æ­£è¦åŒ–
    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
    
    return text.strip()

def ensure_english_consistency(text):
    """è‹±èªã®ä¸€è²«æ€§ã‚’ç¢ºä¿"""
    import re
    
    # åŒæ§˜ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’è‹±èªç”¨ã«å®Ÿè£…
    text = re.sub(r'[^\x00-\x7F\s]{10,}', '', text)  # éASCIIæ–‡å­—ã®é•·ã„é€£ç¶šã‚’é™¤å»
    
    return text.strip()

def extract_main_content_from_thinking_response(response) -> str:
    """
    thinkingå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆtextã¾ãŸã¯summary_textï¼‰ã®ã¿ã‚’æŠ½å‡º
    thinkingã€signatureç­‰ã®ä¸è¦ãªæƒ…å ±ã¯é™¤å¤–
    JSONæ§‹é€ ã‚„ç ´æã—ãŸãƒ†ã‚­ã‚¹ãƒˆã®æ··å…¥ã‚’é˜²æ­¢
    """
    if not isinstance(response, list):
        cleaned_text = clean_response_text(str(response))
        return final_quality_check(cleaned_text)
    
    # é™¤å¤–ã™ã¹ãã‚­ãƒ¼
    excluded_keys = {
        'thinking', 'signature', 'metadata', 'id', 'request_id', 
        'timestamp', 'uuid', 'reasoning', 'type', 'model'
    }
    
    for item in response:
        if isinstance(item, dict):
            # æœ€é©ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            content = extract_best_content_from_dict(item, excluded_keys)
            if content:
                cleaned_content = clean_response_text(content)
                if is_valid_content(cleaned_content):
                    return final_quality_check(cleaned_content)
    
    # ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…¨ä½“ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    return format_thinking_response(response)

def convert_sets_to_lists(obj):
    """setå‹ã‚’listå‹ã«å¤‰æ›ã—ã¦JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ã«ã™ã‚‹"""
    if isinstance(obj, set):
        return list(obj)
    elif isinstance(obj, dict):
        return {key: convert_sets_to_lists(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_sets_to_lists(item) for item in obj]
    else:
        return obj

# output_extracted_metrics ã®ç”Ÿæˆã¯é™¤å¤–ï¼ˆä¸è¦ï¼‰

# ğŸŒ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10
print(f"\nğŸŒ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10")
print("=" * 80)
print("ğŸ“Š ã‚¢ã‚¤ã‚³ãƒ³èª¬æ˜: â±ï¸æ™‚é–“ ğŸ’¾ãƒ¡ãƒ¢ãƒª ğŸ”¥ğŸŒä¸¦åˆ—åº¦ ğŸ’¿ã‚¹ãƒ”ãƒ« âš–ï¸ã‚¹ã‚­ãƒ¥ãƒ¼")
print('ğŸ’¿ ã‚¹ãƒ”ãƒ«åˆ¤å®š: "Sink - Num bytes spilled to disk due to memory pressure" > 0')
print("ğŸ¯ ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®š: 'AQEShuffleRead - Number of skewed partitions' > 0")

# ãƒãƒ¼ãƒ‰ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
sorted_nodes = sorted(extracted_metrics['node_metrics'], 
                     key=lambda x: x['key_metrics'].get('durationMs', 0), 
                     reverse=True)

if sorted_nodes:
    # å…¨ä½“ã®å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
    total_duration = sum(node['key_metrics'].get('durationMs', 0) for node in sorted_nodes)
    
    print(f"ğŸ“Š å…¨ä½“å®Ÿè¡Œæ™‚é–“: {total_duration:,} ms ({total_duration/1000:.1f} sec)")
    print(f"ğŸ“ˆ TOP10åˆè¨ˆæ™‚é–“: {sum(node['key_metrics'].get('durationMs', 0) for node in sorted_nodes[:10]):,} ms")
    print()
    
    for i, node in enumerate(sorted_nodes[:10]):
        rows_num = node['key_metrics'].get('rowsNum', 0)
        duration_ms = node['key_metrics'].get('durationMs', 0)
        memory_mb = node['key_metrics'].get('peakMemoryBytes', 0) / 1024 / 1024
        
        # å…¨ä½“ã«å¯¾ã™ã‚‹æ™‚é–“ã®å‰²åˆã‚’è¨ˆç®—
        time_percentage = (duration_ms / max(total_duration, 1)) * 100
        
        # æ™‚é–“ã®é‡è¦åº¦ã«åŸºã¥ã„ã¦ã‚¢ã‚¤ã‚³ãƒ³ã‚’é¸æŠ
        if duration_ms >= 10000:  # 10ç§’ä»¥ä¸Š
            time_icon = "ï¿½"
            severity = "CRITICAL"
        elif duration_ms >= 5000:  # 5ç§’ä»¥ä¸Š
            time_icon = "ğŸŸ "
            severity = "HIGH"
        elif duration_ms >= 1000:  # 1ç§’ä»¥ä¸Š
            time_icon = "ğŸŸ¡"
            severity = "MEDIUM"
        else:
            time_icon = "ï¿½"
            severity = "LOW"
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ã‚¢ã‚¤ã‚³ãƒ³
        memory_icon = "ï¿½" if memory_mb < 100 else "âš ï¸" if memory_mb < 1000 else "ğŸš¨"
        
        # ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹ãƒãƒ¼ãƒ‰åã‚’å–å¾—
        raw_node_name = node['name']
        node_name = get_meaningful_node_name(node, extracted_metrics)
        short_name = node_name[:100] + "..." if len(node_name) > 100 else node_name
        
        # ä¸¦åˆ—åº¦æƒ…å ±ã®å–å¾—
        num_tasks = 0
        for stage in extracted_metrics.get('stage_metrics', []):
            if duration_ms > 0:  # ã“ã®ãƒãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’æ¨å®š
                num_tasks = max(num_tasks, stage.get('num_tasks', 0))
        
        # ãƒ‡ã‚£ã‚¹ã‚¯ã‚¹ãƒ”ãƒ«ã‚¢ã‚¦ãƒˆã®æ¤œå‡ºï¼ˆãƒ¡ãƒ¢ãƒªãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã«ã‚ˆã‚‹ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹å¯¾å¿œæ”¹å–„ç‰ˆï¼‰
        spill_detected = False
        spill_bytes = 0
        spill_details = []
        
        # ã‚¹ãƒ”ãƒ«æ¤œå‡ºã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹åãƒªã‚¹ãƒˆï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
        target_spill_metrics = [
            "Sink - Num bytes spilled to disk due to memory pressure",
            "Num bytes spilled to disk due to memory pressure",
            "bytes spilled to disk due to memory pressure",
            "spilled to disk due to memory pressure"
        ]
        
        # 1. detailed_metricsã‹ã‚‰ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ¤œç´¢ï¼ˆæŸ”è»Ÿæ¤œç´¢ï¼‰
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            metric_value = metric_info.get('value', 0)
            metric_label = metric_info.get('label', '')
            
            # éƒ¨åˆ†æ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°ã§ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ¤œç´¢
            key_matches = any(target in metric_key for target in target_spill_metrics)
            label_matches = any(target in metric_label for target in target_spill_metrics)
            
            if (key_matches or label_matches) and metric_value > 0:
                spill_detected = True
                spill_bytes = metric_value
                matched_target = next((target for target in target_spill_metrics if target in metric_key or target in metric_label), "unknown")
                spill_details.append({
                    'metric_name': metric_key,
                    'value': metric_value,
                    'label': metric_label,
                    'source': 'detailed_metrics',
                    'matched_field': 'key' if key_matches else 'label',
                    'matched_pattern': matched_target
                })
                break  # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ç”¨
        
        # 2. detailed_metricsã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ç”Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰æ¤œç´¢
        if not spill_detected:
            raw_metrics = node.get('metrics', [])
            for metric in raw_metrics:
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                # éƒ¨åˆ†æ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°ã§ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ¤œç´¢
                key_matches = any(target in metric_key for target in target_spill_metrics)
                label_matches = any(target in metric_label for target in target_spill_metrics)
                
                if (key_matches or label_matches) and metric_value > 0:
                    spill_detected = True
                    spill_bytes = metric_value
                    matched_target = next((target for target in target_spill_metrics if target in metric_key or target in metric_label), "unknown")
                    spill_details.append({
                        'metric_name': metric_key,
                        'value': metric_value,
                        'label': metric_label,
                        'source': 'raw_metrics',
                        'matched_field': 'key' if key_matches else 'label',
                        'matched_pattern': matched_target
                    })
                    break  # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ç”¨
        
        # 3. key_metricsã§ã‚‚æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not spill_detected:
            key_metrics = node.get('key_metrics', {})
            for key_metric_name, key_metric_value in key_metrics.items():
                key_matches = any(target in key_metric_name for target in target_spill_metrics)
                if key_matches and key_metric_value > 0:
                    spill_detected = True
                    spill_bytes = key_metric_value
                    matched_target = next((target for target in target_spill_metrics if target in key_metric_name), "unknown")
                    spill_details.append({
                        'metric_name': f"key_metrics.{key_metric_name}",
                        'value': key_metric_value,
                        'label': f"Key metric: {key_metric_name}",
                        'source': 'key_metrics',
                        'matched_field': 'key',
                        'matched_pattern': matched_target
                    })
                    break
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ã®æ¤œå‡ºï¼ˆAQEãƒ™ãƒ¼ã‚¹ã®ç²¾å¯†åˆ¤å®šï¼‰
        skew_detected = False
        skew_details = []
        
        # AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º: "AQEShuffleRead - Number of skewed partitions" > 0
        target_aqe_metrics = [
            "AQEShuffleRead - Number of skewed partitions",
            "AQEShuffleRead - Number of skewed partition splits"
        ]
        
        aqe_skew_value = 0
        aqe_split_value = 0
        aqe_metric_name = ""
        aqe_split_metric_name = ""
        
        # 1. detailed_metricsã§æ¤œç´¢
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            if metric_key == "AQEShuffleRead - Number of skewed partitions":
                aqe_skew_value = metric_info.get('value', 0)
                aqe_metric_name = metric_key
            elif metric_key == "AQEShuffleRead - Number of skewed partition splits":
                aqe_split_value = metric_info.get('value', 0)
                aqe_split_metric_name = metric_key
            elif metric_info.get('label', '') == "AQEShuffleRead - Number of skewed partitions":
                aqe_skew_value = metric_info.get('value', 0)
                aqe_metric_name = metric_info.get('label', '')
            elif metric_info.get('label', '') == "AQEShuffleRead - Number of skewed partition splits":
                aqe_split_value = metric_info.get('value', 0)
                aqe_split_metric_name = metric_info.get('label', '')
        
        # 2. raw_metricsã§æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if aqe_skew_value == 0 or aqe_split_value == 0:
            raw_metrics = node.get('metrics', [])
            if isinstance(raw_metrics, list):
                for raw_metric in raw_metrics:
                    if isinstance(raw_metric, dict):
                        # 'label'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æœ€åˆã«ãƒã‚§ãƒƒã‚¯
                        raw_metric_label = raw_metric.get('label', '')
                        if raw_metric_label == "AQEShuffleRead - Number of skewed partitions" and aqe_skew_value == 0:
                            aqe_skew_value = raw_metric.get('value', 0)
                            aqe_metric_name = raw_metric_label
                        elif raw_metric_label == "AQEShuffleRead - Number of skewed partition splits" and aqe_split_value == 0:
                            aqe_split_value = raw_metric.get('value', 0)
                            aqe_split_metric_name = raw_metric_label
                        
                        # 'key'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚ãƒã‚§ãƒƒã‚¯
                        raw_metric_key = raw_metric.get('key', '')
                        if raw_metric_key == "AQEShuffleRead - Number of skewed partitions" and aqe_skew_value == 0:
                            aqe_skew_value = raw_metric.get('value', 0)
                            aqe_metric_name = raw_metric_key
                        elif raw_metric_key == "AQEShuffleRead - Number of skewed partition splits" and aqe_split_value == 0:
                            aqe_split_value = raw_metric.get('value', 0)
                            aqe_split_metric_name = raw_metric_key
                        
                        # 'metricName'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚ãƒã‚§ãƒƒã‚¯ï¼ˆå¾“æ¥ã®äº’æ›æ€§ï¼‰
                        raw_metric_name = raw_metric.get('metricName', '')
                        if raw_metric_name == "AQEShuffleRead - Number of skewed partitions" and aqe_skew_value == 0:
                            aqe_skew_value = raw_metric.get('value', 0)
                            aqe_metric_name = raw_metric_name
                        elif raw_metric_name == "AQEShuffleRead - Number of skewed partition splits" and aqe_split_value == 0:
                            aqe_split_value = raw_metric.get('value', 0)
                            aqe_split_metric_name = raw_metric_name
        
        # 3. key_metricsã§æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if aqe_skew_value == 0 or aqe_split_value == 0:
            key_metrics = node.get('key_metrics', {})
            for key_metric_name, key_metric_value in key_metrics.items():
                if "AQEShuffleRead - Number of skewed partitions" in key_metric_name and aqe_skew_value == 0:
                    aqe_skew_value = key_metric_value
                    aqe_metric_name = key_metric_name
                elif "AQEShuffleRead - Number of skewed partition splits" in key_metric_name and aqe_split_value == 0:
                    aqe_split_value = key_metric_value
                    aqe_split_metric_name = key_metric_name
        
        # AQEã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®š
        if aqe_skew_value > 0:
            skew_detected = True
            severity_level = "é«˜" if aqe_skew_value >= 5 else "ä¸­"
            
            # åŸºæœ¬çš„ãªAQEã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºæƒ…å ±
            description = f'AQEã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º: {aqe_metric_name} = {aqe_skew_value} > åŸºæº–å€¤ 0 [é‡è¦åº¦:{severity_level}]'
            
            # splitå€¤ã‚‚å–å¾—ã§ããŸå ´åˆã€è©³ç´°æƒ…å ±ã‚’è¿½åŠ 
            if aqe_split_value > 0:
                description += f' | AQEæ¤œå‡ºè©³ç´°: SparkãŒè‡ªå‹•çš„ã«{aqe_skew_value}å€‹ã®ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’æ¤œå‡º'
                description += f' | AQEè‡ªå‹•å¯¾å¿œ: SparkãŒè‡ªå‹•çš„ã«{aqe_split_value}å€‹ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«åˆ†å‰²'
            
            skew_details.append({
                'type': 'aqe_skew',
                'value': aqe_skew_value,
                'split_value': aqe_split_value,
                'threshold': 0,
                'metric_name': aqe_metric_name,
                'split_metric_name': aqe_split_metric_name,
                'severity': severity_level,
                'description': description
            })
        
        # 4. ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«åŸºã¥ãã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºï¼ˆãƒ¡ãƒ¢ãƒªãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã«ã‚ˆã‚‹ã‚¹ãƒ”ãƒ«ã®ä¸å‡ç­‰ã‚’æ¤œå‡ºï¼‰
        if spill_detected and spill_bytes > 0:
            # ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã€ãã‚Œè‡ªä½“ãŒã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§ã‚’ç¤ºã™
            # ç‰¹ã«å¤§ããªã‚¹ãƒ”ãƒ«ãŒç™ºç”Ÿã—ã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§ãŒé«˜ã„
            spill_mb = spill_bytes / 1024 / 1024
            if spill_mb > 100:  # 100MBä»¥ä¸Šã®ã‚¹ãƒ”ãƒ«ãŒã‚ã‚‹å ´åˆ
                skew_detected = True
                severity_level = "é«˜" if spill_mb > 1000 else "ä¸­"  # 1GBä»¥ä¸Šã¯é«˜é‡è¦åº¦
                skew_details.append({
                    'type': 'memory_pressure_spill_skew',
                    'spill_bytes': spill_bytes,
                    'spill_mb': spill_mb,
                    'threshold': 100,
                    'severity': severity_level,
                    'description': f'ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã«ã‚ˆã‚‹ã‚¹ãƒ”ãƒ«ã‚¹ã‚­ãƒ¥ãƒ¼: {spill_mb:.1f}MB ã‚¹ãƒ”ãƒ«ç™ºç”Ÿï¼ˆåŸºæº–å€¤: 100MBï¼‰ [é‡è¦åº¦:{severity_level}]'
                })
        
        # 5. ã‚¿ã‚¹ã‚¯æ•°ã¨ã‚¹ãƒ”ãƒ«ã®é–¢ä¿‚ã«ã‚ˆã‚‹ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã®å¼·åŒ–
        if spill_detected and num_tasks > 10:
            # å¤šæ•°ã®ã‚¿ã‚¹ã‚¯ãŒã‚ã‚‹ã®ã«ã‚¹ãƒ”ãƒ«ãŒç™ºç”Ÿã—ã¦ã„ã‚‹å ´åˆã€ãƒ‡ãƒ¼ã‚¿ã®ä¸å‡ç­‰åˆ†æ•£ã®å¯èƒ½æ€§
            # ã‚¿ã‚¹ã‚¯ã‚ãŸã‚Šã®ã‚¹ãƒ”ãƒ«é‡ã‚’è¨ˆç®—
            spill_per_task_mb = (spill_bytes / num_tasks) / 1024 / 1024
            if spill_per_task_mb > 10:  # ã‚¿ã‚¹ã‚¯ã‚ãŸã‚Š10MBä»¥ä¸Šã®ã‚¹ãƒ”ãƒ«
                skew_detected = True
                severity_level = "é«˜" if spill_per_task_mb > 50 else "ä¸­"
                skew_details.append({
                    'type': 'task_spill_distribution_skew',
                    'spill_per_task_mb': spill_per_task_mb,
                    'num_tasks': num_tasks,
                    'total_spill_mb': spill_bytes / 1024 / 1024,
                    'threshold': 10,
                    'severity': severity_level,
                    'description': f'ã‚¿ã‚¹ã‚¯ã‚ãŸã‚Šã‚¹ãƒ”ãƒ«é‡ã‚¹ã‚­ãƒ¥ãƒ¼: {spill_per_task_mb:.1f}MB/ã‚¿ã‚¹ã‚¯ ({num_tasks}ã‚¿ã‚¹ã‚¯ä¸­) [é‡è¦åº¦:{severity_level}]'
                })
        
        # ä¸¦åˆ—åº¦ã‚¢ã‚¤ã‚³ãƒ³
        parallelism_icon = "ğŸ”¥" if num_tasks >= 10 else "âš ï¸" if num_tasks >= 5 else "ğŸŒ"
        # ã‚¹ãƒ”ãƒ«ã‚¢ã‚¤ã‚³ãƒ³
        spill_icon = "ğŸ’¿" if spill_detected else "âœ…"
        # ã‚¹ã‚­ãƒ¥ãƒ¼ã‚¢ã‚¤ã‚³ãƒ³
        skew_icon = "âš–ï¸" if skew_detected else "âœ…"
        
        print(f"{i+1:2d}. {time_icon}{memory_icon}{parallelism_icon}{spill_icon}{skew_icon} [{severity:8}] {short_name}")
        print(f"    â±ï¸  å®Ÿè¡Œæ™‚é–“: {duration_ms:>8,} ms ({duration_ms/1000:>6.1f} sec) - å…¨ä½“ã® {time_percentage:>5.1f}%")
        print(f"    ğŸ“Š å‡¦ç†è¡Œæ•°: {rows_num:>8,} è¡Œ")
        print(f"    ğŸ’¾ ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: {memory_mb:>6.1f} MB")
        print(f"    ğŸ”§ ä¸¦åˆ—åº¦: {num_tasks:>3d} ã‚¿ã‚¹ã‚¯ | ğŸ’¿ ã‚¹ãƒ”ãƒ«: {'ã‚ã‚Š' if spill_detected else 'ãªã—'} | âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼: {'æ¤œå‡º' if skew_detected else 'ãªã—'}")
        
        # åŠ¹ç‡æ€§æŒ‡æ¨™ï¼ˆè¡Œ/ç§’ï¼‰ã‚’è¨ˆç®—
        if duration_ms > 0:
            rows_per_sec = (rows_num * 1000) / duration_ms
            print(f"    ğŸš€ å‡¦ç†åŠ¹ç‡: {rows_per_sec:>8,.0f} è¡Œ/ç§’")
        
        # ã‚¹ãƒ”ãƒ«è©³ç´°æƒ…å ±ï¼ˆãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºä»˜ãå¼·åŒ–ç‰ˆï¼‰
        if spill_detected:
            if spill_bytes > 0:
                print(f"    ğŸ’¿ ã‚¹ãƒ”ãƒ«è©³ç´°: {spill_bytes/1024/1024:.1f} MB")
            
            # ã‚¹ãƒ”ãƒ«åˆ¤å®šæ ¹æ‹ ã®è©³ç´°è¡¨ç¤ºï¼ˆç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ï¼‰
            print(f"    ğŸ” ã‚¹ãƒ”ãƒ«åˆ¤å®šæ ¹æ‹ :")
            for detail in spill_details:
                metric_name = detail['metric_name']
                value = detail['value']
                label = detail['label']
                source = detail['source']
                matched_field = detail.get('matched_field', 'unknown')
                
                if value >= 1024 * 1024 * 1024:  # GBå˜ä½
                    value_display = f"{value/1024/1024/1024:.2f} GB"
                elif value >= 1024 * 1024:  # MBå˜ä½
                    value_display = f"{value/1024/1024:.1f} MB"
                elif value >= 1024:  # KBå˜ä½
                    value_display = f"{value/1024:.1f} KB"
                else:
                    value_display = f"{value} bytes"
                
                # æ­£ç¢ºãªã‚¹ãƒ”ãƒ«å€¤ã‚’é©åˆ‡ãªå˜ä½ã§è¡¨ç¤º
                if value >= 1024 * 1024 * 1024:  # GBå˜ä½
                    value_display = f"{value/1024/1024/1024:.2f} GB"
                elif value >= 1024 * 1024:  # MBå˜ä½
                    value_display = f"{value/1024/1024:.1f} MB"
                elif value >= 1024:  # KBå˜ä½
                    value_display = f"{value/1024:.1f} KB"
                else:
                    value_display = f"{value} bytes"
                
                print(f"       ğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹: 'Num bytes spilled to disk due to memory pressure' ã¾ãŸã¯ 'Sink - Num bytes spilled to disk due to memory pressure'")
                print(f"       ğŸ“Š æ¤œå‡ºå€¤: {value_display}")
                print(f"       ğŸ” æ¤œå‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹å: {metric_name}")
                if label and label != metric_name:
                    print(f"       ğŸ·ï¸  ãƒ©ãƒ™ãƒ«: {label}")
                print(f"       âœ… åˆ¤å®š: ã‚¹ãƒ”ãƒ«ã‚ã‚Š (å€¤ > 0)")
        else:
            # ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œãªã‹ã£ãŸå ´åˆã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆè©³ç´°è¡¨ç¤ºæ™‚ã®ã¿ï¼‰
            import os
            if os.environ.get('DEBUG_SPILL_ANALYSIS', '').lower() in ['true', '1', 'yes']:
                print(f"    ğŸ” ã‚¹ãƒ”ãƒ«æœªæ¤œå‡º:")
                print(f"       ğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹: 'Num bytes spilled to disk due to memory pressure' ã¾ãŸã¯ 'Sink - Num bytes spilled to disk due to memory pressure'")
                print(f"       âŒ æ¤œå‡ºçµæœ: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€å€¤ãŒ0")
                
                # å„ã‚½ãƒ¼ã‚¹ã§ã®æ¤œç´¢çµæœã‚’è¡¨ç¤º
                detailed_metrics = node.get('detailed_metrics', {})
                raw_metrics = node.get('metrics', [])
                key_metrics = node.get('key_metrics', {})
                
                target_spill_metrics = [
                    "Sink - Num bytes spilled to disk due to memory pressure",
                    "Num bytes spilled to disk due to memory pressure"
                ]
                
                # detailed_metricsã§ã®æ¤œç´¢çµæœ
                found_in_detailed = False
                for metric_key, metric_info in detailed_metrics.items():
                    if metric_key in target_spill_metrics or metric_info.get('label', '') in target_spill_metrics:
                        found_in_detailed = True
                        value = metric_info.get('value', 0)
                        matched_metric = metric_key if metric_key in target_spill_metrics else metric_info.get('label', '')
                        print(f"       ğŸ“Š detailed_metrics: {matched_metric} è¦‹ã¤ã‹ã£ãŸãŒå€¤={value} (â‰¤ 0)")
                        break
                if not found_in_detailed:
                    print(f"       ğŸ“Š detailed_metrics: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹æœªç™ºè¦‹ ({len(detailed_metrics)}å€‹ä¸­)")
                
                # raw_metricsã§ã®æ¤œç´¢çµæœ
                found_in_raw = False
                for metric in raw_metrics:
                    metric_key = metric.get('key', '')
                    metric_label = metric.get('label', '')
                    if metric_key in target_spill_metrics or metric_label in target_spill_metrics:
                        found_in_raw = True
                        value = metric.get('value', 0)
                        matched_metric = metric_key if metric_key in target_spill_metrics else metric_label
                        print(f"       ğŸ“Š raw_metrics: {matched_metric} è¦‹ã¤ã‹ã£ãŸãŒå€¤={value} (â‰¤ 0)")
                        break
                if not found_in_raw:
                    print(f"       ğŸ“Š raw_metrics: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹æœªç™ºè¦‹ ({len(raw_metrics)}å€‹ä¸­)")
                
                # key_metricsã§ã®æ¤œç´¢çµæœ
                found_in_key = False
                for target_metric in target_spill_metrics:
                    if target_metric in key_metrics:
                        found_in_key = True
                        value = key_metrics[target_metric]
                        print(f"       ğŸ“Š key_metrics: {target_metric} è¦‹ã¤ã‹ã£ãŸãŒå€¤={value} (â‰¤ 0)")
                        break
                if not found_in_key:
                    print(f"       ğŸ“Š key_metrics: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹æœªç™ºè¦‹ ({len(key_metrics)}å€‹ä¸­)")
                
                # åˆ©ç”¨å¯èƒ½ãªã‚¹ãƒ”ãƒ«é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¸€è¦§ï¼ˆå‚è€ƒï¼‰
                spill_related = []
                for key in detailed_metrics.keys():
                    if key in target_spill_metrics:
                        spill_related.append(f"detailed_metrics.{key}")
                for metric in raw_metrics:
                    key = metric.get('key', '')
                    label = metric.get('label', '')
                    if key in target_spill_metrics or label in target_spill_metrics:
                        spill_related.append(f"raw_metrics.{key}")
                for key in key_metrics.keys():
                    if key in target_spill_metrics:
                        spill_related.append(f"key_metrics.{key}")
                
                if spill_related:
                    print(f"       ğŸ” å‚è€ƒ: ãã®ä»–ã®ã‚¹ãƒ”ãƒ«é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹ {len(spill_related)}å€‹")
                    for related in spill_related[:3]:  # æœ€å¤§3å€‹è¡¨ç¤º
                        print(f"           - {related}")
                    if len(spill_related) > 3:
                        print(f"           ... ä»–{len(spill_related) - 3}å€‹")
                else:
                    print(f"       ğŸ” å‚è€ƒ: ãã®ä»–ã®ã‚¹ãƒ”ãƒ«é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯ç™ºè¦‹ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        
        # ã‚¹ã‚­ãƒ¥ãƒ¼è©³ç´°æƒ…å ±ï¼ˆAQEãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºä»˜ãï¼‰
        if skew_detected:
            print(f"    ğŸ” ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®šæ ¹æ‹ :")
            for detail in skew_details:
                description = detail['description']
                print(f"       âš–ï¸ {description}")
                
                # ã‚ˆã‚Šè©³ç´°ãªAQEæƒ…å ±ã®è¡¨ç¤º
                if detail['type'] == 'aqe_skew':
                    aqe_value = detail['value']
                    aqe_split_value = detail.get('split_value', 0)
                    metric_name = detail['metric_name']
                    split_metric_name = detail.get('split_metric_name', '')
                    
                    print(f"           ğŸ“Š AQEãƒ™ãƒ¼ã‚¹æ¤œå‡º: {metric_name} = {aqe_value}")
                    print(f"           ğŸ¯ AQEæ¤œå‡ºè©³ç´°: SparkãŒè‡ªå‹•çš„ã«{aqe_value}å€‹ã®ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’æ¤œå‡º")
                    
                    # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†å‰²æƒ…å ±ãŒã‚ã‚‹å ´åˆ
                    if aqe_split_value > 0 and split_metric_name:
                        print(f"           âš¡ AQEè‡ªå‹•å¯¾å¿œ: SparkãŒè‡ªå‹•çš„ã«{aqe_split_value}å€‹ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«åˆ†å‰²")
                        print(f"           ğŸ”„ åˆ†å‰²ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {split_metric_name} = {aqe_split_value}")
                    
                    severity = detail.get('severity', 'ä¸­')
                    severity_emoji = "ğŸš¨" if severity == "é«˜" else "âš ï¸"
                    print(f"           {severity_emoji} é‡è¦åº¦: {severity} ({'5å€‹ä»¥ä¸Š' if severity == 'é«˜' else '1-4å€‹'}ã®ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³)")
        else:
            # ã‚¹ã‚­ãƒ¥ãƒ¼ãŒæ¤œå‡ºã•ã‚Œãªã‹ã£ãŸå ´åˆã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆè©³ç´°è¡¨ç¤ºæ™‚ã®ã¿ï¼‰
            import os
            if os.environ.get('DEBUG_SKEW_ANALYSIS', '').lower() in ['true', '1', 'yes']:
                debug_info = []
                
                # AQEãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¤œç´¢çµæœ
                debug_info.append(f"AQEãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œç´¢çµæœ: {aqe_metric_name if aqe_metric_name else 'ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹æœªç™ºè¦‹'}")
                if aqe_metric_name:
                    debug_info.append(f"AQEãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤: {aqe_skew_value} â‰¤ åŸºæº–å€¤: 0")
                else:
                    debug_info.append("AQEãƒ¡ãƒˆãƒªã‚¯ã‚¹: 'AQEShuffleRead - Number of skewed partitions' æœªç™ºè¦‹")
                
                # å„ã‚½ãƒ¼ã‚¹ã§ã®æ¤œç´¢çµæœã‚’è¡¨ç¤º
                detailed_metrics = node.get('detailed_metrics', {})
                raw_metrics = node.get('metrics', [])
                key_metrics = node.get('key_metrics', {})
                
                debug_info.append(f"detailed_metrics: {len(detailed_metrics)}å€‹ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹")
                debug_info.append(f"raw_metrics: {len(raw_metrics) if isinstance(raw_metrics, list) else 0}å€‹ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹")
                debug_info.append(f"key_metrics: {len(key_metrics)}å€‹ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹")
                
                if debug_info:
                    print(f"    ğŸ” ã‚¹ã‚­ãƒ¥ãƒ¼æœªæ¤œå‡ºç†ç”±:")
                    for info in debug_info:
                        print(f"       âœ… {info}")
        
        # ãƒãƒ¼ãƒ‰IDã‚‚è¡¨ç¤º
        print(f"    ğŸ†” ãƒãƒ¼ãƒ‰ID: {node.get('node_id', node.get('id', 'N/A'))}")
        print()
        
else:
    print("âš ï¸ ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

print()

# ğŸ”¥ Sparkã‚¹ãƒ†ãƒ¼ã‚¸å®Ÿè¡Œåˆ†æ
if extracted_metrics['stage_metrics']:
    print("\nğŸ”¥ Sparkã‚¹ãƒ†ãƒ¼ã‚¸å®Ÿè¡Œåˆ†æ")
    print("=" * 60)
    
    stage_metrics = extracted_metrics['stage_metrics']
    total_stages = len(stage_metrics)
    completed_stages = len([s for s in stage_metrics if s.get('status') == 'COMPLETE'])
    failed_stages = len([s for s in stage_metrics if s.get('num_failed_tasks', 0) > 0])
    
    print(f"ğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¸æ¦‚è¦: å…¨{total_stages}ã‚¹ãƒ†ãƒ¼ã‚¸ (å®Œäº†:{completed_stages}, å¤±æ•—ã‚¿ã‚¹ã‚¯ã‚ã‚Š:{failed_stages})")
    print()
    
    # ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
    sorted_stages = sorted(stage_metrics, key=lambda x: x.get('duration_ms', 0), reverse=True)
    
    print("â±ï¸ ã‚¹ãƒ†ãƒ¼ã‚¸å®Ÿè¡Œæ™‚é–“ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
    print("-" * 60)
    
    for i, stage in enumerate(sorted_stages[:5]):  # TOP5ã‚¹ãƒ†ãƒ¼ã‚¸ã®ã¿è¡¨ç¤º
        stage_id = stage.get('stage_id', 'N/A')
        status = stage.get('status', 'UNKNOWN')
        duration_ms = stage.get('duration_ms', 0)
        num_tasks = stage.get('num_tasks', 0)
        failed_tasks = stage.get('num_failed_tasks', 0)
        complete_tasks = stage.get('num_complete_tasks', 0)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã«å¿œã˜ãŸã‚¢ã‚¤ã‚³ãƒ³
        if status == 'COMPLETE' and failed_tasks == 0:
            status_icon = "âœ…"
        elif failed_tasks > 0:
            status_icon = "âš ï¸"
        else:
            status_icon = "â“"
        
        # ä¸¦åˆ—åº¦ã‚¢ã‚¤ã‚³ãƒ³
        parallelism_icon = "ğŸ”¥" if num_tasks >= 10 else "âš ï¸" if num_tasks >= 5 else "ğŸŒ"
        
        # å®Ÿè¡Œæ™‚é–“ã®é‡è¦åº¦
        if duration_ms >= 10000:
            time_icon = "ğŸ”´"
            severity = "CRITICAL"
        elif duration_ms >= 5000:
            time_icon = "ğŸŸ "
            severity = "HIGH"
        elif duration_ms >= 1000:
            time_icon = "ğŸŸ¡"
            severity = "MEDIUM"
        else:
            time_icon = "ğŸŸ¢"
            severity = "LOW"
        
        print(f"{i+1}. {status_icon}{parallelism_icon}{time_icon} ã‚¹ãƒ†ãƒ¼ã‚¸ {stage_id} [{severity:8}]")
        print(f"   â±ï¸ å®Ÿè¡Œæ™‚é–“: {duration_ms:,} ms ({duration_ms/1000:.1f} sec)")
        print(f"   ğŸ”§ ã‚¿ã‚¹ã‚¯: {complete_tasks}/{num_tasks} å®Œäº† (å¤±æ•—: {failed_tasks})")
        
        # ã‚¿ã‚¹ã‚¯ã‚ãŸã‚Šã®å¹³å‡æ™‚é–“
        if num_tasks > 0:
            avg_task_time = duration_ms / num_tasks
            print(f"   ğŸ“Š å¹³å‡ã‚¿ã‚¹ã‚¯æ™‚é–“: {avg_task_time:.1f} ms")
        
        # åŠ¹ç‡æ€§è©•ä¾¡
        if num_tasks > 0:
            task_efficiency = "é«˜åŠ¹ç‡" if num_tasks >= 10 and failed_tasks == 0 else "è¦æ”¹å–„" if failed_tasks > 0 else "æ¨™æº–"
            print(f"   ğŸ¯ åŠ¹ç‡æ€§: {task_efficiency}")
        
        print()
    
    if len(sorted_stages) > 5:
        print(f"... ä»– {len(sorted_stages) - 5} ã‚¹ãƒ†ãƒ¼ã‚¸")
    
    # å•é¡Œã®ã‚ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    problematic_stages = [s for s in stage_metrics if s.get('num_failed_tasks', 0) > 0 or s.get('duration_ms', 0) > 30000]
    if problematic_stages:
        print("\nğŸš¨ æ³¨æ„ãŒå¿…è¦ãªã‚¹ãƒ†ãƒ¼ã‚¸:")
        print("-" * 40)
        for stage in problematic_stages[:3]:
            stage_id = stage.get('stage_id', 'N/A')
            duration_sec = stage.get('duration_ms', 0) / 1000
            failed_tasks = stage.get('num_failed_tasks', 0)
            
            issues = []
            if failed_tasks > 0:
                issues.append(f"å¤±æ•—ã‚¿ã‚¹ã‚¯{failed_tasks}å€‹")
            if duration_sec > 30:
                issues.append(f"é•·æ™‚é–“å®Ÿè¡Œ({duration_sec:.1f}sec)")
            
            print(f"   âš ï¸ ã‚¹ãƒ†ãƒ¼ã‚¸ {stage_id}: {', '.join(issues)}")
    
    
    print()
else:
    print("\nğŸ”¥ Sparkã‚¹ãƒ†ãƒ¼ã‚¸å®Ÿè¡Œåˆ†æ")
    print("=" * 60)
    print("âš ï¸ ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    print()

print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ—‚ï¸ Liquid Clusteringåˆ†æçµæœã®è©³ç´°è¡¨ç¤º
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚«ãƒ©ãƒ ã®è©³ç´°è¡¨ç¤º
# MAGIC - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šè¦‹è¾¼ã¿ã®åˆ†æ
# MAGIC - ã‚«ãƒ©ãƒ ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©³ç´°åˆ†æ
# MAGIC - ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æƒ…å ±ã®è¡¨ç¤º
# MAGIC - SQLå®Ÿè£…ä¾‹ã®æç¤º

# COMMAND ----------

# ğŸ—‚ï¸ LLMã«ã‚ˆã‚‹Liquid Clusteringåˆ†æçµæœã®è©³ç´°è¡¨ç¤º
print("\n" + "=" * 50)
print("ğŸ¤– LLM Liquid Clusteringæ¨å¥¨åˆ†æ")
print("=" * 50)

# LLMãƒ™ãƒ¼ã‚¹ã®Liquid Clusteringåˆ†æã‚’å®Ÿè¡Œ
liquid_analysis = extracted_metrics['liquid_clustering_analysis']

# LLMåˆ†æçµæœã‚’è¡¨ç¤º
print("\nğŸ¤– LLMåˆ†æçµæœ:")
print("=" * 50)
llm_analysis = liquid_analysis.get('llm_analysis', '')
if llm_analysis:
    print(llm_analysis)
else:
    print("âŒ LLMåˆ†æçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

# æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦ã‚’è¡¨ç¤º
extracted_data = liquid_analysis.get('extracted_data', {})
metadata_summary = extracted_data.get('metadata_summary', {})

print(f"\nğŸ“Š æŠ½å‡ºãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
print(f"   ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶: {metadata_summary.get('filter_expressions_count', 0)}å€‹")
print(f"   ğŸ”— JOINæ¡ä»¶: {metadata_summary.get('join_expressions_count', 0)}å€‹")
print(f"   ğŸ“Š GROUP BYæ¡ä»¶: {metadata_summary.get('groupby_expressions_count', 0)}å€‹")
print(f"   ğŸ“ˆ é›†ç´„é–¢æ•°: {metadata_summary.get('aggregate_expressions_count', 0)}å€‹")
print(f"   ğŸ·ï¸ è­˜åˆ¥ãƒ†ãƒ¼ãƒ–ãƒ«: {metadata_summary.get('tables_identified', 0)}å€‹")
print(f"   ğŸ“‚ ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰: {metadata_summary.get('scan_nodes_count', 0)}å€‹")

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¤º
performance_context = liquid_analysis.get('performance_context', {})
print(f"\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±:")
print(f"   â±ï¸ å®Ÿè¡Œæ™‚é–“: {performance_context.get('total_time_sec', 0):.1f}ç§’")
print(f"   ğŸ’¾ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {performance_context.get('read_gb', 0):.2f}GB")
print(f"   ğŸ“Š å‡ºåŠ›è¡Œæ•°: {performance_context.get('rows_produced', 0):,}è¡Œ")
print(f"   ğŸ¯ ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§: {performance_context.get('data_selectivity', 0):.4f}")

# åˆ†æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
print(f"\nğŸ’¾ åˆ†æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ä¸­...")
try:
    saved_files = save_liquid_clustering_analysis(liquid_analysis, "/tmp")
    
    if "error" in saved_files:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {saved_files['error']}")
    else:
        print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›å®Œäº†:")
        for file_type, file_path in saved_files.items():
            if file_type == "json":
                print(f"   ğŸ“„ JSONè©³ç´°ãƒ‡ãƒ¼ã‚¿: {file_path}")
            elif file_type == "markdown":
                print(f"   ğŸ“ Markdownãƒ¬ãƒãƒ¼ãƒˆ: {file_path}")
            elif file_type == "sql":
                print(f"   ğŸ”§ SQLå®Ÿè£…ä¾‹: {file_path}")
                
except Exception as e:
    print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

# ã‚µãƒãƒªãƒ¼æƒ…å ±
summary = liquid_analysis.get('summary', {})
print(f"\nğŸ“‹ åˆ†æã‚µãƒãƒªãƒ¼:")
print(f"   ğŸ”¬ åˆ†ææ–¹æ³•: {summary.get('analysis_method', 'Unknown')}")
print(f"   ğŸ¤– LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {summary.get('llm_provider', 'Unknown')}")
print(f"   ğŸ“Š å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {summary.get('tables_identified', 0)}")
print(f"   ğŸ“ˆ æŠ½å‡ºã‚«ãƒ©ãƒ æ•°: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼({summary.get('total_filter_columns', 0)}) + JOIN({summary.get('total_join_columns', 0)}) + GROUP BY({summary.get('total_groupby_columns', 0)})")

print()

# COMMAND ----------

# ğŸ¤– è¨­å®šã•ã‚ŒãŸLLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ
provider = LLM_CONFIG["provider"]
if provider == "databricks":
    endpoint_name = LLM_CONFIG["databricks"]["endpoint_name"]
    print(f"ğŸ¤– Databricks Model Serving ({endpoint_name}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print(f"âš ï¸  Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ '{endpoint_name}' ãŒå¿…è¦ã§ã™")
elif provider == "openai":
    model = LLM_CONFIG["openai"]["model"]
    print(f"ğŸ¤– OpenAI ({model}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print("âš ï¸  OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™")
elif provider == "azure_openai":
    deployment = LLM_CONFIG["azure_openai"]["deployment_name"]
    print(f"ğŸ¤– Azure OpenAI ({deployment}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print("âš ï¸  Azure OpenAI APIã‚­ãƒ¼ã¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒå¿…è¦ã§ã™")
elif provider == "anthropic":
    model = LLM_CONFIG["anthropic"]["model"]
    print(f"ğŸ¤– Anthropic ({model}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print("âš ï¸  Anthropic APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™")

print("ğŸ“ åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç°¡æ½”åŒ–ã—ã¦ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒªã‚¹ã‚¯ã‚’è»½æ¸›ã—ã¦ã„ã¾ã™...")
print()

analysis_result = analyze_bottlenecks_with_llm(extracted_metrics)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¯ LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã®è¡¨ç¤º
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«ã‚ˆã‚‹è©³ç´°åˆ†æçµæœã®è¡¨ç¤º
# MAGIC - ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šã¨æ”¹å–„ææ¡ˆã®å¯è¦–åŒ–
# MAGIC - åˆ†æçµæœã®æ•´å½¢ã¨èª­ã¿ã‚„ã™ã„è¡¨ç¤º

# COMMAND ----------

# ğŸ“Š åˆ†æçµæœã®è¡¨ç¤º
print("\n" + "=" * 80)
print(f"ğŸ¯ ã€{provider.upper()} LLM ã«ã‚ˆã‚‹ SQLãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã€‘")
print("=" * 80)
print()
print(analysis_result)
print()
print("=" * 80)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ’¾ åˆ†æçµæœã®ä¿å­˜ã¨å®Œäº†ã‚µãƒãƒªãƒ¼
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - LLMåˆ†æçµæœã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ä¿å­˜
# MAGIC - åˆ†æå¯¾è±¡ã®åŸºæœ¬æƒ…å ±ã®è¨˜éŒ²
# MAGIC - å…¨ä½“å‡¦ç†ã®å®Œäº†ã‚µãƒãƒªãƒ¼è¡¨ç¤º
# MAGIC - å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§è¡¨ç¤º

# COMMAND ----------

# ğŸ’¾ åˆ†æçµæœã®ä¿å­˜ã¨å®Œäº†ã‚µãƒãƒªãƒ¼
from datetime import datetime
# output_bottleneck_analysis_result_XXX.txtãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›ã¯å»ƒæ­¢ï¼ˆoptimization_reportã«çµ±åˆï¼‰

# æœ€çµ‚çš„ãªã‚µãƒãƒªãƒ¼
print("\n" + "ğŸ‰" * 20)
print("ğŸ ã€å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼ã€‘")
print("ğŸ‰" * 20)
print("âœ… SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
print(f"âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºå®Œäº†")

# LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±ã®å‹•çš„è¡¨ç¤º
try:
    current_provider = LLM_CONFIG.get('provider', 'unknown')
    provider_display_names = {
        'databricks': f"Databricks ({LLM_CONFIG.get('databricks', {}).get('endpoint_name', 'Model Serving')})",
        'openai': f"OpenAI ({LLM_CONFIG.get('openai', {}).get('model', 'GPT-4')})",
        'azure_openai': f"Azure OpenAI ({LLM_CONFIG.get('azure_openai', {}).get('deployment_name', 'GPT-4')})",
        'anthropic': f"Anthropic ({LLM_CONFIG.get('anthropic', {}).get('model', 'Claude')})"
    }
    provider_display = provider_display_names.get(current_provider, f"{current_provider}ï¼ˆæœªçŸ¥ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼‰")
    print(f"âœ… {provider_display}ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Œäº†")
except Exception as e:
    print("âœ… LLMã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Œäº†")

print("âœ… åˆ†æçµæœã¯å¾Œã§optimization_reportã«çµ±åˆã•ã‚Œã¾ã™")
print()
print("ğŸš€ åˆ†æå®Œäº†ï¼çµæœã‚’ç¢ºèªã—ã¦ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã«ãŠå½¹ç«‹ã¦ãã ã•ã„ã€‚")
print("ğŸ‰" * 20)

# COMMAND ----------

# MAGIC %md
# MAGIC # ğŸ”§ SQLæœ€é©åŒ–æ©Ÿèƒ½ã‚»ã‚¯ã‚·ãƒ§ãƒ³
# MAGIC
# MAGIC **ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯SQLã‚¯ã‚¨ãƒªã®æœ€é©åŒ–ã‚’è¡Œã„ã¾ã™**
# MAGIC
# MAGIC ğŸ“‹ **æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹:**
# MAGIC - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®æŠ½å‡º
# MAGIC - LLMã«ã‚ˆã‚‹ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã®å®Ÿè¡Œ
# MAGIC - æœ€é©åŒ–çµæœã®ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
# MAGIC - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã®æº–å‚™
# MAGIC
# MAGIC âš ï¸ **å‰ææ¡ä»¶:** ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Œäº†ã—ã¦ã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ”§ SQLæœ€é©åŒ–é–¢é€£é–¢æ•°å®šç¾©
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - `extract_original_query_from_profiler_data`: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã‚’æŠ½å‡º
# MAGIC - `generate_optimized_query_with_llm`: LLMåˆ†æçµæœã«åŸºã¥ãã‚¯ã‚¨ãƒªæœ€é©åŒ–
# MAGIC - `save_optimized_sql_files`: æœ€é©åŒ–çµæœã®å„ç¨®ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜

# COMMAND ----------

def extract_original_query_from_profiler_data(profiler_data: Dict[str, Any]) -> str:
    """
    ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã‚’æŠ½å‡º
    """
    
    # è¤‡æ•°ã®å ´æ‰€ã‹ã‚‰SQLã‚¯ã‚¨ãƒªã‚’æ¢ã™
    query_candidates = []
    
    # 1. query.queryText ã‹ã‚‰æŠ½å‡º
    if 'query' in profiler_data and 'queryText' in profiler_data['query']:
        query_text = profiler_data['query']['queryText']
        if query_text and query_text.strip():
            query_candidates.append(query_text.strip())
    
    # 2. metadata ã‹ã‚‰æŠ½å‡º
    if 'metadata' in profiler_data:
        metadata = profiler_data['metadata']
        for key, value in metadata.items():
            if 'sql' in key.lower() or 'query' in key.lower():
                if isinstance(value, str) and value.strip():
                    query_candidates.append(value.strip())
    
    # 3. graphs ã® metadata ã‹ã‚‰æŠ½å‡º
    if 'graphs' in profiler_data:
        for graph in profiler_data['graphs']:
            nodes = graph.get('nodes', [])
            for node in nodes:
                node_metadata = node.get('metadata', [])
                for meta in node_metadata:
                    if meta.get('key', '').upper() in ['SQL', 'QUERY', 'SQL_TEXT']:
                        value = meta.get('value', '')
                        if value and value.strip():
                            query_candidates.append(value.strip())
    
    # æœ€ã‚‚é•·ã„ã‚¯ã‚¨ãƒªã‚’é¸æŠï¼ˆé€šå¸¸ã€æœ€ã‚‚å®Œå…¨ãªã‚¯ã‚¨ãƒªï¼‰
    if query_candidates:
        original_query = max(query_candidates, key=len)
        return original_query
    
    return ""

def extract_table_size_estimates_from_plan(profiler_data: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã®æ¨å®šã‚µã‚¤ã‚ºæƒ…å ±ã‚’æŠ½å‡º
    
    æ³¨æ„: Databricksã‚¯ã‚¨ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ estimatedSizeInBytes ãŒå«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€
    ã“ã®æ©Ÿèƒ½ã¯ç¾åœ¨ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã®æ¨å®šã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
    
    Args:
        profiler_data: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿
        
    Returns:
        Dict: ç©ºã®è¾æ›¸ï¼ˆæ©Ÿèƒ½ç„¡åŠ¹åŒ–ï¼‰
    """
    # Databricksã‚¯ã‚¨ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«estimatedSizeInBytesãŒå«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ç„¡åŠ¹åŒ–
    return {}

def extract_table_name_from_scan_node(node: Dict[str, Any]) -> str:
    """
    ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
    
    Args:
        node: å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã®ãƒãƒ¼ãƒ‰
        
    Returns:
        str: ãƒ†ãƒ¼ãƒ–ãƒ«å
    """
    try:
        # è¤‡æ•°ã®æ–¹æ³•ã§ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡ºã‚’è©¦è¡Œ
        
        # 1. node outputã‹ã‚‰ã®æŠ½å‡º
        output = node.get("output", "")
        if output:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: [col1#123, col2#456] table_name
            import re
            table_match = re.search(r'\]\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)', output)
            if table_match:
                return table_match.group(1)
        
        # 2. nodeè©³ç´°ã‹ã‚‰ã®æŠ½å‡º
        details = node.get("details", "")
        if details:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: Location: /path/to/table/name
            location_match = re.search(r'Location:.*?([a-zA-Z_][a-zA-Z0-9_]*)', details)
            if location_match:
                return location_match.group(1)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: Table: database.table_name
            table_match = re.search(r'Table:\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)', details)
            if table_match:
                return table_match.group(1)
        
        # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®æŠ½å‡º
        metadata = node.get("metadata", [])
        for meta in metadata:
            if meta.get("key") == "table" or meta.get("key") == "relation":
                values = meta.get("values", [])
                if values:
                    return str(values[0])
        
        # 4. nodeåã‹ã‚‰ã®æ¨æ¸¬ï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
        node_name = node.get("nodeName", "")
        if "delta" in node_name.lower():
            # Delta Scan ã®å ´åˆã€è©³ç´°æƒ…å ±ã‹ã‚‰æŠ½å‡º
            pass
    
    except Exception as e:
        print(f"âš ï¸ ãƒ†ãƒ¼ãƒ–ãƒ«åæŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    return None

def extract_broadcast_table_names(profiler_data: Dict[str, Any], broadcast_nodes: list) -> Dict[str, Any]:
    """
    BROADCASTãƒãƒ¼ãƒ‰ã‹ã‚‰é–¢é€£ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
    """
    broadcast_table_info = {
        "broadcast_tables": [],
        "broadcast_table_mapping": {},
        "broadcast_nodes_with_tables": []
    }
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã®ã‚°ãƒ©ãƒ•æƒ…å ±ã‚’å–å¾—
    graphs = profiler_data.get('graphs', [])
    if not graphs:
        return broadcast_table_info
    
    # å…¨ãƒãƒ¼ãƒ‰ã‚’åé›†
    all_nodes = []
    for graph in graphs:
        nodes = graph.get('nodes', [])
        all_nodes.extend(nodes)
    
    # ã‚¨ãƒƒã‚¸æƒ…å ±ã‚’åé›†ï¼ˆãƒãƒ¼ãƒ‰é–“ã®é–¢ä¿‚ï¼‰
    all_edges = []
    for graph in graphs:
        edges = graph.get('edges', [])
        all_edges.extend(edges)
    
    # å„BROADCASTãƒãƒ¼ãƒ‰ã«ã¤ã„ã¦é–¢é€£ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®š
    for broadcast_node in broadcast_nodes:
        broadcast_node_id = broadcast_node.get('node_id', '')
        broadcast_node_name = broadcast_node.get('node_name', '')
        
        # BROADCASTãƒãƒ¼ãƒ‰ã‹ã‚‰ç›´æ¥ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
        table_names = set()
        
        # 1. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
        metadata = broadcast_node.get('metadata', [])
        for meta in metadata:
            key = meta.get('key', '')
            value = meta.get('value', '')
            values = meta.get('values', [])
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ç¤ºã™ã‚­ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
            if key in ['SCAN_IDENTIFIER', 'TABLE_NAME', 'RELATION']:
                if value:
                    table_names.add(value)
                table_names.update(values)
        
        # 2. ãƒãƒ¼ãƒ‰åã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æ¨å®š
        if 'SCAN' in broadcast_node_name:
            # "Broadcast Scan delta orders" â†’ "orders"
            import re
            table_match = re.search(r'SCAN\s+(?:DELTA|PARQUET|JSON|CSV)?\s*([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)', broadcast_node_name, re.IGNORECASE)
            if table_match:
                table_names.add(table_match.group(1))
        
        # 3. ã‚¨ãƒƒã‚¸æƒ…å ±ã‹ã‚‰é–¢é€£ã™ã‚‹ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‚’ç‰¹å®š
        for edge in all_edges:
            source_id = edge.get('source', '')
            target_id = edge.get('target', '')
            
            # BROADCASTãƒãƒ¼ãƒ‰ã«å…¥åŠ›ã•ã‚Œã‚‹ãƒãƒ¼ãƒ‰ã‚’æ¤œç´¢
            if target_id == broadcast_node_id:
                # å…¥åŠ›ãƒãƒ¼ãƒ‰ãŒã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‹ãƒã‚§ãƒƒã‚¯
                for node in all_nodes:
                    if node.get('id', '') == source_id:
                        node_name = node.get('name', '').upper()
                        if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN']):
                            # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
                            scan_table_name = extract_table_name_from_scan_node(node)
                            if scan_table_name:
                                table_names.add(scan_table_name)
        
        # 4. åŒã˜ã‚°ãƒ©ãƒ•å†…ã®ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã¨ã®é–¢é€£ä»˜ã‘
        for node in all_nodes:
            node_name = node.get('name', '').upper()
            if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN']):
                # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®åå‰ãŒBROADCASTãƒãƒ¼ãƒ‰åã«å«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                scan_table_name = extract_table_name_from_scan_node(node)
                if scan_table_name:
                    # ãƒ†ãƒ¼ãƒ–ãƒ«åã®éƒ¨åˆ†ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
                    if any(part in broadcast_node_name for part in scan_table_name.split('.') if len(part) > 2):
                        table_names.add(scan_table_name)
        
        # çµæœã‚’è¨˜éŒ²
        table_names_list = list(table_names)
        if table_names_list:
            broadcast_table_info["broadcast_tables"].extend(table_names_list)
            broadcast_table_info["broadcast_table_mapping"][broadcast_node_id] = table_names_list
            
            # BROADCASTãƒãƒ¼ãƒ‰æƒ…å ±ã‚’æ‹¡å¼µ
            enhanced_broadcast_node = broadcast_node.copy()
            enhanced_broadcast_node["associated_tables"] = table_names_list
            enhanced_broadcast_node["table_count"] = len(table_names_list)
            broadcast_table_info["broadcast_nodes_with_tables"].append(enhanced_broadcast_node)
    
    # é‡è¤‡ã‚’é™¤å»
    broadcast_table_info["broadcast_tables"] = list(set(broadcast_table_info["broadcast_tables"]))
    
    return broadcast_table_info

def extract_execution_plan_info(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    JSONãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’æŠ½å‡º
    """
    plan_info = {
        "broadcast_nodes": [],
        "join_nodes": [],
        "scan_nodes": [],
        "shuffle_nodes": [],
        "aggregate_nodes": [],
        "plan_summary": {},
        "broadcast_already_applied": False,
        "join_strategies": [],
        "table_scan_details": {},
        "broadcast_table_info": {}
    }
    
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿè¡Œã‚°ãƒ©ãƒ•æƒ…å ±ã‚’å–å¾—
    graphs = profiler_data.get('graphs', [])
    if not graphs:
        return plan_info
    
    # ã™ã¹ã¦ã®ã‚°ãƒ©ãƒ•ã‹ã‚‰ãƒãƒ¼ãƒ‰ã‚’åé›†
    all_nodes = []
    for graph_index, graph in enumerate(graphs):
        nodes = graph.get('nodes', [])
        for node in nodes:
            node['graph_index'] = graph_index
            all_nodes.append(node)
    
    # ãƒãƒ¼ãƒ‰åˆ†æ
    for node in all_nodes:
        node_name = node.get('name', '').upper()
        node_tag = node.get('tag', '').upper()
        node_metadata = node.get('metadata', [])
        
        # BROADCASTãƒãƒ¼ãƒ‰ã®æ¤œå‡º
        if 'BROADCAST' in node_name or 'BROADCAST' in node_tag:
            plan_info["broadcast_already_applied"] = True
            broadcast_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "metadata": []
            }
            
            # BROADCASTã«é–¢é€£ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                value = meta.get('value', '')
                values = meta.get('values', [])
                
                if any(keyword in key.upper() for keyword in ['BROADCAST', 'BUILD', 'PROBE']):
                    broadcast_info["metadata"].append({
                        "key": key,
                        "value": value,
                        "values": values
                    })
            
            plan_info["broadcast_nodes"].append(broadcast_info)
        
        # JOINãƒãƒ¼ãƒ‰ã®æ¤œå‡ºã¨æˆ¦ç•¥åˆ†æ
        elif any(keyword in node_name for keyword in ['JOIN', 'HASH']):
            join_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "join_strategy": "unknown",
                "join_keys": [],
                "join_type": "unknown"
            }
            
            # JOINæˆ¦ç•¥ã®ç‰¹å®š
            if 'BROADCAST' in node_name:
                join_info["join_strategy"] = "broadcast_hash_join"
            elif 'SORT' in node_name and 'MERGE' in node_name:
                join_info["join_strategy"] = "sort_merge_join"
            elif 'HASH' in node_name:
                join_info["join_strategy"] = "shuffle_hash_join"
            elif 'NESTED' in node_name:
                join_info["join_strategy"] = "broadcast_nested_loop_join"
            
            # JOINã‚¿ã‚¤ãƒ—ã®ç‰¹å®š
            if 'INNER' in node_name:
                join_info["join_type"] = "inner"
            elif 'LEFT' in node_name:
                join_info["join_type"] = "left"
            elif 'RIGHT' in node_name:
                join_info["join_type"] = "right"
            elif 'OUTER' in node_name:
                join_info["join_type"] = "outer"
            
            # JOINæ¡ä»¶ã®æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                values = meta.get('values', [])
                
                if key in ['LEFT_KEYS', 'RIGHT_KEYS']:
                    join_info["join_keys"].extend(values)
            
            plan_info["join_nodes"].append(join_info)
            plan_info["join_strategies"].append(join_info["join_strategy"])
        
        # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®è©³ç´°åˆ†æ
        elif any(keyword in node_name for keyword in ['SCAN', 'FILESCAN']):
            scan_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "table_name": "unknown",
                "file_format": "unknown",
                "pushed_filters": [],
                "output_columns": []
            }
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                value = meta.get('value', '')
                values = meta.get('values', [])
                
                if key == 'SCAN_IDENTIFIER':
                    scan_info["table_name"] = value
                elif key == 'OUTPUT':
                    scan_info["output_columns"] = values
                elif key == 'PUSHED_FILTERS' or key == 'FILTERS':
                    scan_info["pushed_filters"] = values
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®æ¨å®š
            if 'DELTA' in node_name:
                scan_info["file_format"] = "delta"
            elif 'PARQUET' in node_name:
                scan_info["file_format"] = "parquet"
            elif 'JSON' in node_name:
                scan_info["file_format"] = "json"
            elif 'CSV' in node_name:
                scan_info["file_format"] = "csv"
            
            plan_info["scan_nodes"].append(scan_info)
            plan_info["table_scan_details"][scan_info["table_name"]] = scan_info
        
        # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰ã®æ¤œå‡º
        elif any(keyword in node_name for keyword in ['SHUFFLE', 'EXCHANGE']):
            shuffle_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "partition_keys": []
            }
            
            # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±ã®æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                values = meta.get('values', [])
                
                if key in ['PARTITION_EXPRESSIONS', 'PARTITION_KEYS']:
                    shuffle_info["partition_keys"] = values
            
            plan_info["shuffle_nodes"].append(shuffle_info)
        
        # é›†ç´„ãƒãƒ¼ãƒ‰ã®æ¤œå‡º
        elif any(keyword in node_name for keyword in ['AGGREGATE', 'GROUP']):
            agg_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "group_keys": [],
                "aggregate_expressions": []
            }
            
            # é›†ç´„æƒ…å ±ã®æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                values = meta.get('values', [])
                
                if key == 'GROUPING_EXPRESSIONS':
                    agg_info["group_keys"] = values
                elif key == 'AGGREGATE_EXPRESSIONS':
                    agg_info["aggregate_expressions"] = values
            
            plan_info["aggregate_nodes"].append(agg_info)
    
    # ãƒ—ãƒ©ãƒ³ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ
    plan_info["plan_summary"] = {
        "total_nodes": len(all_nodes),
        "broadcast_nodes_count": len(plan_info["broadcast_nodes"]),
        "join_nodes_count": len(plan_info["join_nodes"]),
        "scan_nodes_count": len(plan_info["scan_nodes"]),
        "shuffle_nodes_count": len(plan_info["shuffle_nodes"]),
        "aggregate_nodes_count": len(plan_info["aggregate_nodes"]),
        "unique_join_strategies": list(set(plan_info["join_strategies"])),
        "has_broadcast_joins": plan_info["broadcast_already_applied"],
        "tables_scanned": len(plan_info["table_scan_details"])
    }
    
    # BROADCASTãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’æŠ½å‡º
    if plan_info["broadcast_nodes"]:
        broadcast_table_info = extract_broadcast_table_names(profiler_data, plan_info["broadcast_nodes"])
        plan_info["broadcast_table_info"] = broadcast_table_info
        
        # ãƒ—ãƒ©ãƒ³ã‚µãƒãƒªãƒ¼ã«BROADCASTãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’è¿½åŠ 
        plan_info["plan_summary"]["broadcast_tables"] = broadcast_table_info["broadcast_tables"]
        plan_info["plan_summary"]["broadcast_table_count"] = len(broadcast_table_info["broadcast_tables"])
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®šæƒ…å ±ã‚’è¿½åŠ ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
    plan_info["table_size_estimates"] = {}  # extract_table_size_estimates_from_plan(profiler_data)
    
    return plan_info

def get_spark_broadcast_threshold() -> float:
    """
    Sparkã®å®Ÿéš›ã®broadcasté–¾å€¤è¨­å®šã‚’å–å¾—
    """
    try:
        # Sparkã®è¨­å®šå€¤ã‚’å–å¾—
        threshold_bytes = spark.conf.get("spark.databricks.optimizer.autoBroadcastJoinThreshold", "31457280")  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30MB
        threshold_mb = float(threshold_bytes) / 1024 / 1024
        return threshold_mb
    except:
        # å–å¾—ã§ããªã„å ´åˆã¯æ¨™æº–çš„ãª30MBã‚’è¿”ã™
        return 30.0

def estimate_uncompressed_size(compressed_size_mb: float, file_format: str = "parquet") -> float:
    """
    åœ§ç¸®ã‚µã‚¤ã‚ºã‹ã‚‰éåœ§ç¸®ã‚µã‚¤ã‚ºã‚’æ¨å®šï¼ˆ3.0å€å›ºå®šï¼‰
    
    æ³¨æ„: å®Ÿéš›ã®estimatedSizeInBytesãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€
    ä¿å®ˆçš„ãª3.0å€åœ§ç¸®ç‡ã§çµ±ä¸€ã—ã¦æ¨å®šã—ã¾ã™ã€‚
    """
    # ä¿å®ˆçš„ãª3.0å€åœ§ç¸®ç‡ã§çµ±ä¸€ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ï¼‰
    compression_ratio = 3.0
    
    return compressed_size_mb * compression_ratio

def analyze_broadcast_feasibility(metrics: Dict[str, Any], original_query: str, plan_info: Dict[str, Any] = None) -> Dict[str, Any]:
    """
    BROADCASTãƒ’ãƒ³ãƒˆã®é©ç”¨å¯èƒ½æ€§ã‚’åˆ†æï¼ˆæ­£ç¢ºãª30MBé–¾å€¤é©ç”¨ï¼‰
    """
    broadcast_analysis = {
        "is_join_query": False,
        "broadcast_candidates": [],
        "recommendations": [],
        "feasibility": "not_applicable",
        "reasoning": [],
        "spark_threshold_mb": get_spark_broadcast_threshold(),
        "compression_analysis": {},
        "detailed_size_analysis": [],
        "execution_plan_analysis": {},
        "existing_broadcast_nodes": [],
        "already_optimized": False,
        "broadcast_applied_tables": []
    }
    
    # ã‚¯ã‚¨ãƒªã«JOINãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    query_upper = original_query.upper()
    join_types = ['JOIN', 'INNER JOIN', 'LEFT JOIN', 'RIGHT JOIN', 'LEFT OUTER JOIN', 'RIGHT OUTER JOIN', 'SEMI JOIN', 'ANTI JOIN']
    has_join = any(join_type in query_upper for join_type in join_types)
    
    if not has_join:
        broadcast_analysis["reasoning"].append("JOINã‚¯ã‚¨ãƒªã§ã¯ãªã„ãŸã‚ã€BROADCASTãƒ’ãƒ³ãƒˆã¯é©ç”¨ä¸å¯")
        return broadcast_analysis
    
    broadcast_analysis["is_join_query"] = True
    broadcast_analysis["reasoning"].append(f"Spark BROADCASTé–¾å€¤: {broadcast_analysis['spark_threshold_mb']:.1f}MBï¼ˆéåœ§ç¸®ï¼‰")
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã®åˆ†æ
    if plan_info:
        plan_summary = plan_info.get("plan_summary", {})
        broadcast_nodes = plan_info.get("broadcast_nodes", [])
        join_nodes = plan_info.get("join_nodes", [])
        table_scan_details = plan_info.get("table_scan_details", {})
        table_size_estimates = plan_info.get("table_size_estimates", {})
        
        # æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã®è¨˜éŒ²
        broadcast_analysis["existing_broadcast_nodes"] = broadcast_nodes
        broadcast_analysis["already_optimized"] = len(broadcast_nodes) > 0
        
        # ãƒ—ãƒ©ãƒ³åˆ†æçµæœã®è¨˜éŒ²
        broadcast_analysis["execution_plan_analysis"] = {
            "has_broadcast_joins": plan_summary.get("has_broadcast_joins", False),
            "unique_join_strategies": plan_summary.get("unique_join_strategies", []),
            "broadcast_nodes_count": len(broadcast_nodes),
            "join_nodes_count": len(join_nodes),
            "scan_nodes_count": plan_summary.get("scan_nodes_count", 0),
            "shuffle_nodes_count": plan_summary.get("shuffle_nodes_count", 0),
            "tables_in_plan": list(table_scan_details.keys())
        }
        
        # æ—¢ã«BROADCASTãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹å ´åˆã®è©³ç´°è¨˜éŒ²
        if broadcast_nodes:
            broadcast_analysis["reasoning"].append(f"âœ… å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã§æ—¢ã«BROADCAST JOINãŒé©ç”¨æ¸ˆã¿: {len(broadcast_nodes)}å€‹ã®ãƒãƒ¼ãƒ‰")
            
            # BROADCASTãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’å–å¾—
            broadcast_table_info = plan_info.get("broadcast_table_info", {})
            broadcast_tables = broadcast_table_info.get("broadcast_tables", [])
            
            if broadcast_tables:
                broadcast_analysis["reasoning"].append(f"ğŸ“‹ BROADCASTã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(broadcast_tables)}")
                broadcast_analysis["broadcast_applied_tables"] = broadcast_tables
                
                # å„BROADCASTãƒãƒ¼ãƒ‰ã®è©³ç´°
                broadcast_nodes_with_tables = broadcast_table_info.get("broadcast_nodes_with_tables", [])
                for i, node in enumerate(broadcast_nodes_with_tables[:3]):  # æœ€å¤§3å€‹ã¾ã§è¡¨ç¤º
                    node_name_short = node['node_name'][:50] + "..." if len(node['node_name']) > 50 else node['node_name']
                    associated_tables = node.get('associated_tables', [])
                    if associated_tables:
                        broadcast_analysis["reasoning"].append(f"  â€¢ BROADCAST Node {i+1}: {node_name_short}")
                        broadcast_analysis["reasoning"].append(f"    â””â”€ ãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(associated_tables)}")
                    else:
                        broadcast_analysis["reasoning"].append(f"  â€¢ BROADCAST Node {i+1}: {node_name_short} (ãƒ†ãƒ¼ãƒ–ãƒ«åæœªç‰¹å®š)")
            else:
                # BROADCASTãƒãƒ¼ãƒ‰ã¯å­˜åœ¨ã™ã‚‹ãŒãƒ†ãƒ¼ãƒ–ãƒ«åãŒç‰¹å®šã§ããªã„å ´åˆ
                for i, node in enumerate(broadcast_nodes[:3]):  # æœ€å¤§3å€‹ã¾ã§è¡¨ç¤º
                    broadcast_analysis["reasoning"].append(f"  â€¢ BROADCAST Node {i+1}: {node['node_name'][:50]}... (ãƒ†ãƒ¼ãƒ–ãƒ«åè§£æä¸­)")
        else:
            # BROADCASTæœªé©ç”¨ã ãŒã€JOINãŒå­˜åœ¨ã™ã‚‹å ´åˆ
            if join_nodes:
                join_strategies = set(node["join_strategy"] for node in join_nodes)
                broadcast_analysis["reasoning"].append(f"ğŸ” ç¾åœ¨ã®JOINæˆ¦ç•¥: {', '.join(join_strategies)}")
                broadcast_analysis["reasoning"].append("ğŸ’¡ BROADCASTæœ€é©åŒ–ã®æ©Ÿä¼šã‚’æ¤œè¨ä¸­...")
    else:
        broadcast_analysis["reasoning"].append("âš ï¸ å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¨å®šã«åŸºã¥ãåˆ†æã‚’å®Ÿè¡Œ")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±ã‚’å–å¾—
    overall_metrics = metrics.get('overall_metrics', {})
    node_metrics = metrics.get('node_metrics', [])
    
    # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’æŠ½å‡º
    scan_nodes = []
    total_compressed_bytes = 0
    total_rows_all_tables = 0
    
    for node in node_metrics:
        node_name = node.get('name', '').upper()
        if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN', 'PARQUET', 'DELTA']):
            key_metrics = node.get('key_metrics', {})
            rows_num = key_metrics.get('rowsNum', 0)
            duration_ms = key_metrics.get('durationMs', 0)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®æ¨å®šï¼ˆãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’å„ªå…ˆï¼‰
            file_format = "parquet"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            table_name_from_plan = "unknown"
            
            # ãƒ—ãƒ©ãƒ³æƒ…å ±ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’å–å¾—
            if plan_info and plan_info.get("table_scan_details"):
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©³ç´°ãªãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
                node_metadata = node.get('metadata', [])
                for meta in node_metadata:
                    meta_key = meta.get('key', '')
                    meta_value = meta.get('value', '')
                    if meta_key in ['SCAN_IDENTIFIER', 'SCAN_TABLE', 'TABLE_NAME'] and meta_value:
                        # ãƒ—ãƒ©ãƒ³ã®è©³ç´°ã¨ç…§åˆ
                        for plan_table, scan_detail in plan_info["table_scan_details"].items():
                            if meta_value in plan_table or plan_table in meta_value:
                                table_name_from_plan = plan_table
                                if scan_detail["file_format"] != "unknown":
                                    file_format = scan_detail["file_format"]
                                break
                        break
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒãƒ¼ãƒ‰åã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’æ¨å®š
            if file_format == "parquet":  # ã¾ã ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å ´åˆ
                if "DELTA" in node_name:
                    file_format = "delta"
                elif "PARQUET" in node_name:
                    file_format = "parquet"
                elif "JSON" in node_name:
                    file_format = "json"
                elif "CSV" in node_name:
                    file_format = "csv"
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®šã®ã¿ä½¿ç”¨ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ï¼‰
            estimated_compressed_mb = 0
            estimated_uncompressed_mb = 0
            size_source = "metrics_estimation"
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®š
            total_read_bytes = overall_metrics.get('read_bytes', 0)
            total_rows = overall_metrics.get('rows_read_count', 0)
            
            if total_rows > 0 and total_read_bytes > 0 and rows_num > 0:
                # å…¨ä½“ã®èª­ã¿è¾¼ã¿é‡ã‹ã‚‰ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®å‰²åˆã‚’è¨ˆç®—
                table_ratio = rows_num / total_rows
                estimated_compressed_bytes = total_read_bytes * table_ratio
                estimated_compressed_mb = estimated_compressed_bytes / 1024 / 1024
                 
                # éåœ§ç¸®ã‚µã‚¤ã‚ºã‚’æ¨å®š
                estimated_uncompressed_mb = estimate_uncompressed_size(estimated_compressed_mb, file_format)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è¡Œæ•°ãƒ™ãƒ¼ã‚¹ã®æ¨å®šï¼ˆä¿å®ˆçš„ï¼‰
                # å¹³å‡è¡Œã‚µã‚¤ã‚ºã‚’æ¨å®šï¼ˆéåœ§ç¸®ï¼‰
                if total_rows > 0 and total_read_bytes > 0:
                    # å…¨ä½“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åœ§ç¸®å¾Œã®å¹³å‡è¡Œã‚µã‚¤ã‚ºã‚’è¨ˆç®—
                    compressed_avg_row_size = total_read_bytes / total_rows
                    # åœ§ç¸®ç‡ã‚’è€ƒæ…®ã—ã¦éåœ§ç¸®ã‚µã‚¤ã‚ºã‚’æ¨å®š
                    uncompressed_avg_row_size = compressed_avg_row_size * estimate_uncompressed_size(1.0, file_format)
                else:
                    # å®Œå…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ä¸€èˆ¬çš„ãªéåœ§ç¸®è¡Œã‚µã‚¤ã‚ºï¼ˆ1KBï¼‰
                    uncompressed_avg_row_size = 1024
                
                estimated_compressed_mb = (rows_num * compressed_avg_row_size) / 1024 / 1024 if 'compressed_avg_row_size' in locals() else 0
                estimated_uncompressed_mb = (rows_num * uncompressed_avg_row_size) / 1024 / 1024
            
            # æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯
            is_already_broadcasted = False
            if plan_info and plan_info.get("broadcast_nodes"):
                for broadcast_node in plan_info["broadcast_nodes"]:
                    # ãƒ†ãƒ¼ãƒ–ãƒ«åã®éƒ¨åˆ†ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
                    broadcast_node_name = broadcast_node["node_name"]
                    if (table_name_from_plan != "unknown" and 
                        any(part in broadcast_node_name for part in table_name_from_plan.split('.') if len(part) > 3)):
                        is_already_broadcasted = True
                        break
                    # ãƒãƒ¼ãƒ‰åã§ã®ç…§åˆ
                    elif any(part in broadcast_node_name for part in node_name.split() if len(part) > 3):
                        is_already_broadcasted = True
                        break

            scan_info = {
                "node_name": node_name,
                "table_name_from_plan": table_name_from_plan,
                "rows": rows_num,
                "duration_ms": duration_ms,
                "estimated_compressed_mb": estimated_compressed_mb,
                "estimated_uncompressed_mb": estimated_uncompressed_mb,
                "file_format": file_format,
                "compression_ratio": 3.0,  # å›ºå®š3.0å€åœ§ç¸®ç‡
                "node_id": node.get('node_id', ''),
                "is_already_broadcasted": is_already_broadcasted,
                "size_estimation_source": size_source,
                "size_confidence": "medium"  # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®šã®ãŸã‚ä¸­ç¨‹åº¦ä¿¡é ¼åº¦
            }
            scan_nodes.append(scan_info)
            
            total_compressed_bytes += estimated_compressed_bytes if 'estimated_compressed_bytes' in locals() else 0
            total_rows_all_tables += rows_num
    
    # BROADCASTå€™è£œã®åˆ¤å®šï¼ˆ30MBé–¾å€¤ä½¿ç”¨ï¼‰
    broadcast_threshold_mb = broadcast_analysis["spark_threshold_mb"]  # å®Ÿéš›ã®Sparkè¨­å®šå€¤
    broadcast_safe_mb = broadcast_threshold_mb * 0.8  # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ï¼ˆ80%ï¼‰
    broadcast_max_mb = broadcast_threshold_mb * 10    # æ˜ã‚‰ã‹ã«å¤§ãã™ãã‚‹é–¾å€¤
    
    small_tables = []
    large_tables = []
    marginal_tables = []
    
    # åœ§ç¸®åˆ†æã®è¨˜éŒ²
    broadcast_analysis["compression_analysis"] = {
        "total_compressed_gb": total_compressed_bytes / 1024 / 1024 / 1024 if total_compressed_bytes > 0 else 0,
        "total_rows": total_rows_all_tables,
        "avg_compression_ratio": 0
    }
    
    for scan in scan_nodes:
        uncompressed_size_mb = scan["estimated_uncompressed_mb"]
        compressed_size_mb = scan["estimated_compressed_mb"]
        
        # è©³ç´°ã‚µã‚¤ã‚ºåˆ†æã®è¨˜éŒ²
        table_display_name = scan.get("table_name_from_plan", scan["node_name"])
        is_already_broadcasted = scan.get("is_already_broadcasted", False)
        
        size_analysis = {
            "table": table_display_name,
            "node_name": scan["node_name"],
            "rows": scan["rows"],
            "compressed_mb": compressed_size_mb,
            "uncompressed_mb": uncompressed_size_mb,
            "file_format": scan["file_format"],
            "compression_ratio": scan["compression_ratio"],
            "broadcast_decision": "",
            "decision_reasoning": "",
            "is_already_broadcasted": is_already_broadcasted
        }
        
        # 30MBé–¾å€¤ã§ã®åˆ¤å®šï¼ˆéåœ§ç¸®ã‚µã‚¤ã‚ºï¼‰- æ—¢å­˜é©ç”¨çŠ¶æ³ã‚’è€ƒæ…®
        if is_already_broadcasted:
            # æ—¢ã«BROADCASTãŒé©ç”¨æ¸ˆã¿
            small_tables.append(scan)  # çµ±è¨ˆç›®çš„ã§è¨˜éŒ²
            size_analysis["broadcast_decision"] = "already_applied"
            size_analysis["decision_reasoning"] = f"æ—¢ã«BROADCASTé©ç”¨æ¸ˆã¿ï¼ˆæ¨å®šã‚µã‚¤ã‚º: éåœ§ç¸®{uncompressed_size_mb:.1f}MBï¼‰"
            broadcast_analysis["broadcast_candidates"].append({
                "table": table_display_name,
                "estimated_uncompressed_mb": uncompressed_size_mb,
                "estimated_compressed_mb": compressed_size_mb,
                "rows": scan["rows"],
                "file_format": scan["file_format"],
                "compression_ratio": scan["compression_ratio"],
                "broadcast_feasible": True,
                "confidence": "confirmed",
                "status": "already_applied",
                "reasoning": f"å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã§æ—¢ã«BROADCASTé©ç”¨ç¢ºèªæ¸ˆã¿ï¼ˆæ¨å®šã‚µã‚¤ã‚º: éåœ§ç¸®{uncompressed_size_mb:.1f}MBã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®šï¼‰"
            })
        elif uncompressed_size_mb <= broadcast_safe_mb and scan["rows"] > 0:
            # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³å†…ï¼ˆ24MBä»¥ä¸‹ï¼‰- å¼·ãæ¨å¥¨
            small_tables.append(scan)
            size_analysis["broadcast_decision"] = "strongly_recommended"
            size_analysis["decision_reasoning"] = f"éåœ§ç¸®{uncompressed_size_mb:.1f}MB â‰¤ å®‰å…¨é–¾å€¤{broadcast_safe_mb:.1f}MB"
            broadcast_analysis["broadcast_candidates"].append({
                "table": table_display_name,
                "estimated_uncompressed_mb": uncompressed_size_mb,
                "estimated_compressed_mb": compressed_size_mb,
                "rows": scan["rows"],
                "file_format": scan["file_format"],
                "compression_ratio": scan["compression_ratio"],
                "broadcast_feasible": True,
                "confidence": "high",
                "status": "new_recommendation",
                "reasoning": f"éåœ§ç¸®æ¨å®šã‚µã‚¤ã‚º {uncompressed_size_mb:.1f}MBï¼ˆå®‰å…¨é–¾å€¤ {broadcast_safe_mb:.1f}MB ä»¥ä¸‹ï¼‰ã§BROADCASTå¼·ãæ¨å¥¨ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®šã€3.0å€åœ§ç¸®ç‡ï¼‰"
            })
        elif uncompressed_size_mb <= broadcast_threshold_mb and scan["rows"] > 0:
            # é–¾å€¤å†…ã ãŒå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ã¯è¶…éï¼ˆ24-30MBï¼‰- æ¡ä»¶ä»˜ãæ¨å¥¨
            marginal_tables.append(scan)
            size_analysis["broadcast_decision"] = "conditionally_recommended"
            size_analysis["decision_reasoning"] = f"éåœ§ç¸®{uncompressed_size_mb:.1f}MB â‰¤ é–¾å€¤{broadcast_threshold_mb:.1f}MBï¼ˆå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³è¶…éï¼‰"
            broadcast_analysis["broadcast_candidates"].append({
                "table": table_display_name,
                "estimated_uncompressed_mb": uncompressed_size_mb,
                "estimated_compressed_mb": compressed_size_mb,
                "rows": scan["rows"],
                "file_format": scan["file_format"],
                "compression_ratio": scan["compression_ratio"],
                "broadcast_feasible": True,
                "confidence": "medium",
                "status": "new_recommendation",
                "reasoning": f"éåœ§ç¸®æ¨å®šã‚µã‚¤ã‚º {uncompressed_size_mb:.1f}MBï¼ˆé–¾å€¤ {broadcast_threshold_mb:.1f}MB ä»¥ä¸‹ã ãŒå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ {broadcast_safe_mb:.1f}MB è¶…éï¼‰ã§æ¡ä»¶ä»˜ãBROADCASTæ¨å¥¨ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®šã€3.0å€åœ§ç¸®ç‡ï¼‰"
            })
        elif uncompressed_size_mb > broadcast_max_mb:
            # æ˜ã‚‰ã‹ã«å¤§ãã™ãã‚‹ï¼ˆ300MBè¶…ï¼‰
            large_tables.append(scan)
            size_analysis["broadcast_decision"] = "not_recommended"
            size_analysis["decision_reasoning"] = f"éåœ§ç¸®{uncompressed_size_mb:.1f}MB > æœ€å¤§é–¾å€¤{broadcast_max_mb:.1f}MB"
            broadcast_analysis["reasoning"].append(f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_display_name}: éåœ§ç¸®{uncompressed_size_mb:.1f}MB - BROADCASTä¸å¯ï¼ˆ>{broadcast_max_mb:.1f}MBï¼‰")
        else:
            # ä¸­é–“ã‚µã‚¤ã‚ºã®ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ30-300MBï¼‰
            large_tables.append(scan)
            size_analysis["broadcast_decision"] = "not_recommended"
            size_analysis["decision_reasoning"] = f"éåœ§ç¸®{uncompressed_size_mb:.1f}MB > é–¾å€¤{broadcast_threshold_mb:.1f}MB"
            broadcast_analysis["reasoning"].append(f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_display_name}: éåœ§ç¸®{uncompressed_size_mb:.1f}MB - BROADCASTéæ¨å¥¨ï¼ˆ>{broadcast_threshold_mb:.1f}MBé–¾å€¤ï¼‰")
        
        broadcast_analysis["detailed_size_analysis"].append(size_analysis)
    
    # åœ§ç¸®åˆ†æã‚µãƒãƒªãƒ¼ã®æ›´æ–°
    if scan_nodes:
        total_uncompressed_mb = sum(scan["estimated_uncompressed_mb"] for scan in scan_nodes)
        total_compressed_mb = sum(scan["estimated_compressed_mb"] for scan in scan_nodes)
        if total_compressed_mb > 0:
            broadcast_analysis["compression_analysis"]["avg_compression_ratio"] = total_uncompressed_mb / total_compressed_mb
        broadcast_analysis["compression_analysis"]["total_uncompressed_mb"] = total_uncompressed_mb
        broadcast_analysis["compression_analysis"]["total_compressed_mb"] = total_compressed_mb
    
    # ç·ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é‡ã¨ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆåœ§ç¸®ãƒ™ãƒ¼ã‚¹ï¼‰
    total_read_gb = overall_metrics.get('read_bytes', 0) / 1024 / 1024 / 1024
    estimated_total_compressed_mb = sum(scan["estimated_compressed_mb"] for scan in scan_nodes)
    
    if estimated_total_compressed_mb > 0:
        size_ratio = (total_read_gb * 1024) / estimated_total_compressed_mb
        if size_ratio > 3 or size_ratio < 0.3:
            broadcast_analysis["reasoning"].append(f"æ¨å®šåœ§ç¸®ã‚µã‚¤ã‚º({estimated_total_compressed_mb:.1f}MB)ã¨å®Ÿèª­ã¿è¾¼ã¿é‡({total_read_gb:.1f}GB)ã«ä¹–é›¢ã‚ã‚Š - ã‚µã‚¤ã‚ºæ¨å®šã«æ³¨æ„")
        else:
            broadcast_analysis["reasoning"].append(f"ã‚µã‚¤ã‚ºæ¨å®šæ•´åˆæ€§: æ¨å®šåœ§ç¸®{estimated_total_compressed_mb:.1f}MB vs å®Ÿéš›{total_read_gb:.1f}GBï¼ˆæ¯”ç‡:{size_ratio:.2f}ï¼‰")
    
    # BROADCASTæ¨å¥¨äº‹é …ã®ç”Ÿæˆï¼ˆ30MBé–¾å€¤å¯¾å¿œã€æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã‚’è€ƒæ…®ï¼‰
    total_broadcast_candidates = len(small_tables) + len(marginal_tables)
    total_tables = len(scan_nodes)
    
    if small_tables or marginal_tables:
        if large_tables:
            # æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã‚’è€ƒæ…®ã—ãŸåˆ¤å®š
            if broadcast_analysis["already_optimized"]:
                broadcast_analysis["feasibility"] = "already_optimized_with_improvements"
                broadcast_analysis["recommendations"] = [
                    f"âœ… æ—¢ã«BROADCAST JOINé©ç”¨æ¸ˆã¿ - è¿½åŠ æ”¹å–„ã®æ¤œè¨",
                    f"ğŸ¯ è¿½åŠ æœ€é©åŒ–ãƒ†ãƒ¼ãƒ–ãƒ«: {total_broadcast_candidates}å€‹ï¼ˆå…¨{total_tables}å€‹ä¸­ï¼‰",
                    f"  âœ… å¼·ãæ¨å¥¨: {len(small_tables)}å€‹ï¼ˆå®‰å…¨é–¾å€¤{broadcast_safe_mb:.1f}MBä»¥ä¸‹ï¼‰",
                    f"  âš ï¸ æ¡ä»¶ä»˜ãæ¨å¥¨: {len(marginal_tables)}å€‹ï¼ˆé–¾å€¤{broadcast_threshold_mb:.1f}MBä»¥ä¸‹ã€è¦æ³¨æ„ï¼‰",
                    f"  âŒ éæ¨å¥¨: {len(large_tables)}å€‹ï¼ˆé–¾å€¤è¶…éï¼‰"
                ]
            else:
                broadcast_analysis["feasibility"] = "recommended"
                broadcast_analysis["recommendations"] = [
                    f"ğŸ¯ BROADCASTæ¨å¥¨ãƒ†ãƒ¼ãƒ–ãƒ«: {total_broadcast_candidates}å€‹ï¼ˆå…¨{total_tables}å€‹ä¸­ï¼‰",
                    f"  âœ… å¼·ãæ¨å¥¨: {len(small_tables)}å€‹ï¼ˆå®‰å…¨é–¾å€¤{broadcast_safe_mb:.1f}MBä»¥ä¸‹ï¼‰",
                    f"  âš ï¸ æ¡ä»¶ä»˜ãæ¨å¥¨: {len(marginal_tables)}å€‹ï¼ˆé–¾å€¤{broadcast_threshold_mb:.1f}MBä»¥ä¸‹ã€è¦æ³¨æ„ï¼‰",
                    f"  âŒ éæ¨å¥¨: {len(large_tables)}å€‹ï¼ˆé–¾å€¤è¶…éï¼‰"
                ]
        else:
            # å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå°ã•ã„å ´åˆ
            if broadcast_analysis["already_optimized"]:
                broadcast_analysis["feasibility"] = "already_optimized_complete"
                broadcast_analysis["recommendations"] = [
                    f"âœ… æ—¢ã«BROADCAST JOINé©ç”¨æ¸ˆã¿ - æœ€é©åŒ–å®Œäº†",
                    f"ğŸ¯ å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ{total_tables}å€‹ï¼‰ãŒBROADCASTé–¾å€¤ä»¥ä¸‹ã§é©åˆ‡ã«å‡¦ç†æ¸ˆã¿",
                    f"  âœ… å¼·ãæ¨å¥¨: {len(small_tables)}å€‹",
                    f"  âš ï¸ æ¡ä»¶ä»˜ãæ¨å¥¨: {len(marginal_tables)}å€‹",
                    "ğŸ“‹ ç¾åœ¨ã®è¨­å®šãŒæœ€é©ã§ã™"
                ]
            else:
                broadcast_analysis["feasibility"] = "all_small"
                broadcast_analysis["recommendations"] = [
                    f"ğŸ¯ å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ{total_tables}å€‹ï¼‰ãŒBROADCASTé–¾å€¤ä»¥ä¸‹",
                    f"  âœ… å¼·ãæ¨å¥¨: {len(small_tables)}å€‹",
                    f"  âš ï¸ æ¡ä»¶ä»˜ãæ¨å¥¨: {len(marginal_tables)}å€‹",
                    "ğŸ“‹ æœ€å°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å„ªå…ˆçš„ã«BROADCASTã™ã‚‹ã“ã¨ã‚’æ¨å¥¨"
                ]
        
        # å…·ä½“çš„ãªBROADCASTå€™è£œã®è©³ç´°
        for small_table in small_tables:
            broadcast_analysis["recommendations"].append(
                f"ğŸ”¹ BROADCAST({small_table['node_name']}) - éåœ§ç¸®{small_table['estimated_uncompressed_mb']:.1f}MBï¼ˆåœ§ç¸®{small_table['estimated_compressed_mb']:.1f}MBã€{small_table['file_format']}ã€åœ§ç¸®ç‡{small_table['compression_ratio']:.1f}xï¼‰"
            )
        
        for marginal_table in marginal_tables:
            broadcast_analysis["recommendations"].append(
                f"ğŸ”¸ BROADCAST({marginal_table['node_name']}) - éåœ§ç¸®{marginal_table['estimated_uncompressed_mb']:.1f}MBï¼ˆæ¡ä»¶ä»˜ãã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¦æ³¨æ„ï¼‰"
            )
            
    elif large_tables:
        broadcast_analysis["feasibility"] = "not_recommended"
        broadcast_analysis["recommendations"] = [
            f"âŒ å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ{len(large_tables)}å€‹ï¼‰ãŒ30MBé–¾å€¤è¶…éã®ãŸã‚BROADCASTéæ¨å¥¨",
            f"ğŸ“Š æœ€å°ãƒ†ãƒ¼ãƒ–ãƒ«ã§ã‚‚éåœ§ç¸®{min(scan['estimated_uncompressed_mb'] for scan in large_tables):.1f}MB",
            "ğŸ”§ ä»£æ›¿æœ€é©åŒ–æ‰‹æ³•ã‚’æ¨å¥¨:",
            "  â€¢ Liquid Clusteringå®Ÿè£…",
            "  â€¢ ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°",
            "  â€¢ ã‚¯ã‚¨ãƒªæœ€é©åŒ–ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ç­‰ï¼‰",
            "  â€¢ spark.databricks.optimizer.autoBroadcastJoinThresholdè¨­å®šå€¤ã®èª¿æ•´æ¤œè¨"
        ]
    else:
        broadcast_analysis["feasibility"] = "insufficient_data"
        broadcast_analysis["recommendations"] = [
            "âš ï¸ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€æ‰‹å‹•ã§ã®ã‚µã‚¤ã‚ºç¢ºèªãŒå¿…è¦",
            "ğŸ“‹ ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã‚’ç¢ºèª:",
            "  â€¢ DESCRIBE DETAIL table_name",
            "  â€¢ SELECT COUNT(*) FROM table_name",
            "  â€¢ SHOW TABLE EXTENDED LIKE 'table_name'"
        ]
    
    # 30MBé–¾å€¤ã«ãƒ’ãƒƒãƒˆã™ã‚‹ç‰¹åˆ¥ãªã‚±ãƒ¼ã‚¹åˆ†æï¼ˆsmall_tables + marginal_tables ã‚’è€ƒæ…®ï¼‰
    all_30mb_candidates = small_tables + marginal_tables  # 30MBä»¥ä¸‹ã®å…¨å€™è£œ
    
    if all_30mb_candidates:
        broadcast_analysis["30mb_hit_analysis"] = {
            "has_30mb_candidates": True,
            "candidate_count": len(all_30mb_candidates),
            "small_tables_count": len(small_tables),  # 24MBä»¥ä¸‹ï¼ˆå¼·ãæ¨å¥¨ï¼‰
            "marginal_tables_count": len(marginal_tables),  # 24-30MBï¼ˆæ¡ä»¶ä»˜ãæ¨å¥¨ï¼‰
            "smallest_table_mb": min(scan["estimated_uncompressed_mb"] for scan in all_30mb_candidates),
            "largest_candidate_mb": max(scan["estimated_uncompressed_mb"] for scan in all_30mb_candidates),
            "total_candidate_size_mb": sum(scan["estimated_uncompressed_mb"] for scan in all_30mb_candidates),
            "recommended_broadcast_table": all_30mb_candidates[0]["node_name"] if all_30mb_candidates else None,
            "memory_impact_estimation": f"{sum(scan['estimated_uncompressed_mb'] for scan in all_30mb_candidates):.1f}MB ãŒãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã«ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ"
        }
        
        # æœ€é©ãªBROADCASTå€™è£œã®ç‰¹å®šï¼ˆå…¨30MBå€™è£œã‹ã‚‰é¸æŠï¼‰
        if len(all_30mb_candidates) > 1:
            optimal_candidate = min(all_30mb_candidates, key=lambda x: x["estimated_uncompressed_mb"])
            broadcast_analysis["30mb_hit_analysis"]["optimal_candidate"] = {
                "table": optimal_candidate["node_name"],
                "size_mb": optimal_candidate["estimated_uncompressed_mb"],
                "rows": optimal_candidate["rows"],
                "reasoning": f"æœ€å°ã‚µã‚¤ã‚º{optimal_candidate['estimated_uncompressed_mb']:.1f}MBã§æœ€ã‚‚åŠ¹ç‡çš„"
            }
        
        # 30MBé–¾å€¤å†…ã®è©³ç´°åˆ†é¡æƒ…å ±ã‚’è¿½åŠ 
        broadcast_analysis["30mb_hit_analysis"]["size_classification"] = {
            "safe_zone_tables": len(small_tables),  # 0-24MBï¼ˆå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³å†…ï¼‰
            "caution_zone_tables": len(marginal_tables),  # 24-30MBï¼ˆè¦æ³¨æ„ï¼‰
            "safe_zone_description": "24MBä»¥ä¸‹ï¼ˆå¼·ãæ¨å¥¨ã€å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³å†…ï¼‰",
            "caution_zone_description": "24-30MBï¼ˆæ¡ä»¶ä»˜ãæ¨å¥¨ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¦æ³¨æ„ï¼‰"
        }
    else:
        broadcast_analysis["30mb_hit_analysis"] = {
            "has_30mb_candidates": False,
            "reason": f"å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ãŒ30MBé–¾å€¤ã‚’è¶…éï¼ˆæœ€å°: {min(scan['estimated_uncompressed_mb'] for scan in scan_nodes):.1f}MBï¼‰" if scan_nodes else "ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ãªã—"
        }
    
    return broadcast_analysis

def generate_optimized_query_with_llm(original_query: str, analysis_result: str, metrics: Dict[str, Any]) -> str:
    """
    LLMåˆ†æçµæœã«åŸºã¥ã„ã¦SQLã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ï¼ˆBROADCASTåˆ†æã‚’å«ã‚€ï¼‰
    """
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã®æŠ½å‡ºï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ï¼‰
    profiler_data = metrics.get('raw_profiler_data', {})
    plan_info = None
    if profiler_data:
        plan_info = extract_execution_plan_info(profiler_data)
    
    # BROADCASTé©ç”¨å¯èƒ½æ€§ã®åˆ†æï¼ˆãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’å«ã‚€ï¼‰
    broadcast_analysis = analyze_broadcast_feasibility(metrics, original_query, plan_info)
    
    # ãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã§ä½¿ç”¨ï¼‰
    if plan_info:
        metrics['execution_plan_info'] = plan_info
    
    # æœ€é©åŒ–ã®ãŸã‚ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’æº–å‚™
    optimization_context = []
    
    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æƒ…å ±ã®æŠ½å‡º
    bottlenecks = metrics.get('bottleneck_indicators', {})
    
    if bottlenecks.get('has_spill', False):
        spill_gb = bottlenecks.get('spill_bytes', 0) / 1024 / 1024 / 1024
        optimization_context.append(f"ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ: {spill_gb:.1f}GB - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®æ”¹å–„ãŒå¿…è¦")
    
    if bottlenecks.get('has_shuffle_bottleneck', False):
        optimization_context.append("ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ - JOINã¨GROUP BYã®æœ€é©åŒ–ãŒå¿…è¦")
    
    if bottlenecks.get('cache_hit_ratio', 0) < 0.5:
        optimization_context.append("ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ä½ä¸‹ - ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ€é©åŒ–ãŒå¿…è¦")
    
    # Liquid Clusteringæ¨å¥¨æƒ…å ±ï¼ˆLLMãƒ™ãƒ¼ã‚¹å¯¾å¿œï¼‰
    liquid_analysis = metrics.get('liquid_clustering_analysis', {})
    extracted_data = liquid_analysis.get('extracted_data', {})
    table_info = extracted_data.get('table_info', {})
    
    clustering_recommendations = []
    if table_info:
        for table_name in list(table_info.keys())[:3]:  # ä¸Šä½3ãƒ†ãƒ¼ãƒ–ãƒ«
            clustering_recommendations.append(f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_name}: LLMåˆ†æã«ã‚ˆã‚‹æ¨å¥¨ã‚«ãƒ©ãƒ ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¨å¥¨")
    
    # æœ€é©åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆï¼ˆç°¡æ½”ç‰ˆã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿ï¼‰
    
    # åˆ†æçµæœã‚’ç°¡æ½”åŒ–ï¼ˆ128Kåˆ¶é™å†…ã§æœ€å¤§åŠ¹ç‡åŒ–ï¼‰
    analysis_summary = ""
    if isinstance(analysis_result, str) and len(analysis_result) > 2000:
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®¹é‡ã®ç¢ºä¿ã®ãŸã‚ã€åˆ†æçµæœã¯è¦ç‚¹ã®ã¿ã«åœ§ç¸®
        analysis_summary = analysis_result[:2000] + "...[è¦ç´„ï¼šä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ã¿ä¿æŒ]"
    else:
        analysis_summary = str(analysis_result)
    
    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æƒ…å ±ã®ç°¡æ½”åŒ–
    bottleneck_summary = "ã€".join(optimization_context[:3]) if optimization_context else "ç‰¹ã«ãªã—"
    
    # Liquid Clusteringæ¨å¥¨ã®ç°¡æ½”åŒ–
    clustering_summary = "ã€".join(clustering_recommendations[:2]) if clustering_recommendations else "ç‰¹ã«ãªã—"
    
    # BROADCASTåˆ†æçµæœã®ã‚µãƒãƒªãƒ¼ä½œæˆï¼ˆ30MBé–¾å€¤å¯¾å¿œï¼‰
    broadcast_summary = []
    if broadcast_analysis["is_join_query"]:
        # æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã‚’æœ€åˆã«è¡¨ç¤º
        if broadcast_analysis["already_optimized"]:
            existing_broadcast_count = len(broadcast_analysis["existing_broadcast_nodes"])
            broadcast_summary.append(f"âœ… æ—¢ã«BROADCAST JOINé©ç”¨æ¸ˆã¿: {existing_broadcast_count}å€‹ã®ãƒãƒ¼ãƒ‰")
            
            # BROADCASTã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
            broadcast_applied_tables = broadcast_analysis.get("broadcast_applied_tables", [])
            if broadcast_applied_tables:
                broadcast_summary.append(f"ğŸ“‹ BROADCASTã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(broadcast_applied_tables)}")
            
            # æ—¢å­˜ã®BROADCASTãƒãƒ¼ãƒ‰ã®è©³ç´°ã‚’è¡¨ç¤ºï¼ˆæœ€å¤§3å€‹ï¼‰
            for i, node in enumerate(broadcast_analysis["existing_broadcast_nodes"][:3]):
                node_name_short = node["node_name"][:50] + "..." if len(node["node_name"]) > 50 else node["node_name"]
                broadcast_summary.append(f"  ğŸ”¹ BROADCAST Node {i+1}: {node_name_short}")
            
            # å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æã‹ã‚‰ã®JOINæˆ¦ç•¥æƒ…å ±
            plan_analysis = broadcast_analysis.get("execution_plan_analysis", {})
            if plan_analysis.get("unique_join_strategies"):
                broadcast_summary.append(f"ğŸ” æ¤œå‡ºã•ã‚ŒãŸJOINæˆ¦ç•¥: {', '.join(plan_analysis['unique_join_strategies'])}")
        else:
            broadcast_summary.append("ğŸ” BROADCAST JOINæœªé©ç”¨ - æœ€é©åŒ–ã®æ©Ÿä¼šã‚’æ¤œè¨ä¸­")
        
        broadcast_summary.append(f"ğŸ¯ BROADCASTé©ç”¨å¯èƒ½æ€§: {broadcast_analysis['feasibility']}")
        broadcast_summary.append(f"âš–ï¸ Sparké–¾å€¤: {broadcast_analysis['spark_threshold_mb']:.1f}MBï¼ˆéåœ§ç¸®ï¼‰")
        
        # 30MBä»¥ä¸‹ã®å€™è£œãŒã‚ã‚‹å ´åˆ
        if broadcast_analysis["30mb_hit_analysis"]["has_30mb_candidates"]:
            hit_analysis = broadcast_analysis["30mb_hit_analysis"]
            broadcast_summary.append(f"âœ… 30MBé–¾å€¤ãƒ’ãƒƒãƒˆ: {hit_analysis['candidate_count']}å€‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ãŒæ¡ä»¶é©åˆ")
            broadcast_summary.append(f"ğŸ“Š å€™è£œã‚µã‚¤ã‚ºç¯„å›²: {hit_analysis['smallest_table_mb']:.1f}MB - {hit_analysis['largest_candidate_mb']:.1f}MB")
            
            if "optimal_candidate" in hit_analysis:
                optimal = hit_analysis["optimal_candidate"]
                broadcast_summary.append(f"ğŸ† æœ€é©å€™è£œ: {optimal['table']} ({optimal['size_mb']:.1f}MB)")
        else:
            broadcast_summary.append(f"âŒ 30MBé–¾å€¤ãƒ’ãƒƒãƒˆãªã—: {broadcast_analysis['30mb_hit_analysis']['reason']}")
        
        # BROADCASTå€™è£œã®è©³ç´°ï¼ˆæœ€å¤§3å€‹ï¼‰
        if broadcast_analysis["broadcast_candidates"]:
            broadcast_summary.append("ğŸ“‹ BROADCASTå€™è£œè©³ç´°:")
            for i, candidate in enumerate(broadcast_analysis["broadcast_candidates"][:3]):
                confidence_icon = "ğŸ”¹" if candidate['confidence'] == 'high' else "ğŸ”¸"
                # æ—¢ã«BROADCASTæ¸ˆã¿ã‹ã©ã†ã‹ã‚’è¡¨ç¤º
                already_broadcasted = candidate.get('is_already_broadcasted', False)
                status_icon = "âœ…" if already_broadcasted else "ğŸ’¡"
                status_text = "æ—¢ã«é©ç”¨æ¸ˆã¿" if already_broadcasted else "é©ç”¨æ¨å¥¨"
                
                broadcast_summary.append(
                    f"  {confidence_icon} {candidate['table']}: éåœ§ç¸®{candidate['estimated_uncompressed_mb']:.1f}MB "
                    f"(åœ§ç¸®{candidate['estimated_compressed_mb']:.1f}MB, {candidate['file_format']}, "
                    f"åœ§ç¸®ç‡{candidate['compression_ratio']:.1f}x) {status_icon} {status_text}"
                )
        
        # æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã‚’è€ƒæ…®ã—ãŸæ¨å¥¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        if broadcast_analysis["already_optimized"]:
            if broadcast_analysis["feasibility"] in ["recommended", "all_small"]:
                broadcast_summary.append("ğŸ’¡ è¿½åŠ æœ€é©åŒ–: å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã¯æ—¢ã«æœ€é©åŒ–æ¸ˆã¿ã§ã™ãŒã€æ›´ãªã‚‹æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™")
            else:
                broadcast_summary.append("âœ… æœ€é©åŒ–å®Œäº†: å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã¯é©åˆ‡ã«BROADCAST JOINãŒé©ç”¨ã•ã‚Œã¦ã„ã¾ã™")
        else:
            if broadcast_analysis["feasibility"] in ["recommended", "all_small"]:
                broadcast_summary.append("ğŸš€ æœ€é©åŒ–æ¨å¥¨: BROADCASTãƒ’ãƒ³ãƒˆã®é©ç”¨ã«ã‚ˆã‚Šå¤§å¹…ãªæ€§èƒ½æ”¹å–„ãŒæœŸå¾…ã§ãã¾ã™")
            elif broadcast_analysis["feasibility"] == "not_recommended":
                broadcast_summary.append("âš ï¸ æœ€é©åŒ–å›°é›£: ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ããã€BROADCASTé©ç”¨ã¯æ¨å¥¨ã•ã‚Œã¾ã›ã‚“")
        
        # é‡è¦ãªæ³¨æ„äº‹é …
        if broadcast_analysis["reasoning"]:
            broadcast_summary.append("âš ï¸ é‡è¦ãªæ³¨æ„äº‹é …:")
            for reason in broadcast_analysis["reasoning"][:3]:  # æœ€å¤§3å€‹ã«æ‹¡å¼µ
                broadcast_summary.append(f"  â€¢ {reason}")
    else:
        broadcast_summary.append("âŒ JOINã‚¯ã‚¨ãƒªã§ã¯ãªã„ãŸã‚ã€BROADCASTãƒ’ãƒ³ãƒˆé©ç”¨å¯¾è±¡å¤–")
    
    optimization_prompt = f"""
ã‚ãªãŸã¯Databricksã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«SQLã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªå‡¦ç†æ–¹é‡ã€‘
- ä¸€å›ã®å‡ºåŠ›ã§å®Œå…¨ãªSQLã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„
- æ®µéšçš„ãªå‡ºåŠ›ã‚„è¤‡æ•°å›ã«åˆ†ã‘ã¦ã®å‡ºåŠ›ã¯ç¦æ­¢ã§ã™
- thinkingæ©Ÿèƒ½ã§æ§‹é€ ç†è§£â†’ä¸€å›ã§å®Œå…¨ãªSQLå‡ºåŠ›

ã€å…ƒã®SQLã‚¯ã‚¨ãƒªã€‘
```sql
{original_query}
```

ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æçµæœã€‘
{analysis_summary}

ã€ç‰¹å®šã•ã‚ŒãŸãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã€‘
{chr(10).join(optimization_context) if optimization_context else "ä¸»è¦ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"}

ã€BROADCASTåˆ†æçµæœã€‘
{chr(10).join(broadcast_summary)}

ã€Liquid Clusteringæ¨å¥¨ã€‘
{chr(10).join(clustering_recommendations) if clustering_recommendations else "ç‰¹åˆ¥ãªæ¨å¥¨äº‹é …ã¯ã‚ã‚Šã¾ã›ã‚“"}

ã€æœ€é©åŒ–è¦æ±‚ã€‘
1. ä¸Šè¨˜ã®åˆ†æçµæœã«åŸºã¥ã„ã¦ã€å…ƒã®SQLã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„
2. æœ€é©åŒ–ã®ãƒã‚¤ãƒ³ãƒˆã‚’å…·ä½“çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„
3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã®è¦‹è¾¼ã¿ã‚’å®šé‡çš„ã«ç¤ºã—ã¦ãã ã•ã„
4. å®Ÿè¡Œå¯èƒ½ãªSQLã‚³ãƒ¼ãƒ‰ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„
5. å¿…ãšåŒã˜çµæœã‚»ãƒƒãƒˆã‚’è¿”å´ã™ã‚‹ã‚¯ã‚¨ãƒªã«ã—ã¦ãã ã•ã„
6. PHOTONã‚¨ãƒ³ã‚¸ãƒ³ã®åˆ©ç”¨ã‚’å„ªå…ˆã—ã¦ãã ã•ã„
7. Whereå¥ã§LiquidClusteringãŒåˆ©ç”¨ã§ãã‚‹å ´åˆã¯åˆ©ç”¨ã§ãã‚‹æ›¸å¼ã‚’å„ªå…ˆã—ã¦ãã ã•ã„
8. åŒä¸€ãƒ‡ãƒ¼ã‚¿ã‚’ç¹°ã‚Šè¿”ã—å‚ç…§ã™ã‚‹å ´åˆã¯CTEã§å…±é€šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦å®šç¾©ã—ã¦ãã ã•ã„
9. Liquid Clusteringå®Ÿè£…æ™‚ã¯æ­£ã—ã„Databricks SQLæ§‹æ–‡ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼ˆALTER TABLE table_name CLUSTER BY (column1, column2, ...)ï¼‰
10. **BROADCASTåˆ†æçµæœã‚’å³å¯†ã«åæ˜ ã—ã¦ãã ã•ã„ï¼ˆ30MBé–¾å€¤ï¼‰**ï¼š
    - Sparkæ¨™æº–ã®30MBé–¾å€¤ï¼ˆéåœ§ç¸®ï¼‰ã‚’å³æ ¼ã«é©ç”¨
    - BROADCASTé©ç”¨æ¨å¥¨ï¼ˆrecommended/all_smallï¼‰ã®å ´åˆã®ã¿BROADCASTãƒ’ãƒ³ãƒˆï¼ˆ/*+ BROADCAST(table_name) */ï¼‰ã‚’é©ç”¨
    - **é‡è¦: BROADCASTãƒ’ãƒ³ãƒˆå¥ã¯å¿…ãšSELECTæ–‡ã®ç›´å¾Œã«é…ç½®ã—ã¦ãã ã•ã„**
      ```sql
      SELECT /*+ BROADCAST(table_name) */
        column1, column2, ...
      FROM table1
        JOIN table2 ON ...
      ```
    - **çµ¶å¯¾ã«é¿ã‘ã‚‹ã¹ãèª¤ã£ãŸé…ç½®:**
      ```sql
      -- âŒ é–“é•ã„: JOINå¥ã®ä¸­ã«ãƒ’ãƒ³ãƒˆã‚’é…ç½®
      JOIN /*+ BROADCAST(table_name) */ table2 ON ...
      ```
    - éåœ§ç¸®ã‚µã‚¤ã‚ºãŒ30MBä»¥ä¸‹ã®å°ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿BROADCASTå¯¾è±¡
    - 30MBè¶…éã®å¤§ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯çµ¶å¯¾ã«BROADCASTãƒ’ãƒ³ãƒˆã‚’é©ç”¨ã—ãªã„
    - æ¡ä»¶ä»˜ãæ¨å¥¨ï¼ˆ24-30MBï¼‰ã®å ´åˆã¯è¦æ³¨æ„ã¨ã—ã¦æ˜è¨˜
    - BROADCASTé©ç”¨æ™‚ã¯å¿…ãšæ¨å®šéåœ§ç¸®ã‚µã‚¤ã‚ºã€åœ§ç¸®ã‚µã‚¤ã‚ºã€åœ§ç¸®ç‡ã‚’èª¬æ˜ã«è¨˜è¼‰
    - JOINã‚¯ã‚¨ãƒªã§ãªã„å ´åˆã¯BROADCASTãƒ’ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ãªã„
    - 30MBé–¾å€¤ãƒ’ãƒƒãƒˆãŒãªã„å ´åˆã¯ä»£æ›¿æœ€é©åŒ–æ‰‹æ³•ã‚’ææ¡ˆ

ã€é‡è¦ãªåˆ¶ç´„ã€‘
- çµ¶å¯¾ã«ä¸å®Œå…¨ãªã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ãªã„ã§ãã ã•ã„
- ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ åã€ãƒ†ãƒ¼ãƒ–ãƒ«åã€CTEåã‚’å®Œå…¨ã«è¨˜è¿°ã—ã¦ãã ã•ã„
- ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆ...ã€[çœç•¥]ã€ç©ºç™½ãªã©ï¼‰ã¯ä¸€åˆ‡ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„
- ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®ã™ã¹ã¦ã®SELECTé …ç›®ã‚’ä¿æŒã—ã¦ãã ã•ã„
- å…ƒã®ã‚¯ã‚¨ãƒªãŒé•·ã„å ´åˆã§ã‚‚ã€ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ã‚’çœç•¥ã›ãšã«è¨˜è¿°ã—ã¦ãã ã•ã„
- å®Ÿéš›ã«å®Ÿè¡Œã§ãã‚‹å®Œå…¨ãªSQLã‚¯ã‚¨ãƒªã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„
- **BROADCASTãƒ’ãƒ³ãƒˆã¯å¿…ãšSELECTæ–‡ã®ç›´å¾Œã«é…ç½®ã—ã€JOINå¥å†…ã«ã¯çµ¶å¯¾ã«é…ç½®ã—ãªã„ã§ãã ã•ã„**

ã€å‡ºåŠ›å½¢å¼ã€‘ï¼ˆç°¡æ½”ç‰ˆï¼‰
## æœ€é©åŒ–ã•ã‚ŒãŸSQL

**çµ¶å¯¾æ¡ä»¶: çœç•¥ãƒ»ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç¦æ­¢**

```sql
[å®Œå…¨ãªSQL - ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ãƒ»CTEãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’çœç•¥ãªã—ã§è¨˜è¿°]
```

## æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ
[3ã¤ã®ä¸»è¦æ”¹å–„ç‚¹]

## BROADCASTé©ç”¨æ ¹æ‹ ï¼ˆ30MBé–¾å€¤åŸºæº–ï¼‰
[BROADCASTãƒ’ãƒ³ãƒˆé©ç”¨ã®è©³ç´°æ ¹æ‹ ]
- ğŸ“ Sparké–¾å€¤: 30MBï¼ˆéåœ§ç¸®ã€spark.databricks.optimizer.autoBroadcastJoinThresholdï¼‰
- ğŸ¯ é©ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«: [ãƒ†ãƒ¼ãƒ–ãƒ«å]
  - éåœ§ç¸®æ¨å®šã‚µã‚¤ã‚º: [XX]MB
  - åœ§ç¸®æ¨å®šã‚µã‚¤ã‚º: [YY]MB
  - æ¨å®šåœ§ç¸®ç‡: [ZZ]x
  - ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: [parquet/delta/ç­‰]
  - æ¨å®šæ ¹æ‹ : [è¡Œæ•°ãƒ»ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é‡ãƒ™ãƒ¼ã‚¹]
- âš–ï¸ åˆ¤å®šçµæœ: [strongly_recommended/conditionally_recommended/not_recommended]
- ğŸ” é–¾å€¤é©åˆæ€§: [30MBä»¥ä¸‹ã§é©åˆ/30MBè¶…éã§éé©åˆ]
- ğŸ’¾ ãƒ¡ãƒ¢ãƒªå½±éŸ¿: [æ¨å®šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡]MB ãŒãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã«ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ
- ğŸš€ æœŸå¾…åŠ¹æœ: [ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è»¢é€é‡å‰Šæ¸›ãƒ»JOINå‡¦ç†é«˜é€ŸåŒ–ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‰Šæ¸›ãªã©]
- âœ… **ãƒ’ãƒ³ãƒˆå¥é…ç½®**: SELECTæ–‡ã®ç›´å¾Œã« `/*+ BROADCAST(table_name) */` ã‚’æ­£ã—ãé…ç½®

## æœŸå¾…åŠ¹æœ  
[å®Ÿè¡Œæ™‚é–“ãƒ»ãƒ¡ãƒ¢ãƒªãƒ»ã‚¹ãƒ”ãƒ«æ”¹å–„ã®è¦‹è¾¼ã¿ï¼ˆBROADCASTåŠ¹æœã‚’å«ã‚€ï¼‰]
"""

    # è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
    provider = LLM_CONFIG["provider"]
    
    try:
        if provider == "databricks":
            optimized_result = _call_databricks_llm(optimization_prompt)
        elif provider == "openai":
            optimized_result = _call_openai_llm(optimization_prompt)
        elif provider == "azure_openai":
            optimized_result = _call_azure_openai_llm(optimization_prompt)
        elif provider == "anthropic":
            optimized_result = _call_anthropic_llm(optimization_prompt)
        else:
            return "âš ï¸ è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒèªè­˜ã§ãã¾ã›ã‚“"
        
        # thinking_enabled: Trueã®å ´åˆã«optimized_resultãŒãƒªã‚¹ãƒˆã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚å¯¾å¿œ
        # ã“ã“ã§ã¯å…ƒã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã‚’ä¿æŒã—ã¦è¿”ã™ï¼ˆå¾Œã§ç”¨é€”ã«å¿œã˜ã¦å¤‰æ›ï¼‰
        return optimized_result
        
    except Exception as e:
        return f"âš ï¸ SQLæœ€é©åŒ–ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def generate_top10_time_consuming_processes_report(extracted_metrics: Dict[str, Any]) -> str:
    """
    æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’æ–‡å­—åˆ—ã¨ã—ã¦ç”Ÿæˆ
    """
    report_lines = []
    report_lines.append("## ğŸŒ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10")
    report_lines.append("=" * 80)
    report_lines.append("ğŸ“Š ã‚¢ã‚¤ã‚³ãƒ³èª¬æ˜: â±ï¸æ™‚é–“ ğŸ’¾ãƒ¡ãƒ¢ãƒª ğŸ”¥ğŸŒä¸¦åˆ—åº¦ ğŸ’¿ã‚¹ãƒ”ãƒ« âš–ï¸ã‚¹ã‚­ãƒ¥ãƒ¼")
    report_lines.append('ğŸ’¿ ã‚¹ãƒ”ãƒ«åˆ¤å®š: "Num bytes spilled to disk due to memory pressure" ã¾ãŸã¯ "Sink - Num bytes spilled to disk due to memory pressure" > 0')
    report_lines.append("ğŸ¯ ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®š: 'AQEShuffleRead - Number of skewed partitions' > 0")
    report_lines.append("")

    # ãƒãƒ¼ãƒ‰ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
    sorted_nodes = sorted(extracted_metrics['node_metrics'], 
                         key=lambda x: x['key_metrics'].get('durationMs', 0), 
                         reverse=True)

    if sorted_nodes:
        # å…¨ä½“ã®å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
        total_duration = sum(node['key_metrics'].get('durationMs', 0) for node in sorted_nodes)
        
        report_lines.append(f"ğŸ“Š å…¨ä½“å®Ÿè¡Œæ™‚é–“: {total_duration:,} ms ({total_duration/1000:.1f} sec)")
        report_lines.append(f"ğŸ“ˆ TOP10åˆè¨ˆæ™‚é–“: {sum(node['key_metrics'].get('durationMs', 0) for node in sorted_nodes[:10]):,} ms")
        report_lines.append("")
        
        for i, node in enumerate(sorted_nodes[:10]):
            rows_num = node['key_metrics'].get('rowsNum', 0)
            duration_ms = node['key_metrics'].get('durationMs', 0)
            memory_mb = node['key_metrics'].get('peakMemoryBytes', 0) / 1024 / 1024
            
            # å…¨ä½“ã«å¯¾ã™ã‚‹æ™‚é–“ã®å‰²åˆã‚’è¨ˆç®—
            time_percentage = (duration_ms / max(total_duration, 1)) * 100
            
            # æ™‚é–“ã®é‡è¦åº¦ã«åŸºã¥ã„ã¦ã‚¢ã‚¤ã‚³ãƒ³ã‚’é¸æŠ
            if duration_ms >= 10000:  # 10ç§’ä»¥ä¸Š
                time_icon = "ğŸ”´"
                severity = "CRITICAL"
            elif duration_ms >= 5000:  # 5ç§’ä»¥ä¸Š
                time_icon = "ğŸŸ "
                severity = "HIGH"
            elif duration_ms >= 1000:  # 1ç§’ä»¥ä¸Š
                time_icon = "ğŸŸ¡"
                severity = "MEDIUM"
            else:
                time_icon = "ğŸŸ¢"
                severity = "LOW"
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ã‚¢ã‚¤ã‚³ãƒ³
            memory_icon = "ğŸ’š" if memory_mb < 100 else "âš ï¸" if memory_mb < 1000 else "ğŸš¨"
            
            # ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹ãƒãƒ¼ãƒ‰åã‚’å–å¾—
            raw_node_name = node['name']
            node_name = get_meaningful_node_name(node, extracted_metrics)
            short_name = node_name[:100] + "..." if len(node_name) > 100 else node_name
            
            # ä¸¦åˆ—åº¦æƒ…å ±ã®å–å¾—
            num_tasks = 0
            for stage in extracted_metrics.get('stage_metrics', []):
                if duration_ms > 0:  # ã“ã®ãƒãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’æ¨å®š
                    num_tasks = max(num_tasks, stage.get('num_tasks', 0))
            
            # ã‚¹ãƒ”ãƒ«æ¤œå‡ºï¼ˆå …ç‰¢ç‰ˆ - ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹ + åŒ…æ‹¬çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œç´¢ï¼‰
            spill_detected = False
            spill_bytes = 0
            spill_detection_method = "none"
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’äº‹å‰ã«è¨ˆç®—ï¼ˆå¾Œã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ä½¿ç”¨ï¼‰
            memory_mb = node['key_metrics'].get('peakMemoryBytes', 0) / (1024 * 1024)
            
            # 2. æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã«ã‚ˆã‚‹æ¤œç´¢ï¼ˆå„ªå…ˆï¼‰
            exact_spill_metrics = [
                "Num bytes spilled to disk due to memory pressure",
                "Sink - Num bytes spilled to disk due to memory pressure",
                "Sink/Num bytes spilled to disk due to memory pressure"
            ]
            
            detailed_metrics = node.get('detailed_metrics', {})
            for metric_key, metric_info in detailed_metrics.items():
                metric_value = metric_info.get('value', 0)
                metric_label = metric_info.get('label', '')
                
                # ã¾ãšæ­£ç¢ºãªåå‰ã§ãƒã‚§ãƒƒã‚¯
                if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                    spill_detected = True
                    spill_bytes = max(spill_bytes, metric_value)
                    spill_detection_method = f"exact_match_detailed ({metric_key})"
                    break
            
            # 3. åŒ…æ‹¬çš„ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œç´¢ï¼ˆæ­£ç¢ºãªåå‰ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆï¼‰
            if not spill_detected:
                for metric_key, metric_info in detailed_metrics.items():
                    metric_value = metric_info.get('value', 0)
                    metric_label = metric_info.get('label', '')
                    
                    # æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ã¿ã‚’ä½¿ç”¨ï¼ˆéƒ¨åˆ†æ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°ç¦æ­¢ï¼‰
                    is_spill_metric = (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics)
                    
                    if is_spill_metric and metric_value > 0:
                        spill_detected = True
                        spill_bytes = max(spill_bytes, metric_value)  # æœ€å¤§å€¤ã‚’ä½¿ç”¨
                        spill_detection_method = f"pattern_match_detailed ({metric_key})"
            
            # 4. raw_metricsã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢
            if not spill_detected:
                raw_metrics = node.get('metrics', [])
                if isinstance(raw_metrics, list):
                    # ã¾ãšæ­£ç¢ºãªåå‰ã§æ¤œç´¢
                    for metric in raw_metrics:
                        metric_key = metric.get('key', '')
                        metric_label = metric.get('label', '')
                        metric_value = metric.get('value', 0)
                        
                        if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                            spill_detected = True
                            spill_bytes = max(spill_bytes, metric_value)
                            spill_detection_method = f"exact_match_raw ({metric_key})"
                            break
                    
                    # æ­£ç¢ºãªåå‰ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
                    if not spill_detected:
                        for metric in raw_metrics:
                            metric_key = metric.get('key', '')
                            metric_label = metric.get('label', '')
                            metric_value = metric.get('value', 0)
                            
                            # æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ã¿ã‚’ä½¿ç”¨ï¼ˆéƒ¨åˆ†æ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°ç¦æ­¢ï¼‰
                            is_spill_metric = (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics)
                            
                            if is_spill_metric and metric_value > 0:
                                spill_detected = True
                                spill_bytes = max(spill_bytes, metric_value)
                                spill_detection_method = f"pattern_match_raw ({metric_key})"
                                break
            
            # 5. key_metricsã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢
            if not spill_detected:
                key_metrics = node.get('key_metrics', {})
                
                # ã¾ãšæ­£ç¢ºãªåå‰ã§æ¤œç´¢
                for exact_metric in exact_spill_metrics:
                    if exact_metric in key_metrics and key_metrics[exact_metric] > 0:
                        spill_detected = True
                        spill_bytes = max(spill_bytes, key_metrics[exact_metric])
                        spill_detection_method = f"exact_match_key ({exact_metric})"
                        break
                
                # æ­£ç¢ºãªåå‰ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
                if not spill_detected:
                    for key_metric_name, key_metric_value in key_metrics.items():
                        # æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ã¿ã‚’ä½¿ç”¨ï¼ˆéƒ¨åˆ†æ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°ç¦æ­¢ï¼‰
                        is_spill_metric = (key_metric_name in exact_spill_metrics)
                        
                        if is_spill_metric and key_metric_value > 0:
                            spill_detected = True
                            spill_bytes = max(spill_bytes, key_metric_value)
                            spill_detection_method = f"pattern_match_key ({key_metric_name})"
                            break
            
            # 6. ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹ã‚¹ãƒ”ãƒ«æ¤œå‡ºï¼ˆæœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            # ã™ã¹ã¦ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œç´¢ã§è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if not spill_detected and memory_mb > 1024:  # 1GBä»¥ä¸Šã§ã‚¹ãƒ”ãƒ«æœªæ¤œå‡ºã®å ´åˆ
                spill_detected = True
                spill_detection_method = "memory_based_fallback"
                # ãƒ¡ãƒ¢ãƒªã®10%ãŒã‚¹ãƒ”ãƒ«ã—ãŸã¨ä»®å®šï¼ˆä¿å®ˆçš„ãªè¦‹ç©ã‚‚ã‚Šï¼‰
                spill_bytes = int(memory_mb * 1024 * 1024 * 0.1)
            
            # ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º: AQEShuffleRead - Number of skewed partitions ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½¿ç”¨ï¼ˆæ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ã¿ï¼‰
            skew_detected = False
            skewed_partitions = 0
            target_skew_metric = "AQEShuffleRead - Number of skewed partitions"
            
            # detailed_metricsã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢
            detailed_metrics = node.get('detailed_metrics', {})
            for metric_key, metric_info in detailed_metrics.items():
                if metric_key == target_skew_metric:
                    try:
                        skewed_partitions = int(metric_info.get('value', 0))
                        if skewed_partitions > 0:
                            skew_detected = True
                        break
                    except (ValueError, TypeError):
                        continue
            
            # key_metricsã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not skew_detected:
                key_metrics = node.get('key_metrics', {})
                if target_skew_metric in key_metrics:
                    try:
                        skewed_partitions = int(key_metrics[target_skew_metric])
                        if skewed_partitions > 0:
                            skew_detected = True
                    except (ValueError, TypeError):
                        pass
            
            # ä¸¦åˆ—åº¦ã‚¢ã‚¤ã‚³ãƒ³
            parallelism_icon = "ğŸ”¥" if num_tasks >= 10 else "âš ï¸" if num_tasks >= 5 else "ğŸŒ"
            # ã‚¹ãƒ”ãƒ«ã‚¢ã‚¤ã‚³ãƒ³
            spill_icon = "ğŸ’¿" if spill_detected else "âœ…"
            # ã‚¹ã‚­ãƒ¥ãƒ¼ã‚¢ã‚¤ã‚³ãƒ³
            skew_icon = "âš–ï¸" if skew_detected else "âœ…"
            
            report_lines.append(f"{i+1:2d}. {time_icon}{memory_icon}{parallelism_icon}{spill_icon}{skew_icon} [{severity:8}] {short_name}")
            report_lines.append(f"    â±ï¸  å®Ÿè¡Œæ™‚é–“: {duration_ms:>8,} ms ({duration_ms/1000:>6.1f} sec) - å…¨ä½“ã® {time_percentage:>5.1f}%")
            report_lines.append(f"    ğŸ“Š å‡¦ç†è¡Œæ•°: {rows_num:>8,} è¡Œ")
            report_lines.append(f"    ğŸ’¾ ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: {memory_mb:>6.1f} MB")
            report_lines.append(f"    ğŸ”§ ä¸¦åˆ—åº¦: {num_tasks:>3d} ã‚¿ã‚¹ã‚¯ | ğŸ’¿ ã‚¹ãƒ”ãƒ«: {'ã‚ã‚Š' if spill_detected else 'ãªã—'} | âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼: {'æ¤œå‡º' if skew_detected else 'ãªã—'}")
            
            # åŠ¹ç‡æ€§æŒ‡æ¨™ï¼ˆè¡Œ/ç§’ï¼‰ã‚’è¨ˆç®—
            if duration_ms > 0:
                rows_per_sec = (rows_num * 1000) / duration_ms
                report_lines.append(f"    ğŸš€ å‡¦ç†åŠ¹ç‡: {rows_per_sec:>8,.0f} è¡Œ/ç§’")
            
            # ã‚¹ãƒ”ãƒ«è©³ç´°æƒ…å ±ã¨ãƒ‡ãƒãƒƒã‚°æƒ…å ±
            if spill_detected:
                if spill_bytes > 0:
                    spill_mb = spill_bytes / 1024 / 1024
                    if spill_mb >= 1024:  # GBå˜ä½
                        spill_display = f"{spill_mb/1024:.2f} GB"
                    else:  # MBå˜ä½
                        spill_display = f"{spill_mb:.1f} MB"
                    
                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’æŠ½å‡ºã—ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                    metric_name = "Unknown"
                    if "exact_match_detailed" in spill_detection_method or "exact_match_raw" in spill_detection_method:
                        # ã‚«ãƒƒã‚³ã®ä¸­ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’æŠ½å‡º
                        start = spill_detection_method.find("(") + 1
                        end = spill_detection_method.find(")")
                        if start > 0 and end > start:
                            metric_name = spill_detection_method[start:end]
                    
                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®è¡¨ç¤ºå½¢å¼ã‚’çµ±ä¸€
                    formatted_display = f"{metric_name}: {spill_display}"
                    report_lines.append(f"    ğŸ’¿ ã‚¹ãƒ”ãƒ«è©³ç´°: {formatted_display}")
                else:
                    report_lines.append(f"    ğŸ’¿ ã‚¹ãƒ”ãƒ«è©³ç´°: æ¤œå‡ºæ¸ˆã¿ ({spill_detection_method})")
            else:
                # ã‚¹ãƒ”ãƒ«æœªæ¤œå‡ºæ™‚ã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆå¤§å®¹é‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨æ™‚ã®ã¿ï¼‰
                if memory_mb > 1024:  # 1GBä»¥ä¸Šãªã®ã«ã‚¹ãƒ”ãƒ«æœªæ¤œå‡ºã®å ´åˆ
                    report_lines.append(f"    ğŸ” ã‚¹ãƒ”ãƒ«ãƒ‡ãƒãƒƒã‚°: ãƒ¡ãƒ¢ãƒª{memory_mb:.1f}MBãªã®ã«æœªæ¤œå‡º")
                    
                    # åˆ©ç”¨å¯èƒ½ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¸€è¦§ã‚’è¡¨ç¤ºï¼ˆ'spill'ã‚„'disk'é–¢é€£ã®ã¿ï¼‰
                    spill_related_metrics = []
                    
                    # detailed_metricsã‹ã‚‰æ¤œç´¢
                    detailed_metrics = node.get('detailed_metrics', {})
                    for key, info in detailed_metrics.items():
                        if (key in exact_spill_metrics or info.get('label', '') in exact_spill_metrics):
                            value = info.get('value', 0)
                            label = info.get('label', '')
                            spill_related_metrics.append(f"detailed[{key}]={value}")
                            if label and label != key:
                                spill_related_metrics.append(f"  label='{label}'")
                    
                    # raw_metricsã‹ã‚‰æ¤œç´¢
                    raw_metrics = node.get('metrics', [])
                    if isinstance(raw_metrics, list):
                        for metric in raw_metrics:
                            key = metric.get('key', '')
                            label = metric.get('label', '')
                            value = metric.get('value', 0)
                            if (key in exact_spill_metrics or label in exact_spill_metrics):
                                spill_related_metrics.append(f"raw[{key}]={value}")
                                if label and label != key:
                                    spill_related_metrics.append(f"  label='{label}'")
                    
                    # key_metricsã‹ã‚‰æ¤œç´¢
                    key_metrics = node.get('key_metrics', {})
                    for key, value in key_metrics.items():
                        if key in exact_spill_metrics:
                            spill_related_metrics.append(f"key[{key}]={value}")
                    
                    if spill_related_metrics:
                        report_lines.append(f"    ğŸ” ç™ºè¦‹ã•ã‚ŒãŸã‚¹ãƒ”ãƒ«é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
                        for metric in spill_related_metrics[:5]:  # æœ€å¤§5å€‹ã¾ã§è¡¨ç¤º
                            report_lines.append(f"      â€¢ {metric}")
                        if len(spill_related_metrics) > 5:
                            report_lines.append(f"      â€¢ ... ä»–{len(spill_related_metrics)-5}å€‹")
                    else:
                        report_lines.append(f"    ğŸ” ã‚¹ãƒ”ãƒ«é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå…¨ãè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # ã‚¹ã‚­ãƒ¥ãƒ¼è©³ç´°æƒ…å ±
            if skew_detected and skewed_partitions > 0:
                report_lines.append(f"    âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼è©³ç´°: {skewed_partitions} å€‹ã®ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ï¼ˆAQEShuffleReadæ¤œå‡ºï¼‰")
            
            # ãƒãƒ¼ãƒ‰IDã‚‚è¡¨ç¤º
            report_lines.append(f"    ğŸ†” ãƒãƒ¼ãƒ‰ID: {node.get('node_id', node.get('id', 'N/A'))}")
            report_lines.append("")
            
    else:
        report_lines.append("âš ï¸ ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    return "\n".join(report_lines)

def save_execution_plan_analysis(plan_info: Dict[str, Any], output_dir: str = "/tmp") -> Dict[str, str]:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    Args:
        plan_info: extract_execution_plan_info()ã®çµæœ
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        
    Returns:
        Dict: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åã®è¾æ›¸
    """
    from datetime import datetime
    import json
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç”Ÿæˆ
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åå®šç¾©
    plan_json_filename = f"output_execution_plan_analysis_{timestamp}.json"
    plan_report_filename = f"output_execution_plan_report_{timestamp}.md"
    
    # JSONå½¢å¼ã§ãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’ä¿å­˜
    with open(plan_json_filename, 'w', encoding='utf-8') as f:
        json.dump(plan_info, f, ensure_ascii=False, indent=2)
    
    # Markdownå½¢å¼ã§ãƒ—ãƒ©ãƒ³åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
    with open(plan_report_filename, 'w', encoding='utf-8') as f:
        report_content = generate_execution_plan_markdown_report(plan_info)
        f.write(report_content)
    
    return {
        'plan_json_file': plan_json_filename,
        'plan_report_file': plan_report_filename
    }

def generate_execution_plan_markdown_report(plan_info: Dict[str, Any]) -> str:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æçµæœã®Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    
    Args:
        plan_info: extract_execution_plan_info()ã®çµæœ
        
    Returns:
        str: Markdownãƒ¬ãƒãƒ¼ãƒˆ
    """
    if OUTPUT_LANGUAGE == 'ja':
        return generate_execution_plan_markdown_report_ja(plan_info)
    else:
        return generate_execution_plan_markdown_report_en(plan_info)

def generate_execution_plan_markdown_report_ja(plan_info: Dict[str, Any]) -> str:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æçµæœã®Markdownãƒ¬ãƒãƒ¼ãƒˆï¼ˆæ—¥æœ¬èªç‰ˆï¼‰
    """
    from datetime import datetime
    
    lines = []
    lines.append("# Databricks SQLå®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    lines.append("")
    lines.append(f"**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
    lines.append("")
    
    # ãƒ—ãƒ©ãƒ³ã‚µãƒãƒªãƒ¼
    plan_summary = plan_info.get("plan_summary", {})
    lines.append("## ğŸ“Š å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‚µãƒãƒªãƒ¼")
    lines.append("")
    lines.append(f"- **ç·ãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('total_nodes', 0)}")
    lines.append(f"- **BROADCASTãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('broadcast_nodes_count', 0)}")
    lines.append(f"- **JOINãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('join_nodes_count', 0)}")
    lines.append(f"- **ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('scan_nodes_count', 0)}")
    lines.append(f"- **ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('shuffle_nodes_count', 0)}")
    lines.append(f"- **é›†ç´„ãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('aggregate_nodes_count', 0)}")
    lines.append(f"- **BROADCASTãŒä½¿ç”¨ä¸­**: {'ã¯ã„' if plan_summary.get('has_broadcast_joins', False) else 'ã„ã„ãˆ'}")
    lines.append(f"- **ã‚¹ã‚­ãƒ£ãƒ³ã•ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«æ•°**: {plan_summary.get('tables_scanned', 0)}")
    lines.append("")
    
    # JOINæˆ¦ç•¥åˆ†æ
    unique_join_strategies = plan_summary.get('unique_join_strategies', [])
    if unique_join_strategies:
        lines.append("## ğŸ”— JOINæˆ¦ç•¥åˆ†æ")
        lines.append("")
        for strategy in unique_join_strategies:
            strategy_jp = {
                'broadcast_hash_join': 'ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆãƒãƒƒã‚·ãƒ¥JOIN',
                'sort_merge_join': 'ã‚½ãƒ¼ãƒˆãƒãƒ¼ã‚¸JOIN',
                'shuffle_hash_join': 'ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒƒã‚·ãƒ¥JOIN',
                'broadcast_nested_loop_join': 'ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆãƒã‚¹ãƒˆãƒ«ãƒ¼ãƒ—JOIN'
            }.get(strategy, strategy)
            lines.append(f"- **{strategy_jp}** (`{strategy}`)")
        lines.append("")
    
    # BROADCASTãƒãƒ¼ãƒ‰è©³ç´°
    broadcast_nodes = plan_info.get("broadcast_nodes", [])
    if broadcast_nodes:
        lines.append("## ğŸ“¡ BROADCASTãƒãƒ¼ãƒ‰è©³ç´°")
        lines.append("")
        for i, node in enumerate(broadcast_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **ãƒãƒ¼ãƒ‰ID**: {node['node_id']}")
            lines.append(f"- **ãƒãƒ¼ãƒ‰ã‚¿ã‚°**: {node['node_tag']}")
            
            metadata = node.get('metadata', [])
            if metadata:
                lines.append("- **é–¢é€£ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿**:")
                for meta in metadata[:5]:  # æœ€å¤§5å€‹ã¾ã§è¡¨ç¤º
                    key = meta.get('key', '')
                    value = meta.get('value', '')
                    values = meta.get('values', [])
                    if values:
                        lines.append(f"  - **{key}**: {', '.join(map(str, values[:3]))}")
                    elif value:
                        lines.append(f"  - **{key}**: {value}")
            lines.append("")
    
    # JOINãƒãƒ¼ãƒ‰è©³ç´°
    join_nodes = plan_info.get("join_nodes", [])
    if join_nodes:
        lines.append("## ğŸ”— JOINãƒãƒ¼ãƒ‰è©³ç´°")
        lines.append("")
        for i, node in enumerate(join_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **ãƒãƒ¼ãƒ‰ID**: {node['node_id']}")
            lines.append(f"- **JOINæˆ¦ç•¥**: {node['join_strategy']}")
            lines.append(f"- **JOINã‚¿ã‚¤ãƒ—**: {node['join_type']}")
            
            join_keys = node.get('join_keys', [])
            if join_keys:
                lines.append(f"- **JOINã‚­ãƒ¼**: {', '.join(join_keys[:5])}")
            lines.append("")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³è©³ç´°ï¼ˆã‚µã‚¤ã‚ºæ¨å®šæƒ…å ±ã‚’å«ã‚€ï¼‰
    table_scan_details = plan_info.get("table_scan_details", {})
    table_size_estimates = plan_info.get("table_size_estimates", {})
    if table_scan_details:
        lines.append("## ğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³è©³ç´°")
        lines.append("")
        for table_name, scan_detail in table_scan_details.items():
            lines.append(f"### {table_name}")
            lines.append("")
            lines.append(f"- **ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼**: {scan_detail.get('file_format', 'unknown')}")
            lines.append(f"- **ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿æ•°**: {len(scan_detail.get('pushed_filters', []))}")
            lines.append(f"- **å‡ºåŠ›ã‚«ãƒ©ãƒ æ•°**: {len(scan_detail.get('output_columns', []))}")
            
            # å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ã®ã‚µã‚¤ã‚ºæ¨å®šæƒ…å ±ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
            # size_info = table_size_estimates.get(table_name)
            # if size_info:
            #     lines.append(f"- **æ¨å®šã‚µã‚¤ã‚ºï¼ˆå®Ÿè¡Œãƒ—ãƒ©ãƒ³ï¼‰**: {size_info['estimated_size_mb']:.1f}MB")
            #     lines.append(f"- **ã‚µã‚¤ã‚ºæ¨å®šä¿¡é ¼åº¦**: {size_info.get('confidence', 'medium')}")
            #     if 'num_files' in size_info:
            #         lines.append(f"- **ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {size_info['num_files']}")
            #     if 'num_partitions' in size_info:
            #         lines.append(f"- **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°**: {size_info['num_partitions']}")
            
            pushed_filters = scan_detail.get('pushed_filters', [])
            if pushed_filters:
                lines.append("- **ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿**:")
                for filter_expr in pushed_filters[:3]:  # æœ€å¤§3å€‹ã¾ã§è¡¨ç¤º
                    lines.append(f"  - `{filter_expr}`")
            lines.append("")
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰è©³ç´°
    shuffle_nodes = plan_info.get("shuffle_nodes", [])
    if shuffle_nodes:
        lines.append("## ğŸ”„ ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰è©³ç´°")
        lines.append("")
        for i, node in enumerate(shuffle_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **ãƒãƒ¼ãƒ‰ID**: {node['node_id']}")
            
            partition_keys = node.get('partition_keys', [])
            if partition_keys:
                lines.append(f"- **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚­ãƒ¼**: {', '.join(partition_keys)}")
            lines.append("")
    
    # é›†ç´„ãƒãƒ¼ãƒ‰è©³ç´°
    aggregate_nodes = plan_info.get("aggregate_nodes", [])
    if aggregate_nodes:
        lines.append("## ğŸ“Š é›†ç´„ãƒãƒ¼ãƒ‰è©³ç´°")
        lines.append("")
        for i, node in enumerate(aggregate_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **ãƒãƒ¼ãƒ‰ID**: {node['node_id']}")
            
            group_keys = node.get('group_keys', [])
            if group_keys:
                lines.append(f"- **ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã‚­ãƒ¼**: {', '.join(group_keys[:5])}")
            
            agg_expressions = node.get('aggregate_expressions', [])
            if agg_expressions:
                lines.append(f"- **é›†ç´„é–¢æ•°**: {', '.join(agg_expressions[:5])}")
            lines.append("")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®šæƒ…å ±ã‚µãƒãƒªãƒ¼ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     lines.append("## ğŸ“ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®šæƒ…å ±ï¼ˆå®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒ™ãƒ¼ã‚¹ï¼‰")
    #     lines.append("")
    #     total_estimated_size = sum(size_info['estimated_size_mb'] for size_info in table_size_estimates.values())
    #     lines.append(f"- **æ¨å®šå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°**: {len(table_size_estimates)}")
    #     lines.append(f"- **ç·æ¨å®šã‚µã‚¤ã‚º**: {total_estimated_size:.1f}MB")
    #     lines.append("")
    #     
    #     for table_name, size_info in list(table_size_estimates.items())[:5]:  # æœ€å¤§5ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
    #         lines.append(f"### {table_name}")
    #         lines.append(f"- **æ¨å®šã‚µã‚¤ã‚º**: {size_info['estimated_size_mb']:.1f}MB")
    #         lines.append(f"- **ä¿¡é ¼åº¦**: {size_info.get('confidence', 'medium')}")
    #         lines.append(f"- **ãƒãƒ¼ãƒ‰**: {size_info.get('node_name', 'unknown')}")
    #         if 'num_files' in size_info:
    #             lines.append(f"- **ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {size_info['num_files']}")
    #         lines.append("")
    #     
    #     if len(table_size_estimates) > 5:
    #         lines.append(f"...ä»– {len(table_size_estimates) - 5} ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆè©³ç´°ã¯ä¸Šè¨˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³å‚ç…§ï¼‰")
    #         lines.append("")
    
    # æœ€é©åŒ–æ¨å¥¨äº‹é …
    lines.append("## ğŸ’¡ ãƒ—ãƒ©ãƒ³ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–æ¨å¥¨äº‹é …")
    lines.append("")
    
    if plan_summary.get('has_broadcast_joins', False):
        lines.append("âœ… **æ—¢ã«BROADCAST JOINãŒé©ç”¨ã•ã‚Œã¦ã„ã¾ã™**")
        lines.append("- ç¾åœ¨ã®å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã§BROADCASTæœ€é©åŒ–ãŒæœ‰åŠ¹")
        
        # BROADCASTã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
        broadcast_tables = plan_summary.get('broadcast_tables', [])
        if broadcast_tables:
            lines.append(f"- **BROADCASTã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«**: {', '.join(broadcast_tables)}")
        
        lines.append("- è¿½åŠ ã®BROADCASTé©ç”¨æ©Ÿä¼šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    else:
        lines.append("âš ï¸ **BROADCAST JOINãŒæœªé©ç”¨ã§ã™**")
        lines.append("- å°ãƒ†ãƒ¼ãƒ–ãƒ«ã«BROADCASTãƒ’ãƒ³ãƒˆã®é©ç”¨ã‚’æ¤œè¨")
        lines.append("- 30MBé–¾å€¤ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®šã—ã¦ãã ã•ã„")
    lines.append("")
    
    if plan_summary.get('shuffle_nodes_count', 0) > 3:
        lines.append("âš ï¸ **å¤šæ•°ã®ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ**")
        lines.append("- ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ•£ã¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã‚’è¦‹ç›´ã—")
        lines.append("- Liquid Clusteringã®é©ç”¨ã‚’æ¤œè¨")
    lines.append("")
    
    # ã‚µã‚¤ã‚ºæ¨å®šãƒ™ãƒ¼ã‚¹ã®æœ€é©åŒ–ææ¡ˆï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     small_tables = [name for name, info in table_size_estimates.items() if info['estimated_size_mb'] <= 30]
    #     if small_tables:
    #         lines.append("ğŸ’¡ **å®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒ™ãƒ¼ã‚¹BROADCASTæ¨å¥¨**")
    #         lines.append(f"- 30MBä»¥ä¸‹ã®å°ãƒ†ãƒ¼ãƒ–ãƒ«: {len(small_tables)}å€‹æ¤œå‡º")
    #         for table in small_tables[:3]:  # æœ€å¤§3å€‹è¡¨ç¤º
    #             size_mb = table_size_estimates[table]['estimated_size_mb']
    #             lines.append(f"  â€¢ {table}: {size_mb:.1f}MBï¼ˆBROADCASTå€™è£œï¼‰")
    #         if len(small_tables) > 3:
    #             lines.append(f"  â€¢ ...ä»– {len(small_tables) - 3} ãƒ†ãƒ¼ãƒ–ãƒ«")
    #         lines.append("")
    
    lines.append("---")
    lines.append("")
    lines.append("ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ã€Databricks SQLå®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æãƒ„ãƒ¼ãƒ«ã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")
    
    return '\n'.join(lines)

def generate_execution_plan_markdown_report_en(plan_info: Dict[str, Any]) -> str:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æçµæœã®Markdownãƒ¬ãƒãƒ¼ãƒˆï¼ˆè‹±èªç‰ˆï¼‰
    """
    from datetime import datetime
    
    lines = []
    lines.append("# Databricks SQL Execution Plan Analysis Report")
    lines.append("")
    lines.append(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("")
    
    # Plan Summary
    plan_summary = plan_info.get("plan_summary", {})
    lines.append("## ğŸ“Š Execution Plan Summary")
    lines.append("")
    lines.append(f"- **Total Nodes**: {plan_summary.get('total_nodes', 0)}")
    lines.append(f"- **BROADCAST Nodes**: {plan_summary.get('broadcast_nodes_count', 0)}")
    lines.append(f"- **JOIN Nodes**: {plan_summary.get('join_nodes_count', 0)}")
    lines.append(f"- **Scan Nodes**: {plan_summary.get('scan_nodes_count', 0)}")
    lines.append(f"- **Shuffle Nodes**: {plan_summary.get('shuffle_nodes_count', 0)}")
    lines.append(f"- **Aggregate Nodes**: {plan_summary.get('aggregate_nodes_count', 0)}")
    lines.append(f"- **BROADCAST in Use**: {'Yes' if plan_summary.get('has_broadcast_joins', False) else 'No'}")
    lines.append(f"- **Tables Scanned**: {plan_summary.get('tables_scanned', 0)}")
    lines.append("")
    
    # JOIN Strategy Analysis
    unique_join_strategies = plan_summary.get('unique_join_strategies', [])
    if unique_join_strategies:
        lines.append("## ğŸ”— JOIN Strategy Analysis")
        lines.append("")
        for strategy in unique_join_strategies:
            lines.append(f"- **{strategy.replace('_', ' ').title()}** (`{strategy}`)")
        lines.append("")
    
    # BROADCAST Node Details
    broadcast_nodes = plan_info.get("broadcast_nodes", [])
    if broadcast_nodes:
        lines.append("## ğŸ“¡ BROADCAST Node Details")
        lines.append("")
        for i, node in enumerate(broadcast_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            lines.append(f"- **Node Tag**: {node['node_tag']}")
            
            metadata = node.get('metadata', [])
            if metadata:
                lines.append("- **Related Metadata**:")
                for meta in metadata[:5]:  # Show up to 5
                    key = meta.get('key', '')
                    value = meta.get('value', '')
                    values = meta.get('values', [])
                    if values:
                        lines.append(f"  - **{key}**: {', '.join(map(str, values[:3]))}")
                    elif value:
                        lines.append(f"  - **{key}**: {value}")
            lines.append("")
    
    # JOIN Node Details
    join_nodes = plan_info.get("join_nodes", [])
    if join_nodes:
        lines.append("## ğŸ”— JOIN Node Details")
        lines.append("")
        for i, node in enumerate(join_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            lines.append(f"- **JOIN Strategy**: {node['join_strategy']}")
            lines.append(f"- **JOIN Type**: {node['join_type']}")
            
            join_keys = node.get('join_keys', [])
            if join_keys:
                lines.append(f"- **JOIN Keys**: {', '.join(join_keys[:5])}")
            lines.append("")
    
    # Table Scan Details (with size estimation info)
    table_scan_details = plan_info.get("table_scan_details", {})
    table_size_estimates = plan_info.get("table_size_estimates", {})
    if table_scan_details:
        lines.append("## ğŸ“‹ Table Scan Details")
        lines.append("")
        for table_name, scan_detail in table_scan_details.items():
            lines.append(f"### {table_name}")
            lines.append("")
            lines.append(f"- **File Format**: {scan_detail.get('file_format', 'unknown')}")
            lines.append(f"- **Pushed Filters**: {len(scan_detail.get('pushed_filters', []))}")
            lines.append(f"- **Output Columns**: {len(scan_detail.get('output_columns', []))}")
            
            # Add execution plan size estimation info (disabled - estimatedSizeInBytes not available)
            # size_info = table_size_estimates.get(table_name)
            # if size_info:
            #     lines.append(f"- **Estimated Size (Execution Plan)**: {size_info['estimated_size_mb']:.1f}MB")
            #     lines.append(f"- **Size Estimation Confidence**: {size_info.get('confidence', 'medium')}")
            #     if 'num_files' in size_info:
            #         lines.append(f"- **Number of Files**: {size_info['num_files']}")
            #     if 'num_partitions' in size_info:
            #         lines.append(f"- **Number of Partitions**: {size_info['num_partitions']}")
            
            pushed_filters = scan_detail.get('pushed_filters', [])
            if pushed_filters:
                lines.append("- **Pushed Down Filters**:")
                for filter_expr in pushed_filters[:3]:  # Show up to 3
                    lines.append(f"  - `{filter_expr}`")
            lines.append("")
    
    # Shuffle Node Details
    shuffle_nodes = plan_info.get("shuffle_nodes", [])
    if shuffle_nodes:
        lines.append("## ğŸ”„ Shuffle Node Details")
        lines.append("")
        for i, node in enumerate(shuffle_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            
            partition_keys = node.get('partition_keys', [])
            if partition_keys:
                lines.append(f"- **Partition Keys**: {', '.join(partition_keys)}")
            lines.append("")
    
    # Aggregate Node Details
    aggregate_nodes = plan_info.get("aggregate_nodes", [])
    if aggregate_nodes:
        lines.append("## ğŸ“Š Aggregate Node Details")
        lines.append("")
        for i, node in enumerate(aggregate_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            
            group_keys = node.get('group_keys', [])
            if group_keys:
                lines.append(f"- **Group Keys**: {', '.join(group_keys[:5])}")
            
            agg_expressions = node.get('aggregate_expressions', [])
            if agg_expressions:
                lines.append(f"- **Aggregate Functions**: {', '.join(agg_expressions[:5])}")
            lines.append("")
    
    # Table Size Estimation Summary (disabled - estimatedSizeInBytes not available)
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     lines.append("## ğŸ“ Table Size Estimation (Execution Plan Based)")
    #     lines.append("")
    #     total_estimated_size = sum(size_info['estimated_size_mb'] for size_info in table_size_estimates.values())
    #     lines.append(f"- **Estimated Tables Count**: {len(table_size_estimates)}")
    #     lines.append(f"- **Total Estimated Size**: {total_estimated_size:.1f}MB")
    #     lines.append("")
    #     
    #     for table_name, size_info in list(table_size_estimates.items())[:5]:  # Show up to 5 tables
    #         lines.append(f"### {table_name}")
    #         lines.append(f"- **Estimated Size**: {size_info['estimated_size_mb']:.1f}MB")
    #         lines.append(f"- **Confidence**: {size_info.get('confidence', 'medium')}")
    #         lines.append(f"- **Node**: {size_info.get('node_name', 'unknown')}")
    #         if 'num_files' in size_info:
    #             lines.append(f"- **Number of Files**: {size_info['num_files']}")
    #         lines.append("")
    #     
    #     if len(table_size_estimates) > 5:
    #         lines.append(f"...and {len(table_size_estimates) - 5} more tables (see details in sections above)")
    #         lines.append("")
    
    # Plan-based Optimization Recommendations
    lines.append("## ğŸ’¡ Plan-based Optimization Recommendations")
    lines.append("")
    
    if plan_summary.get('has_broadcast_joins', False):
        lines.append("âœ… **BROADCAST JOIN is already applied**")
        lines.append("- Current execution plan has BROADCAST optimization enabled")
        
        # Show list of broadcast tables
        broadcast_tables = plan_summary.get('broadcast_tables', [])
        if broadcast_tables:
            lines.append(f"- **Tables Being Broadcast**: {', '.join(broadcast_tables)}")
        
        lines.append("- Check for additional BROADCAST application opportunities")
    else:
        lines.append("âš ï¸ **BROADCAST JOIN is not applied**")
        lines.append("- Consider applying BROADCAST hints to small tables")
        lines.append("- Identify tables under 30MB threshold")
    lines.append("")
    
    if plan_summary.get('shuffle_nodes_count', 0) > 3:
        lines.append("âš ï¸ **Multiple shuffle operations detected**")
        lines.append("- Review data distribution and partitioning strategy")
        lines.append("- Consider applying Liquid Clustering")
    lines.append("")
    
    # Size estimation based optimization suggestions (disabled - estimatedSizeInBytes not available)
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     small_tables = [name for name, info in table_size_estimates.items() if info['estimated_size_mb'] <= 30]
    #     if small_tables:
    #         lines.append("ğŸ’¡ **Execution Plan Based BROADCAST Recommendations**")
    #         lines.append(f"- Small tables â‰¤30MB detected: {len(small_tables)}")
    #         for table in small_tables[:3]:  # Show up to 3 tables
    #             size_mb = table_size_estimates[table]['estimated_size_mb']
    #             lines.append(f"  â€¢ {table}: {size_mb:.1f}MB (BROADCAST candidate)")
    #         if len(small_tables) > 3:
    #             lines.append(f"  â€¢ ...and {len(small_tables) - 3} more tables")
    #         lines.append("")
    
    lines.append("---")
    lines.append("")
    lines.append("This report was automatically generated by the Databricks SQL Execution Plan Analysis Tool.")
    
    return '\n'.join(lines)

def generate_comprehensive_optimization_report(query_id: str, optimized_result: str, metrics: Dict[str, Any], analysis_result: str = "") -> str:
    """
    åŒ…æ‹¬çš„ãªæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    
    Args:
        query_id: ã‚¯ã‚¨ãƒªID
        optimized_result: æœ€é©åŒ–çµæœ
        metrics: ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±
        analysis_result: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ
    
    Returns:
        str: èª­ã¿ã‚„ã™ãæ§‹æˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ
    """
    from datetime import datetime
    
    # åŸºæœ¬æƒ…å ±ã®å–å¾—
    overall_metrics = metrics.get('overall_metrics', {})
    bottleneck_indicators = metrics.get('bottleneck_indicators', {})
    liquid_analysis = metrics.get('liquid_clustering_analysis', {})
    
    # thinking_enabledå¯¾å¿œ: analysis_resultãŒãƒªã‚¹ãƒˆã®å ´åˆã®å‡¦ç†
    if isinstance(analysis_result, list):
        analysis_result_str = format_thinking_response(analysis_result)
    else:
        analysis_result_str = str(analysis_result)
    
    # signatureæƒ…å ±ã®é™¤å»
    import re
    signature_pattern = r"'signature':\s*'[A-Za-z0-9+/=]{100,}'"
    analysis_result_str = re.sub(signature_pattern, "'signature': '[REMOVED]'", analysis_result_str)
    
    # ãƒ¬ãƒãƒ¼ãƒˆã®æ§‹æˆ
    if OUTPUT_LANGUAGE == 'ja':
        report = f"""# ğŸ“Š SQLæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ

**ã‚¯ã‚¨ãƒªID**: {query_id}  
**ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ¯ 1. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ

### ğŸ¤– AIã«ã‚ˆã‚‹è©³ç´°åˆ†æ

{analysis_result_str}

### ğŸ“Š ä¸»è¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™

| æŒ‡æ¨™ | å€¤ | è©•ä¾¡ |
|------|-----|------|
| å®Ÿè¡Œæ™‚é–“ | {overall_metrics.get('total_time_ms', 0):,} ms | {'âœ… è‰¯å¥½' if overall_metrics.get('total_time_ms', 0) < 60000 else 'âš ï¸ æ”¹å–„å¿…è¦'} |
| Photonæœ‰åŠ¹ | {'ã¯ã„' if overall_metrics.get('photon_enabled', False) else 'ã„ã„ãˆ'} | {'âœ… è‰¯å¥½' if overall_metrics.get('photon_enabled', False) else 'âŒ æœªæœ‰åŠ¹'} |
| ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ | {bottleneck_indicators.get('cache_hit_ratio', 0) * 100:.1f}% | {'âœ… è‰¯å¥½' if bottleneck_indicators.get('cache_hit_ratio', 0) > 0.8 else 'âš ï¸ æ”¹å–„å¿…è¦'} |
| ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§ | {bottleneck_indicators.get('data_selectivity', 0) * 100:.2f}% | {'âœ… è‰¯å¥½' if bottleneck_indicators.get('data_selectivity', 0) > 0.1 else 'âš ï¸ æ”¹å–„å¿…è¦'} |
| ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ | {bottleneck_indicators.get('shuffle_operations_count', 0)}å› | {'âœ… è‰¯å¥½' if bottleneck_indicators.get('shuffle_operations_count', 0) < 5 else 'âš ï¸ å¤šæ•°'} |
| ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ | {'ã¯ã„' if bottleneck_indicators.get('has_spill', False) else 'ã„ã„ãˆ'} | {'âŒ å•é¡Œã‚ã‚Š' if bottleneck_indicators.get('has_spill', False) else 'âœ… è‰¯å¥½'} |

### ğŸš¨ ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯

"""
        
        # ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®è©³ç´°
        bottlenecks = []
        
        if bottleneck_indicators.get('has_spill', False):
            spill_gb = bottleneck_indicators.get('spill_bytes', 0) / 1024 / 1024 / 1024
            bottlenecks.append(f"**ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«**: {spill_gb:.2f}GB - ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ã‚ˆã‚‹æ€§èƒ½ä½ä¸‹")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            bottlenecks.append("**ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: JOIN/GROUP BYå‡¦ç†ã§ã®å¤§é‡ãƒ‡ãƒ¼ã‚¿è»¢é€")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            bottlenecks.append("**ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ä½ä¸‹**: ãƒ‡ãƒ¼ã‚¿å†åˆ©ç”¨åŠ¹ç‡ãŒä½ã„")
        
        if not overall_metrics.get('photon_enabled', False):
            bottlenecks.append("**Photonæœªæœ‰åŠ¹**: é«˜é€Ÿå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã•ã‚Œã¦ã„ãªã„")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.01:
            bottlenecks.append("**ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§ä½ä¸‹**: å¿…è¦ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã‚‹")
        
        if bottlenecks:
            for i, bottleneck in enumerate(bottlenecks, 1):
                report += f"{i}. {bottleneck}\n"
        else:
            report += "ä¸»è¦ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n"
        
        report += "\n"
        
        # Liquid Clusteringåˆ†æçµæœã®è¿½åŠ 
        if liquid_analysis:
            performance_context = liquid_analysis.get('performance_context', {})
            llm_analysis = liquid_analysis.get('llm_analysis', '')
            
            report += f"""
## ğŸ—‚ï¸ 2. Liquid Clusteringåˆ†æçµæœ

### ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦

| é …ç›® | å€¤ |
|------|-----|
| å®Ÿè¡Œæ™‚é–“ | {performance_context.get('total_time_sec', 0):.1f}ç§’ |
| ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ | {performance_context.get('read_gb', 0):.2f}GB |
| å‡ºåŠ›è¡Œæ•° | {performance_context.get('rows_produced', 0):,}è¡Œ |
| èª­ã¿è¾¼ã¿è¡Œæ•° | {performance_context.get('rows_read', 0):,}è¡Œ |
| ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§ | {performance_context.get('data_selectivity', 0):.4f} |

### ğŸ¤– AIåˆ†æçµæœ

{llm_analysis}

"""
        
        # SQLæœ€é©åŒ–åˆ†æçµæœã®è¿½åŠ 
        report += f"""
## ğŸš€ 3. SQLæœ€é©åŒ–åˆ†æçµæœ

### ğŸ’¡ æœ€é©åŒ–ææ¡ˆ

{optimized_result}

### ğŸ“ˆ 4. æœŸå¾…ã•ã‚Œã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„åŠ¹æœ

#### ğŸ¯ äºˆæƒ³ã•ã‚Œã‚‹æ”¹å–„ç‚¹

"""
        
        # æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æœã‚’è¨ˆç®—
        expected_improvements = []
        
        if bottleneck_indicators.get('has_spill', False):
            expected_improvements.append("**ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«è§£æ¶ˆ**: æœ€å¤§50-80%ã®æ€§èƒ½æ”¹å–„ãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            expected_improvements.append("**ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ€é©åŒ–**: 20-60%ã®å®Ÿè¡Œæ™‚é–“çŸ­ç¸®ãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            expected_improvements.append("**ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡å‘ä¸Š**: 30-70%ã®èª­ã¿è¾¼ã¿æ™‚é–“çŸ­ç¸®ãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if not overall_metrics.get('photon_enabled', False):
            expected_improvements.append("**Photonæœ‰åŠ¹åŒ–**: 2-10å€ã®å‡¦ç†é€Ÿåº¦å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.01:
            expected_improvements.append("**ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§æ”¹å–„**: 40-90%ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é‡å‰Šæ¸›ãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if expected_improvements:
            for i, improvement in enumerate(expected_improvements, 1):
                report += f"{i}. {improvement}\n"
            
            # ç·åˆçš„ãªæ”¹å–„åŠ¹æœ
            total_time_ms = overall_metrics.get('total_time_ms', 0)
            if total_time_ms > 0:
                improvement_ratio = min(0.8, len(expected_improvements) * 0.15)  # æœ€å¤§80%æ”¹å–„
                expected_time = total_time_ms * (1 - improvement_ratio)
                report += f"\n**ç·åˆæ”¹å–„åŠ¹æœ**: å®Ÿè¡Œæ™‚é–“ {total_time_ms:,}ms â†’ {expected_time:,.0f}msï¼ˆç´„{improvement_ratio*100:.0f}%æ”¹å–„ï¼‰\n"
        else:
            report += "ç¾åœ¨ã®ã‚¯ã‚¨ãƒªã¯æ—¢ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚å¤§å¹…ãªæ”¹å–„ã¯æœŸå¾…ã•ã‚Œã¾ã›ã‚“ã€‚\n"
        
        report += f"""

#### ğŸ”§ å®Ÿè£…å„ªå…ˆåº¦

1. **é«˜å„ªå…ˆåº¦**: Photonæœ‰åŠ¹åŒ–ã€ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«è§£æ¶ˆ
2. **ä¸­å„ªå…ˆåº¦**: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–ã€ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æˆ¦ç•¥
3. **ä½å„ªå…ˆåº¦**: çµ±è¨ˆæƒ…å ±æ›´æ–°ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥

---

*ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
    else:
        # è‹±èªç‰ˆï¼ˆåŒæ§˜ã®æ§‹æˆï¼‰
        report = f"""# ğŸ“Š SQL Optimization Report

**Query ID**: {query_id}  
**Report Generation Time**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ¯ 1. Bottleneck Analysis Results

### ğŸ¤– AI-Powered Analysis

{analysis_result_str}

### ğŸ“Š Key Performance Indicators

| Metric | Value | Status |
|--------|-------|--------|
| Execution Time | {overall_metrics.get('total_time_ms', 0):,} ms | {'âœ… Good' if overall_metrics.get('total_time_ms', 0) < 60000 else 'âš ï¸ Needs Improvement'} |
| Photon Enabled | {'Yes' if overall_metrics.get('photon_enabled', False) else 'No'} | {'âœ… Good' if overall_metrics.get('photon_enabled', False) else 'âŒ Not Enabled'} |
| Cache Efficiency | {bottleneck_indicators.get('cache_hit_ratio', 0) * 100:.1f}% | {'âœ… Good' if bottleneck_indicators.get('cache_hit_ratio', 0) > 0.8 else 'âš ï¸ Needs Improvement'} |
| Data Selectivity | {bottleneck_indicators.get('data_selectivity', 0) * 100:.2f}% | {'âœ… Good' if bottleneck_indicators.get('data_selectivity', 0) > 0.1 else 'âš ï¸ Needs Improvement'} |
| Shuffle Operations | {bottleneck_indicators.get('shuffle_operations_count', 0)} times | {'âœ… Good' if bottleneck_indicators.get('shuffle_operations_count', 0) < 5 else 'âš ï¸ High'} |
| Spill Occurrence | {'Yes' if bottleneck_indicators.get('has_spill', False) else 'No'} | {'âŒ Issues' if bottleneck_indicators.get('has_spill', False) else 'âœ… Good'} |

### ğŸš¨ Key Bottlenecks

"""
        
        # ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®è©³ç´°ï¼ˆè‹±èªç‰ˆï¼‰
        bottlenecks = []
        
        if bottleneck_indicators.get('has_spill', False):
            spill_gb = bottleneck_indicators.get('spill_bytes', 0) / 1024 / 1024 / 1024
            bottlenecks.append(f"**Memory Spill**: {spill_gb:.2f}GB - Performance degradation due to memory shortage")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            bottlenecks.append("**Shuffle Bottleneck**: Large data transfer in JOIN/GROUP BY operations")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            bottlenecks.append("**Cache Inefficiency**: Low data reuse efficiency")
        
        if not overall_metrics.get('photon_enabled', False):
            bottlenecks.append("**Photon Not Enabled**: High-speed processing engine not utilized")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.01:
            bottlenecks.append("**Poor Data Selectivity**: Reading more data than necessary")
        
        if bottlenecks:
            for i, bottleneck in enumerate(bottlenecks, 1):
                report += f"{i}. {bottleneck}\n"
        else:
            report += "No major bottlenecks detected.\n"
        
        report += "\n"
        
        # Liquid Clusteringåˆ†æçµæœã®è¿½åŠ ï¼ˆè‹±èªç‰ˆï¼‰
        if liquid_analysis:
            performance_context = liquid_analysis.get('performance_context', {})
            llm_analysis = liquid_analysis.get('llm_analysis', '')
            
            report += f"""
## ğŸ—‚ï¸ 2. Liquid Clustering Analysis Results

### ğŸ“Š Performance Overview

| Item | Value |
|------|-------|
| Execution Time | {performance_context.get('total_time_sec', 0):.1f}s |
| Data Read | {performance_context.get('read_gb', 0):.2f}GB |
| Output Rows | {performance_context.get('rows_produced', 0):,} |
| Read Rows | {performance_context.get('rows_read', 0):,} |
| Data Selectivity | {performance_context.get('data_selectivity', 0):.4f} |

### ğŸ¤– AI Analysis Results

{llm_analysis}

"""
        
        # SQLæœ€é©åŒ–åˆ†æçµæœã®è¿½åŠ ï¼ˆè‹±èªç‰ˆï¼‰
        report += f"""
## ğŸš€ 3. SQL Optimization Analysis Results

### ğŸ’¡ Optimization Recommendations

{optimized_result}

### ğŸ“ˆ 4. Expected Performance Improvement

#### ğŸ¯ Anticipated Improvements

"""
        
        # æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æœã‚’è¨ˆç®—ï¼ˆè‹±èªç‰ˆï¼‰
        expected_improvements = []
        
        if bottleneck_indicators.get('has_spill', False):
            expected_improvements.append("**Memory Spill Resolution**: Up to 50-80% performance improvement expected")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            expected_improvements.append("**Shuffle Optimization**: 20-60% execution time reduction expected")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            expected_improvements.append("**Cache Efficiency**: 30-70% read time reduction expected")
        
        if not overall_metrics.get('photon_enabled', False):
            expected_improvements.append("**Photon Enablement**: 2-10x processing speed improvement expected")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.01:
            expected_improvements.append("**Data Selectivity**: 40-90% data read volume reduction expected")
        
        if expected_improvements:
            for i, improvement in enumerate(expected_improvements, 1):
                report += f"{i}. {improvement}\n"
            
            # ç·åˆçš„ãªæ”¹å–„åŠ¹æœ
            total_time_ms = overall_metrics.get('total_time_ms', 0)
            if total_time_ms > 0:
                improvement_ratio = min(0.8, len(expected_improvements) * 0.15)  # æœ€å¤§80%æ”¹å–„
                expected_time = total_time_ms * (1 - improvement_ratio)
                report += f"\n**Overall Improvement**: Execution time {total_time_ms:,}ms â†’ {expected_time:,.0f}ms (approx. {improvement_ratio*100:.0f}% improvement)\n"
        else:
            report += "Current query is already optimized. No significant improvements expected.\n"
        
        report += f"""

#### ğŸ”§ Implementation Priority

1. **High Priority**: Photon enablement, Memory spill resolution
2. **Medium Priority**: Index optimization, Partitioning strategy
3. **Low Priority**: Statistics update, Cache strategy

---

*Report generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    return report

def refine_report_with_llm(raw_report: str, query_id: str) -> str:
    """
    LLMã‚’ä½¿ã£ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’æ¨æ•²ã—ã€èª­ã¿ã‚„ã™ã„æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    
    Args:
        raw_report: åˆæœŸç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ
        query_id: ã‚¯ã‚¨ãƒªID
        
    Returns:
        str: LLMã§æ¨æ•²ã•ã‚ŒãŸèª­ã¿ã‚„ã™ã„ãƒ¬ãƒãƒ¼ãƒˆ
    """
    
    refinement_prompt = f"""
ã‚ãªãŸã¯æŠ€è¡“æ–‡æ›¸ã®ç·¨é›†è€…ã¨ã—ã¦ã€Databricks SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿ã‚„ã™ãæ¨æ•²ã—ã¦ãã ã•ã„ã€‚

ã€æ¨æ•²ã®æ–¹é‡ã€‘
1. **é‡è¤‡å†…å®¹ã®çµ±åˆ**: åŒã˜æƒ…å ±ãŒè¤‡æ•°ç®‡æ‰€ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯çµ±åˆã—ã€ä¸€ç®‡æ‰€ã«ã¾ã¨ã‚ã‚‹
2. **æ§‹æˆã®æœ€é©åŒ–**: æƒ…å ±ã®å„ªå…ˆåº¦ã«åŸºã¥ã„ã¦è«–ç†çš„ãªé †åºã§å†é…ç½®
3. **èª­ã¿ã‚„ã™ã•ã®å‘ä¸Š**: æŠ€è¡“çš„å†…å®¹ã‚’ä¿æŒã—ã¤ã¤ã€èª­ã¿ã‚„ã™ã„æ–‡ç« ã«æ”¹å–„
4. **å…·ä½“æ€§ã®è¿½åŠ **: å¯èƒ½ãªå ´åˆã¯å…·ä½“çš„ãªæ•°å€¤ã‚„æ”¹å–„ææ¡ˆã‚’å¼·èª¿
5. **ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã®æ˜ç¢ºåŒ–**: å®Ÿè£…ã™ã¹ãå¯¾ç­–ã‚’æ˜ç¢ºã«æç¤º

ã€ç¾åœ¨ã®ãƒ¬ãƒãƒ¼ãƒˆã€‘
```
{raw_report}
```

ã€å‡ºåŠ›è¦ä»¶ã€‘
- Markdownãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç¶­æŒ
- æŠ€è¡“çš„ãªæ­£ç¢ºæ€§ã‚’ä¿æŒ
- ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ã‚’å†’é ­ã«è¿½åŠ 
- é‡è¤‡ã‚’å‰Šé™¤ã—ã€æƒ…å ±ã‚’è«–ç†çš„ã«æ•´ç†
- å®Ÿè£…ã®ãŸã‚ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ã‚’è¿½åŠ 
- èª­ã¿ã‚„ã™ã„æ§‹æˆã§çµ±åˆã•ã‚ŒãŸæœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›

æ¨æ•²å¾Œã®èª­ã¿ã‚„ã™ã„ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
    
    try:
        provider = LLM_CONFIG.get("provider", "databricks")
        
        if provider == "databricks":
            refined_report = _call_databricks_llm(refinement_prompt)
        elif provider == "openai":
            refined_report = _call_openai_llm(refinement_prompt)
        elif provider == "azure_openai":
            refined_report = _call_azure_openai_llm(refinement_prompt)
        elif provider == "anthropic":
            refined_report = _call_anthropic_llm(refinement_prompt)
        else:
            raise ValueError(f"Unsupported LLM provider: {provider}")
        
        # thinking_enabledå¯¾å¿œ
        if isinstance(refined_report, list):
            refined_report = format_thinking_response(refined_report)
        
        # signatureæƒ…å ±ã®é™¤å»
        import re
        signature_pattern = r"'signature':\s*'[A-Za-z0-9+/=]{100,}'"
        refined_report = re.sub(signature_pattern, "'signature': '[REMOVED]'", refined_report)
        
        return refined_report
        
    except Exception as e:
        print(f"âš ï¸ LLMã«ã‚ˆã‚‹ãƒ¬ãƒãƒ¼ãƒˆæ¨æ•²ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        print("ğŸ“„ å…ƒã®ãƒ¬ãƒãƒ¼ãƒˆã‚’è¿”ã—ã¾ã™")
        return raw_report

def save_optimized_sql_files(original_query: str, optimized_result: str, metrics: Dict[str, Any], analysis_result: str = "") -> Dict[str, str]:
    """
    æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œå¯èƒ½ãªå½¢ã§ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    ç‰¹å¾´:
    - SQLãƒ•ã‚¡ã‚¤ãƒ«ã®æœ«å°¾ã«è‡ªå‹•ã§ã‚»ãƒŸã‚³ãƒ­ãƒ³(;)ã‚’ä»˜ä¸
    - ãã®ã¾ã¾Databricks Notebookã§å®Ÿè¡Œå¯èƒ½
    - %sql ãƒã‚¸ãƒƒã‚¯ã‚³ãƒãƒ³ãƒ‰ã§ã‚‚ç›´æ¥å®Ÿè¡Œå¯èƒ½
    - LLMã«ã‚ˆã‚‹ãƒ¬ãƒãƒ¼ãƒˆæ¨æ•²ã§èª­ã¿ã‚„ã™ã„æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    """
    
    import re
    from datetime import datetime
    
    # thinking_enabled: Trueã®å ´åˆã«optimized_resultãŒãƒªã‚¹ãƒˆã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚å¯¾å¿œ
    optimized_result_for_file = optimized_result
    optimized_result_main_content = optimized_result
    
    if isinstance(optimized_result, list):
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ç”¨ã¯äººé–“ã«èª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›
        optimized_result_for_file = format_thinking_response(optimized_result)
        # SQLæŠ½å‡ºç”¨ã¯ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã¿ã‚’ä½¿ç”¨
        optimized_result_main_content = extract_main_content_from_thinking_response(optimized_result)
    
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    query_id = metrics.get('query_info', {}).get('query_id', 'unknown')
    
    # ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã¯é™¤å¤–ï¼ˆä¸è¦ï¼‰
    original_filename = None
    
    # æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®æŠ½å‡ºã¨ä¿å­˜
    optimized_filename = f"output_optimized_query_{timestamp}.sql"
    
    # æœ€é©åŒ–çµæœã‹ã‚‰SQLã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æŠ½å‡ºï¼‰
    sql_pattern = r'```sql\s*(.*?)\s*```'
    sql_matches = re.findall(sql_pattern, optimized_result_main_content, re.DOTALL | re.IGNORECASE)
    
    optimized_sql = ""
    if sql_matches:
        # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸSQLãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½¿ç”¨
        optimized_sql = sql_matches[0].strip()
    else:
        # SQLãƒ–ãƒ­ãƒƒã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€SQLé–¢é€£ã®è¡Œã‚’æŠ½å‡º
        lines = optimized_result_main_content.split('\n')
        sql_lines = []
        in_sql_section = False
        
        for line in lines:
            if any(keyword in line.upper() for keyword in ['SELECT', 'FROM', 'WHERE', 'WITH', 'CREATE']):
                in_sql_section = True
            
            if in_sql_section:
                if line.strip().startswith('#') or line.strip().startswith('*'):
                    in_sql_section = False
                else:
                    sql_lines.append(line)
        
        optimized_sql = '\n'.join(sql_lines).strip()
    
    # æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
    with open(optimized_filename, 'w', encoding='utf-8') as f:
        f.write(f"-- æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒª\n")
        f.write(f"-- å…ƒã‚¯ã‚¨ãƒªID: {query_id}\n")
        f.write(f"-- æœ€é©åŒ–æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"-- ãƒ•ã‚¡ã‚¤ãƒ«: {optimized_filename}\n\n")
        
        if optimized_sql:
            # SQLã®æœ«å°¾ã«ã‚»ãƒŸã‚³ãƒ­ãƒ³ã‚’ç¢ºå®Ÿã«è¿½åŠ 
            optimized_sql_clean = optimized_sql.strip()
            if optimized_sql_clean and not optimized_sql_clean.endswith(';'):
                optimized_sql_clean += ';'
            f.write(optimized_sql_clean)
        else:
            f.write("-- âš ï¸ SQLã‚³ãƒ¼ãƒ‰ã®è‡ªå‹•æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ\n")
            f.write("-- ä»¥ä¸‹ã¯æœ€é©åŒ–åˆ†æã®å…¨çµæœã§ã™:\n\n")
            f.write(f"/*\n{optimized_result_main_content}\n*/")
    
    # åˆ†æãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ï¼ˆLLMã§æ¨æ•²ã•ã‚ŒãŸèª­ã¿ã‚„ã™ã„ãƒ¬ãƒãƒ¼ãƒˆï¼‰
    report_filename = f"output_optimization_report_{timestamp}.md"
    
    print("ğŸ¤– LLMã«ã‚ˆã‚‹ãƒ¬ãƒãƒ¼ãƒˆæ¨æ•²ã‚’å®Ÿè¡Œä¸­...")
    
    # åˆæœŸãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
    initial_report = generate_comprehensive_optimization_report(
        query_id, optimized_result_for_file, metrics, analysis_result
    )
    
    # LLMã§ãƒ¬ãƒãƒ¼ãƒˆã‚’æ¨æ•²
    refined_report = refine_report_with_llm(initial_report, query_id)
    
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(refined_report)
    
    print("âœ… LLMã«ã‚ˆã‚‹ãƒ¬ãƒãƒ¼ãƒˆæ¨æ•²å®Œäº†")
    
    # æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10ã‚’ç‹¬ç«‹ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›
    top10_filename = f"output_top10_processes_{timestamp}.md"
    with open(top10_filename, 'w', encoding='utf-8') as f:
        f.write(f"# {get_message('top10_processes')}\n\n")
        f.write(f"**{get_message('query_id')}**: {query_id}\n")
        f.write(f"**{get_message('analysis_time')}**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        try:
            top10_report = generate_top10_time_consuming_processes_report(metrics)
            f.write(top10_report)
        except Exception as e:
            error_msg = f"âš ï¸ TOP10å‡¦ç†æ™‚é–“åˆ†æã®ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\n" if OUTPUT_LANGUAGE == 'ja' else f"âš ï¸ Error generating TOP10 analysis: {str(e)}\n"
            f.write(error_msg)
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®çµæœï¼ˆoutput_optimization_reportã¨TOP10ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
    result = {
        'optimized_file': optimized_filename,
        'report_file': report_filename,
        'top10_file': top10_filename
    }
    
    return result

def demonstrate_execution_plan_size_extraction():
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ã®ã‚µã‚¤ã‚ºæ¨å®šæ©Ÿèƒ½ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    print("ğŸ§ª å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®šæ©Ÿèƒ½ã®ãƒ‡ãƒ¢")
    print("-" * 50)
    
    # ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    sample_profiler_data = {
        "executionPlan": {
            "physicalPlan": {
                "nodes": [
                    {
                        "nodeName": "Scan Delta orders",
                        "id": "1",
                        "metrics": {
                            "estimatedSizeInBytes": 10485760,  # 10MB
                            "numFiles": 5,
                            "numPartitions": 2
                        },
                        "output": "[order_id#123, customer_id#124, amount#125] orders",
                        "details": "Table: catalog.database.orders"
                    },
                    {
                        "nodeName": "Scan Delta customers",
                        "id": "2", 
                        "metrics": {
                            "estimatedSizeInBytes": 52428800,  # 50MB
                            "numFiles": 10,
                            "numPartitions": 4
                        },
                        "output": "[customer_id#126, name#127, region#128] customers"
                    }
                ]
            }
        }
    }
    
    print("ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œãƒ—ãƒ©ãƒ³:")
    print("  â€¢ orders ãƒ†ãƒ¼ãƒ–ãƒ«: estimatedSizeInBytes = 10,485,760 (10MB)")
    print("  â€¢ customers ãƒ†ãƒ¼ãƒ–ãƒ«: estimatedSizeInBytes = 52,428,800 (50MB)")
    print("")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®šã®å®Ÿè¡Œ
    table_size_estimates = extract_table_size_estimates_from_plan(sample_profiler_data)
    
    print("ğŸ” æŠ½å‡ºã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®š:")
    if table_size_estimates:
        for table_name, size_info in table_size_estimates.items():
            print(f"  ğŸ“‹ {table_name}:")
            print(f"    - ã‚µã‚¤ã‚º: {size_info['estimated_size_mb']:.1f}MB")
            print(f"    - ä¿¡é ¼åº¦: {size_info['confidence']}")
            print(f"    - ã‚½ãƒ¼ã‚¹: {size_info['source']}")
            if 'num_files' in size_info:
                print(f"    - ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {size_info['num_files']}")
            if 'num_partitions' in size_info:
                print(f"    - ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {size_info['num_partitions']}")
            print("")
    else:
        print("  âš ï¸ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®šæƒ…å ±ãŒæŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
    
    print("ğŸ’¡ BROADCASTåˆ†æã¸ã®å½±éŸ¿:")
    if table_size_estimates:
        for table_name, size_info in table_size_estimates.items():
            size_mb = size_info['estimated_size_mb']
            if size_mb <= 30:
                print(f"  âœ… {table_name}: {size_mb:.1f}MB â‰¤ 30MB â†’ BROADCASTæ¨å¥¨")
            else:
                print(f"  âŒ {table_name}: {size_mb:.1f}MB > 30MB â†’ BROADCASTéæ¨å¥¨")
    
    print("")
    print("ğŸ¯ å¾“æ¥ã®æ¨å®šæ–¹æ³•ã¨ã®æ¯”è¼ƒ:")
    print("  ğŸ“ˆ å¾“æ¥: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã®é–“æ¥æ¨å®šï¼ˆæ¨å®šç²¾åº¦: ä¸­ï¼‰")
    print("  âŒ æ–°æ©Ÿèƒ½: å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã® estimatedSizeInBytes æ´»ç”¨ï¼ˆåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰")
    print("  â„¹ï¸ ç¾åœ¨: 3.0å€åœ§ç¸®ç‡ã§ã®ä¿å®ˆçš„æ¨å®šã‚’æ¡ç”¨")
    
    return {}

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: SQLæœ€é©åŒ–é–¢é€£é–¢æ•°ï¼ˆå®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‚µã‚¤ã‚ºæ¨å®šå¯¾å¿œï¼‰")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸš€ SQLã‚¯ã‚¨ãƒªæœ€é©åŒ–ã®å®Ÿè¡Œï¼ˆã‚¹ãƒ†ãƒƒãƒ—1: ã‚¯ã‚¨ãƒªæŠ½å‡ºï¼‰
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®æŠ½å‡º
# MAGIC - æŠ½å‡ºã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®è©³ç´°è¡¨ç¤ºï¼ˆ64KBã¾ã§ï¼‰
# MAGIC - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã®è¨­å®šï¼‰

# COMMAND ----------

# ğŸš€ SQLã‚¯ã‚¨ãƒªæœ€é©åŒ–ã®å®Ÿè¡Œ
print("\n" + "ğŸš€" * 20)
print("ğŸ”§ ã€SQLã‚¯ã‚¨ãƒªæœ€é©åŒ–ã®å®Ÿè¡Œã€‘")
print("ğŸš€" * 20)

# 1. ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®æŠ½å‡º
print("\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—1: ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®æŠ½å‡º")
print("-" * 40)

original_query = extract_original_query_from_profiler_data(profiler_data)

if original_query:
    print(f"âœ… ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã‚’æŠ½å‡ºã—ã¾ã—ãŸ ({len(original_query)} æ–‡å­—)")
    print(f"ğŸ” ã‚¯ã‚¨ãƒªãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
    # 64KB (65536æ–‡å­—) ã¾ã§è¡¨ç¤º
    max_display_chars = 65536
    if len(original_query) > max_display_chars:
        preview = original_query[:max_display_chars] + f"\n... (æ®‹ã‚Š {len(original_query) - max_display_chars} æ–‡å­—ã¯çœç•¥)"
    else:
        preview = original_query
    print(f"   {preview}")
else:
    print("âš ï¸ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    print("   æ‰‹å‹•ã§ã‚¯ã‚¨ãƒªã‚’è¨­å®šã—ã¦ãã ã•ã„")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã‚’è¨­å®š
    original_query = """
    -- ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªï¼ˆå®Ÿéš›ã®ã‚¯ã‚¨ãƒªã«ç½®ãæ›ãˆã¦ãã ã•ã„ï¼‰
    SELECT 
        customer_id,
        SUM(order_amount) as total_amount,
        COUNT(*) as order_count
    FROM orders 
    WHERE order_date >= '2023-01-01'
    GROUP BY customer_id
    ORDER BY total_amount DESC
    LIMIT 100
    """
    print(f"ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã‚’è¨­å®šã—ã¾ã—ãŸ")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¤– LLMã«ã‚ˆã‚‹SQLæœ€é©åŒ–ï¼ˆã‚¹ãƒ†ãƒƒãƒ—2: æœ€é©åŒ–å®Ÿè¡Œï¼‰
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - LLMã‚’ä½¿ç”¨ã—ãŸæŠ½å‡ºã‚¯ã‚¨ãƒªã®æœ€é©åŒ–
# MAGIC - æœ€é©åŒ–çµæœã®è©³ç´°è¡¨ç¤ºï¼ˆ1000è¡Œã¾ã§ï¼‰
# MAGIC - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ä»£æ›¿å‡¦ç†

# COMMAND ----------

# ğŸ¤– ã‚¹ãƒ†ãƒƒãƒ—2: LLMã«ã‚ˆã‚‹SQLæœ€é©åŒ–
print("\nğŸ¤– ã‚¹ãƒ†ãƒƒãƒ—2: LLMã«ã‚ˆã‚‹SQLæœ€é©åŒ–")
print("-" * 40)

if original_query.strip():
    print(f"ğŸ”„ {provider.upper()} ã‚’ä½¿ç”¨ã—ã¦ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ä¸­...")
    
    # thinking_enabled: Trueã®å ´åˆã«analysis_resultãŒãƒªã‚¹ãƒˆã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚å¯¾å¿œ
    if isinstance(analysis_result, list):
        # ãƒªã‚¹ãƒˆã®å ´åˆã¯ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã¿ã‚’æŠ½å‡ºã—ã¦LLMã«æ¸¡ã™
        analysis_result_str = extract_main_content_from_thinking_response(analysis_result)
    else:
        analysis_result_str = str(analysis_result)
    
    optimized_result = generate_optimized_query_with_llm(
        original_query, 
        analysis_result_str, 
        extracted_metrics
    )
    
    # thinking_enabled: Trueã®å ´åˆã«optimized_resultãŒãƒªã‚¹ãƒˆã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚å¯¾å¿œ
    optimized_result_display = optimized_result
    if isinstance(optimized_result, list):
        # è¡¨ç¤ºç”¨ã¯äººé–“ã«èª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›
        optimized_result_display = format_thinking_response(optimized_result)
        # ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã¿ã‚’æŠ½å‡ºï¼ˆå¾Œç¶šå‡¦ç†ç”¨ï¼‰
        optimized_result = extract_main_content_from_thinking_response(optimized_result)
    
    if optimized_result and not str(optimized_result).startswith("âš ï¸"):
        print("âœ… SQLæœ€é©åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
        print(f"ğŸ“„ æœ€é©åŒ–çµæœã®è©³ç´°:")
        
        # æœ€é©åŒ–çµæœã®è©³ç´°ã‚’è¡¨ç¤ºï¼ˆ1000è¡Œã¾ã§ï¼‰
        lines = optimized_result_display.split('\n')
        max_display_lines = 1000
        
        if len(lines) <= max_display_lines:
            # å…¨è¡Œè¡¨ç¤º
            for line in lines:
                print(f"   {line}")
        else:
            # 1000è¡Œã¾ã§è¡¨ç¤º
            for line in lines[:max_display_lines]:
                print(f"   {line}")
            print(f"   ... (æ®‹ã‚Š {len(lines) - max_display_lines} è¡Œã¯çœç•¥ã€è©³ç´°ã¯ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª)")
        
    else:
        print(f"âŒ SQLæœ€é©åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        print(f"   ã‚¨ãƒ©ãƒ¼: {optimized_result}")
        optimized_result = "æœ€é©åŒ–ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ‰‹å‹•ã§ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
else:
    print("âš ï¸ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãŒç©ºã®ãŸã‚ã€æœ€é©åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
    optimized_result = "ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æœ€é©åŒ–ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ’¾ æœ€é©åŒ–çµæœã®ä¿å­˜ï¼ˆã‚¹ãƒ†ãƒƒãƒ—3: ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼‰
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼ˆæ¥é ­èª: output_ï¼‰
# MAGIC - ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã€æœ€é©åŒ–ã‚¯ã‚¨ãƒªã€ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
# MAGIC - ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°æƒ…å ±è¡¨ç¤º

# COMMAND ----------

# ğŸ’¾ ã‚¹ãƒ†ãƒƒãƒ—3: æœ€é©åŒ–çµæœã®ä¿å­˜
print("\nğŸ’¾ ã‚¹ãƒ†ãƒƒãƒ—3: æœ€é©åŒ–çµæœã®ä¿å­˜")
print("-" * 40)

# å¿…è¦ãªå¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
missing_variables = []

# original_query ã®ãƒã‚§ãƒƒã‚¯
try:
    original_query
except NameError:
    missing_variables.append("original_query (ã‚»ãƒ«19ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)")
    original_query = ""

# optimized_result ã®ãƒã‚§ãƒƒã‚¯  
try:
    optimized_result
except NameError:
    missing_variables.append("optimized_result (ã‚»ãƒ«20ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)")
    optimized_result = ""

# extracted_metrics ã®ãƒã‚§ãƒƒã‚¯
try:
    extracted_metrics
except NameError:
    missing_variables.append("extracted_metrics (ã‚»ãƒ«12ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)")
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ã—ã¦æœ€å°é™ã®æ§‹é€ ã‚’è¨­å®š
    extracted_metrics = {
        'query_info': {'query_id': 'unknown'},
        'overall_metrics': {},
        'bottleneck_indicators': {}
    }

# analysis_result ã®ãƒã‚§ãƒƒã‚¯
try:
    analysis_result
except NameError:
    missing_variables.append("analysis_result (ã‚»ãƒ«16ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)")
    analysis_result = ""

if missing_variables:
    print("âŒ å¿…è¦ãªå¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“:")
    for var in missing_variables:
        print(f"   â€¢ {var}")
    print("\nâš ï¸ ä¸Šè¨˜ã®ã‚»ãƒ«ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ã‹ã‚‰ã€ã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    print("ğŸ“‹ æ­£ã—ã„å®Ÿè¡Œé †åº: ã‚»ãƒ«11 â†’ ã‚»ãƒ«12 â†’ ... â†’ ã‚»ãƒ«19 â†’ ã‚»ãƒ«20 â†’ ã‚»ãƒ«21")
    print("\nğŸ”„ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¦å‡¦ç†ã‚’ç¶™ç¶šã—ã¾ã™ã€‚")

# å¤‰æ•°ãŒå­˜åœ¨ã™ã‚‹ï¼ˆã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãŒè¨­å®šã•ã‚ŒãŸï¼‰å ´åˆã®å‡¦ç†
if original_query.strip() and str(optimized_result).strip():
    print("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆä¸­...")
    
    try:
        saved_files = save_optimized_sql_files(
            original_query,
            optimized_result,
            extracted_metrics,
            analysis_result
        )
        
        print("âœ… ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸ:")
        for file_type, filename in saved_files.items():
            file_type_jp = {
                'original_file': 'ã‚ªãƒªã‚¸ãƒŠãƒ«SQLã‚¯ã‚¨ãƒª',
                'optimized_file': 'æœ€é©åŒ–SQLã‚¯ã‚¨ãƒª',
                'report_file': 'æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ'
            }
            print(f"   ğŸ“„ {file_type_jp.get(file_type, file_type)}: {filename}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèª
        import os
        print(f"\nğŸ“Š ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°:")
        for file_type, filename in saved_files.items():
            if os.path.exists(filename):
                file_size = os.path.getsize(filename)
                print(f"   {filename}: {file_size:,} bytes")
            else:
                print(f"   âš ï¸ {filename}: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        print("âš ï¸ ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’è¨­å®šã—ã¾ã™ã€‚")
        saved_files = {}
        
else:
    print("âš ï¸ ã‚¯ã‚¨ãƒªã¾ãŸã¯æœ€é©åŒ–çµæœãŒä¸å®Œå…¨ãªãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
    saved_files = {}

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ§ª Databricks Notebookã§ã®å®Ÿè¡Œã‚¬ã‚¤ãƒ‰ï¼ˆã‚¹ãƒ†ãƒƒãƒ—4: å®Ÿè¡Œæ–¹æ³•ï¼‰
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ä½¿ç”¨æ–¹æ³•èª¬æ˜
# MAGIC - Databricks Notebookã§ã®å®Ÿè¡Œæ‰‹é †ã‚¬ã‚¤ãƒ‰
# MAGIC - SQLã‚¯ã‚¨ãƒªã®ç›´æ¥å®Ÿè¡Œæ–¹æ³•
# MAGIC - é‡è¦ãªæ³¨æ„äº‹é …ã®è¡¨ç¤º

# COMMAND ----------

# ğŸ§ª ã‚¹ãƒ†ãƒƒãƒ—4: Databricks Notebookã§ã®å®Ÿè¡Œã‚¬ã‚¤ãƒ‰
print("\nğŸ§ª ã‚¹ãƒ†ãƒƒãƒ—4: Databricks Notebookã§ã®å®Ÿè¡Œã‚¬ã‚¤ãƒ‰")
print("-" * 40)

# saved_fileså¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
try:
    saved_files
except NameError:
    print("âŒ saved_fileså¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    print("âš ï¸ ã‚»ãƒ«21 (æœ€é©åŒ–çµæœã®ä¿å­˜) ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    saved_files = {}

if saved_files:
    original_file = saved_files.get('original_file', '')
    optimized_file = saved_files.get('optimized_file', '')
    report_file = saved_files.get('report_file', '')
    
    print("ğŸš€ Databricks Notebookã§ã®å®Ÿè¡Œæ‰‹é †:")
    print("1. ç”Ÿæˆã•ã‚ŒãŸSQLãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¢ºèª")
    print("2. å¿…è¦ã«å¿œã˜ã¦ã‚¯ã‚¨ãƒªã‚’æ‰‹å‹•èª¿æ•´")
    print("3. Notebookå†…ã§ç›´æ¥SQLã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ")
    print("4. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã¨æ¯”è¼ƒ")
    
    print(f"\nğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:")
    if original_file:
        print(f"   ğŸ“„ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒª: {original_file}")
    if optimized_file:
        print(f"   ğŸš€ æœ€é©åŒ–ã‚¯ã‚¨ãƒª: {optimized_file}")
    if report_file:
        print(f"   ğŸ“Š åˆ†æãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")
    
    if optimized_file:
        print(f"\nğŸ”§ Databrics Notebookã§ã®å®Ÿè¡Œæ–¹æ³•:")
        print(f"   # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰SQLã‚’èª­ã¿è¾¼ã‚“ã§å®Ÿè¡Œ")
        print(f"   optimized_sql = open('{optimized_file}').read()")
        print(f"   df = spark.sql(optimized_sql)")
        print(f"   df.show()")
        print(f"   ")
        print(f"   # ã¾ãŸã¯ %sql ãƒã‚¸ãƒƒã‚¯ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨")
        print(f"   # %sql [ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ã‚³ãƒ”ãƒ¼&ãƒšãƒ¼ã‚¹ãƒˆ]")
        print(f"   ")
        print(f"   # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šä¾‹")
        print(f"   import time")
        print(f"   start_time = time.time()")
        print(f"   result_count = df.count()")
        print(f"   execution_time = time.time() - start_time")
        print(f"   print(f'å®Ÿè¡Œæ™‚é–“: {{execution_time:.2f}} ç§’, è¡Œæ•°: {{result_count:,}}')")
    
    print(f"\nâš ï¸ é‡è¦ãªæ³¨æ„äº‹é …:")
    print(f"   â€¢ æœ¬ç•ªç’°å¢ƒã§ã®å®Ÿè¡Œå‰ã«ã€å¿…ãšãƒ†ã‚¹ãƒˆç’°å¢ƒã§æ¤œè¨¼ã—ã¦ãã ã•ã„")
    print(f"   â€¢ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ§‹é€ ã‚„ã‚µã‚¤ã‚ºã«ã‚ˆã£ã¦çµæœã¯å¤‰ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
    print(f"   â€¢ ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ã®ç¢ºèª: EXPLAIN æ–‡ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
    print(f"   â€¢ Databricks SQLã‚¨ãƒ‡ã‚£ã‚¿ã§ã®å®Ÿè¡Œã‚‚å¯èƒ½ã§ã™")

else:
    print("âš ï¸ å®Ÿè¡Œç”¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ æœ€çµ‚å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - å…¨å‡¦ç†ã®å®Œäº†çŠ¶æ³ç¢ºèª
# MAGIC - ç”Ÿæˆã•ã‚ŒãŸå…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§è¡¨ç¤º
# MAGIC - æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®æç¤º
# MAGIC - å‡¦ç†å®Œäº†ã®ç·åˆå ±å‘Š

# COMMAND ----------

# ğŸ“Š æœ€çµ‚ã‚µãƒãƒªãƒ¼ã®æ›´æ–°
print("\n" + "ğŸ‰" * 25)
print("ğŸ ã€SQLæœ€é©åŒ–å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼ã€‘")
print("ğŸ‰" * 25)

print("âœ… SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºå®Œäº†")

# LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±ã®å‹•çš„è¡¨ç¤º
try:
    current_provider = LLM_CONFIG.get('provider', 'unknown')
    provider_display_names = {
        'databricks': f"Databricks ({LLM_CONFIG.get('databricks', {}).get('endpoint_name', 'Model Serving')})",
        'openai': f"OpenAI ({LLM_CONFIG.get('openai', {}).get('model', 'GPT-4')})",
        'azure_openai': f"Azure OpenAI ({LLM_CONFIG.get('azure_openai', {}).get('deployment_name', 'GPT-4')})",
        'anthropic': f"Anthropic ({LLM_CONFIG.get('anthropic', {}).get('model', 'Claude')})"
    }
    provider_display = provider_display_names.get(current_provider, f"{current_provider}ï¼ˆæœªçŸ¥ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼‰")
    print(f"âœ… {provider_display}ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Œäº†")
except Exception as e:
    print("âœ… LLMã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Œäº†")

print("âœ… åˆ†æçµæœä¿å­˜å®Œäº†")
print("âœ… ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªæŠ½å‡ºå®Œäº†")
print("âœ… LLMã«ã‚ˆã‚‹SQLæœ€é©åŒ–å®Œäº†")
print("âœ… æœ€é©åŒ–çµæœãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå®Œäº†ï¼ˆæ¥é ­èª: output_ï¼‰")

# å¿…è¦ãªå¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
missing_summary_vars = []

try:
    result_output_path
except NameError:
    missing_summary_vars.append("result_output_path (ã‚»ãƒ«16ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)")

try:
    saved_files
except NameError:
    missing_summary_vars.append("saved_files (ã‚»ãƒ«19ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)")
    saved_files = {}

print(f"\nğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:")

if 'result_output_path' in globals():
    print(f"   ğŸ“„ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æãƒ¬ãƒãƒ¼ãƒˆ: {result_output_path}")
else:
    print("   ğŸ“„ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æãƒ¬ãƒãƒ¼ãƒˆ: (ã‚»ãƒ«16ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)")

if saved_files:
    for file_type, filename in saved_files.items():
        file_type_jp = {
            'original_file': 'ğŸ“„ ã‚ªãƒªã‚¸ãƒŠãƒ«SQL',
            'optimized_file': 'ğŸš€ æœ€é©åŒ–SQL',
            'report_file': 'ğŸ“Š æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ'
        }
        icon_name = file_type_jp.get(file_type, f"ğŸ“„ {file_type}")
        print(f"   {icon_name}: {filename}")

print(f"\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
print(f"   1. ç”Ÿæˆã•ã‚ŒãŸSQLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª")
print(f"   2. ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã®å‹•ä½œç¢ºèª")
print(f"   3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š")
print(f"   4. æœ¬ç•ªç’°å¢ƒã¸ã®é©ç”¨æ¤œè¨")

print("ğŸ‰" * 25)

# COMMAND ----------

# MAGIC %md
# MAGIC # ğŸ“š å‚è€ƒãƒ»å¿œç”¨ã‚»ã‚¯ã‚·ãƒ§ãƒ³
# MAGIC
# MAGIC **ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯å¿œç”¨çš„ãªä½¿ç”¨æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™**
# MAGIC
# MAGIC ğŸ“‹ **å‚è€ƒæƒ…å ±:**
# MAGIC - ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ–¹æ³•
# MAGIC - ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆ
# MAGIC - ã‚¨ãƒ©ãƒ¼å¯¾å‡¦æ–¹æ³•
# MAGIC - é«˜åº¦ãªä½¿ç”¨ä¾‹
# MAGIC
# MAGIC ğŸ’¡ **ç”¨é€”:** ãƒ„ãƒ¼ãƒ«ã®æ´»ç”¨ã‚’æ·±ã‚ãŸã„å ´åˆã«å‚ç…§ã—ã¦ãã ã•ã„

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“š è¿½åŠ ã®ä½¿ç”¨æ–¹æ³•ã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
# MAGIC
# MAGIC ### ğŸ”§ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ–¹æ³•
# MAGIC
# MAGIC #### æ–¹æ³• 1: Databricks UI ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# MAGIC 1. **Data** > **Create Table** ã‚’ã‚¯ãƒªãƒƒã‚¯
# MAGIC 2. **Upload File** ã‚’é¸æŠ
# MAGIC 3. SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—
# MAGIC 4. ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†å¾Œã€ãƒ‘ã‚¹ã‚’ã‚³ãƒ”ãƒ¼
# MAGIC 5. ä¸Šè¨˜ã® `JSON_FILE_PATH` ã«è¨­å®š
# MAGIC
# MAGIC #### æ–¹æ³• 2: dbutils ã‚’ä½¿ç”¨
# MAGIC ```python
# MAGIC # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’FileStoreã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# MAGIC dbutils.fs.cp("file:/local/path/profiler.json", "dbfs:/FileStore/profiler.json")
# MAGIC
# MAGIC # å¤–éƒ¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰ã®ã‚³ãƒ”ãƒ¼
# MAGIC dbutils.fs.cp("s3a://bucket/profiler.json", "dbfs:/FileStore/profiler.json")
# MAGIC ```

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ” å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’æ´»ç”¨ã—ãŸBROADCASTåˆ†ææ©Ÿèƒ½ã®è¿½åŠ å®Œäº†
# MAGIC
# MAGIC ã“ã®æ›´æ–°ã«ã‚ˆã‚Šã€JSONãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰æŠ½å‡ºã—ãŸå®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’æ´»ç”¨ã—ãŸBROADCASTåˆ†æãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸï¼š
# MAGIC
# MAGIC ### ğŸ†• æ–°æ©Ÿèƒ½
# MAGIC
# MAGIC #### 1. `extract_execution_plan_info()` é–¢æ•°
# MAGIC - **æ©Ÿèƒ½**: JSONãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰è©³ç´°ãªå®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’æŠ½å‡º
# MAGIC - **æ¤œå‡ºå†…å®¹**:
# MAGIC   - BROADCASTãƒãƒ¼ãƒ‰ï¼ˆæ—¢å­˜ã®é©ç”¨çŠ¶æ³ï¼‰
# MAGIC   - JOINãƒãƒ¼ãƒ‰ï¼ˆæˆ¦ç•¥ï¼šbroadcast_hash_join, sort_merge_join, shuffle_hash_joinç­‰ï¼‰
# MAGIC   - ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«åã€ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã€ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ï¼‰
# MAGIC   - ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±ï¼‰
# MAGIC   - é›†ç´„ãƒãƒ¼ãƒ‰ï¼ˆGROUP BYè¡¨ç¾ã€é›†ç´„é–¢æ•°ï¼‰
# MAGIC - **å‡ºåŠ›**: æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ—ãƒ©ãƒ³æƒ…å ±è¾æ›¸
# MAGIC
# MAGIC #### 2. `analyze_broadcast_feasibility()` é–¢æ•°ã®æ‹¡å¼µ
# MAGIC - **è¿½åŠ æ©Ÿèƒ½**:
# MAGIC   - æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã®è‡ªå‹•æ¤œå‡º
# MAGIC   - ãƒ—ãƒ©ãƒ³æƒ…å ±ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®æ­£ç¢ºãªç‰¹å®š
# MAGIC   - æ—¢ã«æœ€é©åŒ–æ¸ˆã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã¨æ–°è¦æ¨å¥¨ã®åŒºåˆ¥
# MAGIC   - å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æçµæœã®è©³ç´°è¨˜éŒ²
# MAGIC - **åˆ¤å®šå¼·åŒ–**:
# MAGIC   - `already_applied`: æ—¢ã«BROADCASTé©ç”¨æ¸ˆã¿
# MAGIC   - `new_recommendation`: æ–°è¦BROADCASTæ¨å¥¨
# MAGIC   - ãƒ—ãƒ©ãƒ³æƒ…å ±ã¨æ•´åˆæ€§ã®ã‚ã‚‹åˆ†æ
# MAGIC
# MAGIC #### 3. SQLæœ€é©åŒ–ã®æ”¹å–„
# MAGIC - **ãƒ—ãƒ©ãƒ³è€ƒæ…®**: å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’å«ã‚€BROADCASTåˆ†æ
# MAGIC - **LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¼·åŒ–**: ãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’LLMã«æä¾›ã—ã¦æ­£ç¢ºãªæœ€é©åŒ–
# MAGIC - **30MBé–¾å€¤ã®å³æ ¼é©ç”¨**: å®Ÿéš›ã®Sparkãƒ—ãƒ©ãƒ³æƒ…å ±ã¨ã®æ•´åˆæ€§ç¢ºä¿

# COMMAND ----------

# ğŸ” å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’æ´»ç”¨ã—ãŸBROADCASTåˆ†æã®ãƒ‡ãƒ¢é–¢æ•°
def demonstrate_plan_based_broadcast_analysis():
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’æ´»ç”¨ã—ãŸBROADCASTåˆ†æã®ãƒ‡ãƒ¢
    """
    print("\nğŸ” å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’æ´»ç”¨ã—ãŸBROADCASTåˆ†æãƒ‡ãƒ¢")
    print("=" * 60)
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    try:
        # extracted_metrics ã¨ profiler_data ã‚’ä½¿ç”¨
        extracted_metrics
        profiler_data
        
        print("ğŸ“Š å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’æŠ½å‡ºä¸­...")
        plan_info = extract_execution_plan_info(profiler_data)
        
        print(f"âœ… ãƒ—ãƒ©ãƒ³æƒ…å ±ã®æŠ½å‡ºå®Œäº†")
        print(f"   - ç·ãƒãƒ¼ãƒ‰æ•°: {plan_info.get('plan_summary', {}).get('total_nodes', 0)}")
        print(f"   - BROADCASTãƒãƒ¼ãƒ‰æ•°: {plan_info.get('plan_summary', {}).get('broadcast_nodes_count', 0)}")
        print(f"   - JOINãƒãƒ¼ãƒ‰æ•°: {plan_info.get('plan_summary', {}).get('join_nodes_count', 0)}")
        print(f"   - ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰æ•°: {plan_info.get('plan_summary', {}).get('scan_nodes_count', 0)}")
        print(f"   - ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰æ•°: {plan_info.get('plan_summary', {}).get('shuffle_nodes_count', 0)}")
        
        # JOINæˆ¦ç•¥ã®åˆ†æ
        join_strategies = plan_info.get('plan_summary', {}).get('unique_join_strategies', [])
        if join_strategies:
            print(f"\nğŸ”— æ¤œå‡ºã•ã‚ŒãŸJOINæˆ¦ç•¥:")
            for strategy in join_strategies:
                print(f"   - {strategy}")
        
        # æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³
        broadcast_nodes = plan_info.get('broadcast_nodes', [])
        if broadcast_nodes:
            print(f"\nğŸ“¡ æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³:")
            for i, node in enumerate(broadcast_nodes[:3]):  # æœ€å¤§3å€‹ã¾ã§è¡¨ç¤º
                print(f"   {i+1}. {node['node_name'][:60]}...")
                metadata_count = len(node.get('metadata', []))
                print(f"      ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é …ç›®æ•°: {metadata_count}")
        else:
            print(f"\nğŸ“¡ ç¾åœ¨BROADCASTã¯é©ç”¨ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ã®è©³ç´°
        table_scan_details = plan_info.get('table_scan_details', {})
        if table_scan_details:
            print(f"\nğŸ“‹ ã‚¹ã‚­ãƒ£ãƒ³ã•ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«:")
            for table_name, scan_detail in list(table_scan_details.items())[:5]:  # æœ€å¤§5å€‹ã¾ã§è¡¨ç¤º
                file_format = scan_detail.get('file_format', 'unknown')
                pushed_filters = len(scan_detail.get('pushed_filters', []))
                output_columns = len(scan_detail.get('output_columns', []))
                print(f"   - {table_name}")
                print(f"     ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_format}, ãƒ•ã‚£ãƒ«ã‚¿: {pushed_filters}å€‹, ã‚«ãƒ©ãƒ : {output_columns}å€‹")
        
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®æŠ½å‡ºï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        try:
            original_query_for_demo = extract_original_query_from_profiler_data(profiler_data)
            if not original_query_for_demo:
                original_query_for_demo = "SELECT * FROM table1 t1 JOIN table2 t2 ON t1.id = t2.id"
        except:
            original_query_for_demo = "SELECT * FROM table1 t1 JOIN table2 t2 ON t1.id = t2.id"
        
        print(f"\nğŸ¯ ãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’è€ƒæ…®ã—ãŸBROADCASTåˆ†æã‚’å®Ÿè¡Œä¸­...")
        broadcast_analysis = analyze_broadcast_feasibility(
            extracted_metrics, 
            original_query_for_demo, 
            plan_info
        )
        
        # BROADCASTåˆ†æçµæœã®è¡¨ç¤º
        print(f"\nğŸ“Š BROADCASTåˆ†æçµæœ:")
        print(f"   - JOINã‚¯ã‚¨ãƒª: {'ã¯ã„' if broadcast_analysis['is_join_query'] else 'ã„ã„ãˆ'}")
        print(f"   - æ—¢ã«æœ€é©åŒ–æ¸ˆã¿: {'ã¯ã„' if broadcast_analysis['already_optimized'] else 'ã„ã„ãˆ'}")
        print(f"   - Spark BROADCASTé–¾å€¤: {broadcast_analysis['spark_threshold_mb']:.1f}MB")
        print(f"   - é©ç”¨å¯èƒ½æ€§: {broadcast_analysis['feasibility']}")
        print(f"   - BROADCASTå€™è£œæ•°: {len(broadcast_analysis['broadcast_candidates'])}")
        
        # å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æçµæœ
        exec_plan_analysis = broadcast_analysis.get('execution_plan_analysis', {})
        if exec_plan_analysis:
            print(f"\nğŸ” å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æè©³ç´°:")
            print(f"   - BROADCASTãŒæ—¢ã«ä½¿ç”¨ä¸­: {'ã¯ã„' if exec_plan_analysis['has_broadcast_joins'] else 'ã„ã„ãˆ'}")
            print(f"   - ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹JOINæˆ¦ç•¥: {', '.join(exec_plan_analysis.get('unique_join_strategies', []))}")
            print(f"   - ãƒ—ãƒ©ãƒ³å†…ã®ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(exec_plan_analysis.get('tables_in_plan', []))}")
        
        # BROADCASTå€™è£œã®è©³ç´°
        if broadcast_analysis['broadcast_candidates']:
            print(f"\nğŸ”¹ BROADCASTå€™è£œè©³ç´°:")
            for candidate in broadcast_analysis['broadcast_candidates'][:3]:  # æœ€å¤§3å€‹ã¾ã§è¡¨ç¤º
                status = candidate.get('status', 'unknown')
                status_icon = "âœ…" if status == "already_applied" else "ğŸ†•" if status == "new_recommendation" else "ğŸ”"
                print(f"   {status_icon} {candidate['table']}")
                print(f"      éåœ§ç¸®ã‚µã‚¤ã‚º: {candidate['estimated_uncompressed_mb']:.1f}MB")
                print(f"      åœ§ç¸®ã‚µã‚¤ã‚º: {candidate['estimated_compressed_mb']:.1f}MB")
                print(f"      ä¿¡é ¼åº¦: {candidate['confidence']}")
                print(f"      ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}")
                print(f"      æ ¹æ‹ : {candidate['reasoning'][:80]}...")
        
        # æ¨å¥¨äº‹é …
        if broadcast_analysis['recommendations']:
            print(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
            for rec in broadcast_analysis['recommendations'][:5]:  # æœ€å¤§5å€‹ã¾ã§è¡¨ç¤º
                print(f"   â€¢ {rec}")
        
        print(f"\nâœ… ãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’æ´»ç”¨ã—ãŸBROADCASTåˆ†æå®Œäº†")
        
    except NameError as e:
        print(f"âš ï¸ å¿…è¦ãªå¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“: {str(e)}")
        print("   ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š")
        print("   - Cell 11: JSONèª­ã¿è¾¼ã¿")
        print("   - Cell 12: ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º")
    except Exception as e:
        print(f"âŒ ãƒ—ãƒ©ãƒ³åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

# ãƒ‡ãƒ¢å®Ÿè¡Œã®å‘¼ã³å‡ºã—ä¾‹ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã•ã‚Œã¦ã„ã‚‹ã®ã§ã€å¿…è¦ã«å¿œã˜ã¦æœ‰åŠ¹åŒ–ï¼‰
# demonstrate_plan_based_broadcast_analysis()

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’æ´»ç”¨ã—ãŸBROADCASTåˆ†æãƒ‡ãƒ¢")

# COMMAND ----------

print("ğŸ‰ å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’æ´»ç”¨ã—ãŸBROADCASTåˆ†ææ©Ÿèƒ½ã®è¿½åŠ å®Œäº†")
print("ğŸ“Š SQLã®æœ€é©åŒ–ã«ã‚ˆã‚Šç²¾å¯†ã§å®Ÿç”¨çš„ãªBROADCASTæ¨å¥¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸ")
print("ğŸ” æ—¢å­˜ã®æœ€é©åŒ–çŠ¶æ³ã‚’è€ƒæ…®ã—ãŸã€ã‚ˆã‚Šå®Ÿéš›çš„ãªåˆ†æã‚’æä¾›ã—ã¾ã™")
print("âœ… å…¨ã¦ã®æ©Ÿèƒ½ãŒæ­£å¸¸ã«çµ±åˆã•ã‚Œã¾ã—ãŸ")

# ğŸ›ï¸ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆ
#
# - **LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼**: `LLM_CONFIG` ã§ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨APIã‚­ãƒ¼ã‚’åˆ‡ã‚Šæ›¿ãˆ
# - **ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º**: `extract_performance_metrics` é–¢æ•°å†…ã®é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
# - **åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: `analyze_bottlenecks_with_llm` é–¢æ•°å†…ã®åˆ†ææŒ‡ç¤º
# - **è¡¨ç¤ºå½¢å¼**: emoji ã¨å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®èª¿æ•´
#
# ğŸ” ã‚¨ãƒ©ãƒ¼å¯¾å‡¦æ–¹æ³•
#
# 1. **LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼**: 
#    - Databricks: Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®çŠ¶æ…‹ç¢ºèª
#    - OpenAI/Azure/Anthropic: APIã‚­ãƒ¼ã¨ã‚¯ã‚©ãƒ¼ã‚¿ç¢ºèª
# 2. **ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼**: `dbutils.fs.ls("/FileStore/")` ã§ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ã‚’ç¢ºèª
# 3. **ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼**: å¤§ããªJSONãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ã‚¯ãƒ©ã‚¹ã‚¿ã®ãƒ¡ãƒ¢ãƒªè¨­å®šã‚’ç¢ºèª
#
# ğŸ’¡ é«˜åº¦ãªä½¿ç”¨ä¾‹
#
# ```python
# # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ‹¬åˆ†æ
# profiler_files = dbutils.fs.ls("/FileStore/profiler_logs/")
# for file_info in profiler_files:
#     if file_info.path.endswith('.json'):
#         profiler_data = load_profiler_json(file_info.path)
#         metrics = extract_performance_metrics(profiler_data)
#         # åˆ†æå‡¦ç†...
# ```

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        profiler_data = load_profiler_json(sys.argv[1])
        extracted_metrics = extract_performance_metrics(profiler_data)
        print("Testing skew detection...")
        # Test completed

