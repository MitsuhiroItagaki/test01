# Databricks notebook source
# MAGIC %md
# MAGIC # Databricks SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆ†æãƒ„ãƒ¼ãƒ«
# MAGIC 
# MAGIC ã“ã®notebookã¯ã€Databricksã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šã¨æ”¹å–„æ¡ˆã®æç¤ºã«å¿…è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã¦åˆ†æã‚’è¡Œã„ã¾ã™ã€‚
# MAGIC 
# MAGIC ## æ©Ÿèƒ½æ¦‚è¦
# MAGIC 
# MAGIC 1. **SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿**
# MAGIC    - Databricksã§å‡ºåŠ›ã•ã‚ŒãŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ­ã‚°ã®è§£æ
# MAGIC    - `graphs`ã‚­ãƒ¼ã«æ ¼ç´ã•ã‚ŒãŸå®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º
# MAGIC 
# MAGIC 2. **é‡è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º**
# MAGIC    - ã‚¯ã‚¨ãƒªåŸºæœ¬æƒ…å ±ï¼ˆIDã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã€å®Ÿè¡Œæ™‚é–“ãªã©ï¼‰
# MAGIC    - å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆå®Ÿè¡Œæ™‚é–“ã€ãƒ‡ãƒ¼ã‚¿é‡ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ãªã©ï¼‰
# MAGIC    - ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ»ãƒãƒ¼ãƒ‰è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹
# MAGIC    - ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è¨ˆç®—
# MAGIC 
# MAGIC 3. **AI ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ**
# MAGIC    - Databricks Claude 3.7 Sonnetã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨
# MAGIC    - æŠ½å‡ºãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
# MAGIC    - å…·ä½“çš„ãªæ”¹å–„æ¡ˆã®æç¤º
# MAGIC 
# MAGIC ---
# MAGIC 
# MAGIC **äº‹å‰æº–å‚™:**
# MAGIC - Databricks Claude 3.7 Sonnetã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¨­å®š
# MAGIC - SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®æº–å‚™ï¼ˆDBFS ã¾ãŸã¯ FileStoreï¼‰

# COMMAND ----------

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import json
import pandas as pd
from typing import Dict, List, Any
import requests
from pyspark.sql import SparkSession

# PySparké–¢æ•°ã‚’å®‰å…¨ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from pyspark.sql.functions import col, lit, when
    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType
    print("âœ… PySparké–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†")
except ImportError as e:
    print(f"âš ï¸ PySparké–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
    # åŸºæœ¬çš„ãªåˆ†æã«ã¯å½±éŸ¿ã—ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—

# Databricksç’°å¢ƒã®ç¢ºèª
spark = SparkSession.builder.getOrCreate()
print(f"âœ… Spark Version: {spark.version}")

# Databricks Runtimeæƒ…å ±ã‚’å®‰å…¨ã«å–å¾—
try:
    runtime_version = spark.conf.get('spark.databricks.clusterUsageTags.sparkVersion')
    print(f"âœ… Databricks Runtime: {runtime_version}")
except Exception:
    try:
        # ä»£æ›¿æ‰‹æ®µã§DBRæƒ…å ±ã‚’å–å¾—
        dbr_version = spark.conf.get('spark.databricks.clusterUsageTags.clusterName', 'Unknown')
        print(f"âœ… Databricks Cluster: {dbr_version}")
    except Exception:
        print("âœ… Databricks Environment: è¨­å®šæƒ…å ±ã®å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")

# COMMAND ----------

def load_profiler_json(file_path: str) -> Dict[str, Any]:
    """
    SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        file_path: JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆDBFS ã¾ãŸã¯ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ï¼‰
        
    Returns:
        Dict: ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸJSONãƒ‡ãƒ¼ã‚¿
    """
    try:
        # DBFSãƒ‘ã‚¹ã®å ´åˆã¯é©åˆ‡ã«å‡¦ç†
        if file_path.startswith('/dbfs/'):
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        elif file_path.startswith('dbfs:/'):
            # dbfs: ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’/dbfs/ã«å¤‰æ›
            local_path = file_path.replace('dbfs:', '/dbfs')
            with open(local_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        elif file_path.startswith('/FileStore/'):
            # FileStore ãƒ‘ã‚¹ã‚’ /dbfs/FileStore/ ã«å¤‰æ›
            local_path = '/dbfs' + file_path
            with open(local_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        
        print(f"âœ… JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {file_path}")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(str(data)):,} characters")
        return data
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {}

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: load_profiler_json")

# COMMAND ----------

def extract_performance_metrics(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã«å¿…è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
    """
    metrics = {
        "query_info": {},
        "overall_metrics": {},
        "stage_metrics": [],
        "node_metrics": [],
        "bottleneck_indicators": {},
        "liquid_clustering_analysis": {}
    }
    
    # åŸºæœ¬çš„ãªã‚¯ã‚¨ãƒªæƒ…å ±
    if 'query' in profiler_data:
        query = profiler_data['query']
        metrics["query_info"] = {
            "query_id": query.get('id', ''),
            "status": query.get('status', ''),
            "query_start_time": query.get('queryStartTimeMs', 0),
            "query_end_time": query.get('queryEndTimeMs', 0),
            "user": query.get('user', {}).get('displayName', ''),
            "query_text": query.get('queryText', '')[:300] + "..." if len(query.get('queryText', '')) > 300 else query.get('queryText', '')
        }
        
        # å…¨ä½“çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if 'metrics' in query:
            query_metrics = query['metrics']
            metrics["overall_metrics"] = {
                "total_time_ms": query_metrics.get('totalTimeMs', 0),
                "compilation_time_ms": query_metrics.get('compilationTimeMs', 0),
                "execution_time_ms": query_metrics.get('executionTimeMs', 0),
                "read_bytes": query_metrics.get('readBytes', 0),
                "read_remote_bytes": query_metrics.get('readRemoteBytes', 0),
                "read_cache_bytes": query_metrics.get('readCacheBytes', 0),
                "rows_produced_count": query_metrics.get('rowsProducedCount', 0),
                "rows_read_count": query_metrics.get('rowsReadCount', 0),
                "spill_to_disk_bytes": query_metrics.get('spillToDiskBytes', 0),
                "read_files_count": query_metrics.get('readFilesCount', 0),
                "task_total_time_ms": query_metrics.get('taskTotalTimeMs', 0),
                "photon_total_time_ms": query_metrics.get('photonTotalTimeMs', 0)
            }
    
    # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¹ãƒ†ãƒ¼ã‚¸ã¨ãƒãƒ¼ãƒ‰ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
    if 'graphs' in profiler_data and profiler_data['graphs']:
        graph = profiler_data['graphs'][0]
        
        # ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿
        if 'stageData' in graph:
            for stage in graph['stageData']:
                stage_metric = {
                    "stage_id": stage.get('stageId', ''),
                    "status": stage.get('status', ''),
                    "duration_ms": stage.get('keyMetrics', {}).get('durationMs', 0),
                    "num_tasks": stage.get('numTasks', 0),
                    "num_failed_tasks": stage.get('numFailedTasks', 0),
                    "num_complete_tasks": stage.get('numCompleteTasks', 0),
                    "start_time_ms": stage.get('startTimeMs', 0),
                    "end_time_ms": stage.get('endTimeMs', 0)
                }
                metrics["stage_metrics"].append(stage_metric)
        
        # ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ï¼ˆé‡è¦ãªã‚‚ã®ã®ã¿ï¼‰
        if 'nodes' in graph:
            for node in graph['nodes']:
                if not node.get('hidden', False):
                    node_metric = {
                        "node_id": node.get('id', ''),
                        "name": node.get('name', ''),
                        "tag": node.get('tag', ''),
                        "key_metrics": node.get('keyMetrics', {})
                    }
                    
                    # é‡è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã¿è©³ç´°æŠ½å‡º
                    detailed_metrics = {}
                    for metric in node.get('metrics', []):
                        metric_key = metric.get('key', '')
                        if any(keyword in metric_key.upper() for keyword in 
                               ['TIME', 'MEMORY', 'ROWS', 'BYTES', 'DURATION', 'PEAK', 'CUMULATIVE', 'EXCLUSIVE']):
                            detailed_metrics[metric_key] = {
                                'value': metric.get('value', 0),
                                'label': metric.get('label', ''),
                                'type': metric.get('metricType', '')
                            }
                    node_metric['detailed_metrics'] = detailed_metrics
                    metrics["node_metrics"].append(node_metric)
    
    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è¨ˆç®—
    metrics["bottleneck_indicators"] = calculate_bottleneck_indicators(metrics)
    
    # Liquid Clusteringåˆ†æ
    metrics["liquid_clustering_analysis"] = analyze_liquid_clustering_opportunities(profiler_data, metrics)
    
    return metrics

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: extract_performance_metrics")

# COMMAND ----------

def calculate_bottleneck_indicators(metrics: Dict[str, Any]) -> Dict[str, Any]:
    """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã‚’è¨ˆç®—"""
    indicators = {}
    
    overall = metrics.get('overall_metrics', {})
    total_time = overall.get('total_time_ms', 0)
    execution_time = overall.get('execution_time_ms', 0)
    compilation_time = overall.get('compilation_time_ms', 0)
    
    if total_time > 0:
        indicators['compilation_ratio'] = compilation_time / total_time
        indicators['execution_ratio'] = execution_time / total_time
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡
    read_bytes = overall.get('read_bytes', 0)
    cache_bytes = overall.get('read_cache_bytes', 0)
    if read_bytes > 0:
        indicators['cache_hit_ratio'] = cache_bytes / read_bytes
    
    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹ç‡
    rows_read = overall.get('rows_read_count', 0)
    rows_produced = overall.get('rows_produced_count', 0)
    if rows_read > 0:
        indicators['data_selectivity'] = rows_produced / rows_read
    
    # Photonä½¿ç”¨ç‡
    task_time = overall.get('task_total_time_ms', 0)
    photon_time = overall.get('photon_total_time_ms', 0)
    if task_time > 0:
        indicators['photon_ratio'] = photon_time / task_time
    
    # ã‚¹ãƒ”ãƒ«æ¤œå‡º
    spill_bytes = overall.get('spill_to_disk_bytes', 0)
    indicators['has_spill'] = spill_bytes > 0
    indicators['spill_bytes'] = spill_bytes
    
    # æœ€ã‚‚æ™‚é–“ã®ã‹ã‹ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸
    stage_durations = [(s['stage_id'], s['duration_ms']) for s in metrics.get('stage_metrics', []) if s['duration_ms'] > 0]
    if stage_durations:
        slowest_stage = max(stage_durations, key=lambda x: x[1])
        indicators['slowest_stage_id'] = slowest_stage[0]
        indicators['slowest_stage_duration'] = slowest_stage[1]
    
    # æœ€ã‚‚ãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨ã™ã‚‹ãƒãƒ¼ãƒ‰
    memory_usage = []
    for node in metrics.get('node_metrics', []):
        peak_memory = node.get('key_metrics', {}).get('peakMemoryBytes', 0)
        if peak_memory > 0:
            memory_usage.append((node['node_id'], node['name'], peak_memory))
    
    if memory_usage:
        highest_memory_node = max(memory_usage, key=lambda x: x[2])
        indicators['highest_memory_node_id'] = highest_memory_node[0]
        indicators['highest_memory_node_name'] = highest_memory_node[1]
        indicators['highest_memory_bytes'] = highest_memory_node[2]
    
    return indicators

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: calculate_bottleneck_indicators")

# COMMAND ----------

def analyze_liquid_clustering_opportunities(profiler_data: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Liquid Clusteringã«åŠ¹æœçš„ãªã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
    """
    import re
    
    clustering_analysis = {
        "recommended_tables": {},
        "filter_columns": [],
        "join_columns": [],
        "groupby_columns": [],
        "pushdown_filters": [],
        "data_skew_indicators": {},
        "performance_impact": {},
        "detailed_column_analysis": {},
        "summary": {}
    }
    
    # ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆã®è§£æï¼ˆå¼·åŒ–ç‰ˆï¼‰
    query_text = metrics.get('query_info', {}).get('query_text', '').upper()
    
    if query_text:
        # å¼·åŒ–ã•ã‚ŒãŸWHEREå¥ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå®Œå…¨ãªã‚¹ã‚­ãƒ¼ãƒ.ãƒ†ãƒ¼ãƒ–ãƒ«.ã‚«ãƒ©ãƒ å½¢å¼ã«å¯¾å¿œï¼‰
        where_patterns = [
            r'WHERE\s+([a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*)\s*[=<>!]',  # schema.table.column
            r'WHERE\s+([a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*)\s*[=<>!]',  # table.column
            r'WHERE\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*[=<>!]',  # column
            r'AND\s+([a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*)\s*[=<>!]',
            r'AND\s+([a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*)\s*[=<>!]',
            r'AND\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*[=<>!]',
            r'OR\s+([a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*)\s*[=<>!]',
            r'OR\s+([a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*)\s*[=<>!]',
            r'OR\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*[=<>!]'
        ]
        for pattern in where_patterns:
            matches = re.findall(pattern, query_text)
            clustering_analysis["filter_columns"].extend([col.strip() for col in matches])
        
        # å¼·åŒ–ã•ã‚ŒãŸJOINå¥ãƒ‘ã‚¿ãƒ¼ãƒ³
        join_patterns = [
            r'JOIN\s+[a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*\s+[a-zA-Z_][a-zA-Z0-9_]*\s*ON\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)\s*=\s*([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)',
            r'LEFT\s+JOIN\s+[a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*\s+[a-zA-Z_][a-zA-Z0-9_]*\s*ON\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)\s*=\s*([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)',
            r'INNER\s+JOIN\s+[a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*\s+[a-zA-Z_][a-zA-Z0-9_]*\s*ON\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)\s*=\s*([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)'
        ]
        for pattern in join_patterns:
            matches = re.findall(pattern, query_text)
            for match in matches:
                clustering_analysis["join_columns"].extend([col.strip() for col in match])
        
        # å¼·åŒ–ã•ã‚ŒãŸGROUP BYå¥ãƒ‘ã‚¿ãƒ¼ãƒ³
        groupby_pattern = r'GROUP\s+BY\s+((?:[a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*(?:\s*,\s*)?)+)'
        groupby_matches = re.findall(groupby_pattern, query_text)
        for match in groupby_matches:
            cols = [col.strip() for col in re.split(r'\s*,\s*', match) if col.strip()]
            clustering_analysis["groupby_columns"].extend(cols)
    
    # ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ã¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’åˆ†æï¼ˆå¼·åŒ–ç‰ˆï¼‰
    node_metrics = metrics.get('node_metrics', [])
    table_scan_nodes = []
    join_nodes = []
    shuffle_nodes = []
    filter_nodes = []
    
    for node in node_metrics:
        node_name = node.get('name', '').upper()
        node_tag = node.get('tag', '').upper()
        detailed_metrics = node.get('detailed_metrics', {})
        
        # ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®æŠ½å‡º
        for metric_key, metric_info in detailed_metrics.items():
            if any(filter_keyword in metric_key.upper() for filter_keyword in ['FILTER', 'PREDICATE', 'CONDITION']):
                filter_value = metric_info.get('label', '') or str(metric_info.get('value', ''))
                if filter_value:
                    clustering_analysis["pushdown_filters"].append({
                        "node_id": node.get('node_id', ''),
                        "node_name": node_name,
                        "filter_expression": filter_value,
                        "metric_key": metric_key
                    })
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®ç‰¹å®šï¼ˆè©³ç´°åˆ†æï¼‰
        if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN', 'PARQUET', 'DELTA']):
            table_scan_nodes.append(node)
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼æŒ‡æ¨™ã®è¨ˆç®—
            key_metrics = node.get('key_metrics', {})
            rows_num = key_metrics.get('rowsNum', 0)
            duration_ms = key_metrics.get('durationMs', 0)
            
            # å¼·åŒ–ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«åæŠ½å‡ºï¼ˆå®Œå…¨ã‚¹ã‚­ãƒ¼ãƒå¯¾å¿œï¼‰
            table_patterns = [
                r'([a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*)',  # schema.table.subtable
                r'([a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*)',  # schema.table
                r'([a-zA-Z_][a-zA-Z0-9_]*)'  # table
            ]
            
            table_name = None
            for pattern in table_patterns:
                table_match = re.search(pattern, node_name)
                if table_match:
                    table_name = table_match.group(1)
                    break
            
            if not table_name:
                table_name = f"table_{node.get('node_id', 'unknown')}"
            
            # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶æƒ…å ±ã‚’æŠ½å‡º
            filter_info = []
            column_references = []
            
            for metric_key, metric_info in detailed_metrics.items():
                label = metric_info.get('label', '')
                if label:
                    # ã‚«ãƒ©ãƒ å‚ç…§ã®æŠ½å‡º
                    column_matches = re.findall(r'([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)', label)
                    column_references.extend(column_matches)
                    
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã®æŠ½å‡º
                    if any(op in label for op in ['=', '<', '>', '<=', '>=', '!=', 'IN', 'LIKE']):
                        filter_info.append(label)
            
            clustering_analysis["data_skew_indicators"][table_name] = {
                "rows_scanned": rows_num,
                "scan_duration_ms": duration_ms,
                "avg_rows_per_ms": rows_num / max(duration_ms, 1),
                "node_name": node_name,
                "node_id": node.get('node_id', ''),
                "filter_conditions": filter_info,
                "column_references": list(set(column_references))
            }
            
            # ã‚«ãƒ©ãƒ å‚ç…§ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚«ãƒ©ãƒ ã«è¿½åŠ 
            clustering_analysis["filter_columns"].extend(column_references)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã®ç‰¹å®š
        elif any(keyword in node_name for keyword in ['FILTER']):
            filter_nodes.append(node)
        
        # JOINãƒãƒ¼ãƒ‰ã®ç‰¹å®š
        elif any(keyword in node_name for keyword in ['JOIN', 'HASH']):
            join_nodes.append(node)
            
            # JOINæ¡ä»¶ã®è©³ç´°æŠ½å‡º
            for metric_key, metric_info in detailed_metrics.items():
                label = metric_info.get('label', '')
                if label and '=' in label:
                    # JOINæ¡ä»¶ã‹ã‚‰ã‚«ãƒ©ãƒ ã‚’æŠ½å‡º
                    join_cols = re.findall(r'([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)', label)
                    clustering_analysis["join_columns"].extend(join_cols)
        
        # Shuffleãƒãƒ¼ãƒ‰ã®ç‰¹å®š
        elif any(keyword in node_name for keyword in ['SHUFFLE', 'EXCHANGE']):
            shuffle_nodes.append(node)
    
    # è©³ç´°ãªã‚«ãƒ©ãƒ åˆ†æã®å®Ÿè¡Œ
    all_columns = set()
    all_columns.update(clustering_analysis["filter_columns"])
    all_columns.update(clustering_analysis["join_columns"])
    all_columns.update(clustering_analysis["groupby_columns"])
    
    # ã‚«ãƒ©ãƒ åˆ¥ã®è©³ç´°åˆ†æ
    for column in all_columns:
        column_analysis = {
            "filter_usage_count": clustering_analysis["filter_columns"].count(column),
            "join_usage_count": clustering_analysis["join_columns"].count(column),
            "groupby_usage_count": clustering_analysis["groupby_columns"].count(column),
            "total_usage": 0,
            "usage_contexts": [],
            "associated_tables": set(),
            "performance_impact": "low"
        }
        
        # ä½¿ç”¨å›æ•°ã®åˆè¨ˆè¨ˆç®—
        column_analysis["total_usage"] = (
            column_analysis["filter_usage_count"] * 3 +
            column_analysis["join_usage_count"] * 2 +
            column_analysis["groupby_usage_count"] * 1
        )
        
        # ä½¿ç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è¨˜éŒ²
        if column_analysis["filter_usage_count"] > 0:
            column_analysis["usage_contexts"].append("WHERE/Filteræ¡ä»¶")
        if column_analysis["join_usage_count"] > 0:
            column_analysis["usage_contexts"].append("JOINæ¡ä»¶")
        if column_analysis["groupby_usage_count"] > 0:
            column_analysis["usage_contexts"].append("GROUP BY")
        
        # é–¢é€£ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç‰¹å®š
        column_parts = column.split('.')
        if len(column_parts) >= 2:
            if len(column_parts) == 3:  # schema.table.column
                table_name = f"{column_parts[0]}.{column_parts[1]}"
                column_analysis["associated_tables"].add(table_name)
            else:  # table.column
                column_analysis["associated_tables"].add(column_parts[0])
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å½±éŸ¿åº¦ã®è©•ä¾¡
        if column_analysis["total_usage"] >= 6:
            column_analysis["performance_impact"] = "high"
        elif column_analysis["total_usage"] >= 3:
            column_analysis["performance_impact"] = "medium"
        
        clustering_analysis["detailed_column_analysis"][column] = column_analysis
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æ¯ã®æ¨å¥¨äº‹é …ï¼ˆå¼·åŒ–ç‰ˆï¼‰
    for table_name, skew_info in clustering_analysis["data_skew_indicators"].items():
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã«é–¢é€£ã™ã‚‹ã‚«ãƒ©ãƒ ã®ç‰¹å®šï¼ˆã‚ˆã‚ŠæŸ”è»Ÿãªãƒãƒƒãƒãƒ³ã‚°ï¼‰
        table_columns = []
        table_parts = table_name.split('.')
        
        for col in all_columns:
            col_parts = col.split('.')
            # å®Œå…¨ä¸€è‡´ã¾ãŸã¯éƒ¨åˆ†ä¸€è‡´ã§ãƒ†ãƒ¼ãƒ–ãƒ«é–¢é€£ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
            if len(col_parts) >= 2:
                if len(col_parts) == 3 and len(table_parts) >= 2:  # schema.table.column
                    if f"{col_parts[0]}.{col_parts[1]}" == table_name:
                        table_columns.append(col)
                elif len(col_parts) == 2:  # table.column
                    if col_parts[0] in table_name or table_name.endswith(col_parts[0]):
                        table_columns.append(col)
            else:
                # ã‚«ãƒ©ãƒ åã®ã¿ã®å ´åˆã€ãƒãƒ¼ãƒ‰ã®ã‚«ãƒ©ãƒ å‚ç…§ã¨ç…§åˆ
                if col in skew_info.get("column_references", []):
                    table_columns.append(col)
        
        # ãƒãƒ¼ãƒ‰ã‹ã‚‰ç›´æ¥æŠ½å‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ å‚ç…§ã‚‚è¿½åŠ 
        table_columns.extend(skew_info.get("column_references", []))
        table_columns = list(set(table_columns))  # é‡è¤‡é™¤å»
        
        # ã‚«ãƒ©ãƒ ã®é‡è¦åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆè©³ç´°ç‰ˆï¼‰
        column_scores = {}
        for col in table_columns:
            clean_col = col.split('.')[-1] if '.' in col else col
            
            # è©³ç´°åˆ†æã‹ã‚‰ã‚¹ã‚³ã‚¢ã‚’å–å¾—
            if col in clustering_analysis["detailed_column_analysis"]:
                analysis = clustering_analysis["detailed_column_analysis"][col]
                score = analysis["total_usage"]
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨ˆç®—
                score = (clustering_analysis["filter_columns"].count(col) * 3 +
                        clustering_analysis["join_columns"].count(col) * 2 +
                        clustering_analysis["groupby_columns"].count(col) * 1)
            
            if score > 0:
                column_scores[clean_col] = score
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶æƒ…å ±ã‚‚å«ã‚ã‚‹
        filter_conditions = skew_info.get("filter_conditions", [])
        
        # ä¸Šä½ã‚«ãƒ©ãƒ ã‚’æ¨å¥¨
        if column_scores:
            sorted_columns = sorted(column_scores.items(), key=lambda x: x[1], reverse=True)
            recommended_cols = [col for col, score in sorted_columns[:4]]  # æœ€å¤§4ã‚«ãƒ©ãƒ 
            
            clustering_analysis["recommended_tables"][table_name] = {
                "clustering_columns": recommended_cols,
                "column_scores": column_scores,
                "scan_performance": {
                    "rows_scanned": skew_info["rows_scanned"],
                    "scan_duration_ms": skew_info["scan_duration_ms"],
                    "efficiency_score": skew_info["avg_rows_per_ms"]
                },
                "node_details": {
                    "node_id": skew_info.get("node_id", ""),
                    "node_name": skew_info.get("node_name", ""),
                    "filter_conditions": filter_conditions,
                    "column_references": skew_info.get("column_references", [])
                }
            }
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã®è¦‹è¾¼ã¿è©•ä¾¡
    total_scan_time = sum(info["scan_duration_ms"] for info in clustering_analysis["data_skew_indicators"].values())
    total_shuffle_time = 0
    
    for node in shuffle_nodes:
        total_shuffle_time += node.get('key_metrics', {}).get('durationMs', 0)
    
    clustering_analysis["performance_impact"] = {
        "total_scan_time_ms": total_scan_time,
        "total_shuffle_time_ms": total_shuffle_time,
        "potential_scan_improvement": "30-70%" if total_scan_time > 10000 else "10-30%",
        "potential_shuffle_reduction": "20-50%" if total_shuffle_time > 5000 else "5-20%",
        "estimated_overall_improvement": "25-60%" if (total_scan_time + total_shuffle_time) > 15000 else "10-25%"
    }
    
    # ã‚µãƒãƒªãƒ¼æƒ…å ±
    clustering_analysis["summary"] = {
        "tables_identified": len(clustering_analysis["recommended_tables"]),
        "total_filter_columns": len(set(clustering_analysis["filter_columns"])),
        "total_join_columns": len(set(clustering_analysis["join_columns"])),
        "total_groupby_columns": len(set(clustering_analysis["groupby_columns"])),
        "high_impact_tables": len([t for t, info in clustering_analysis["recommended_tables"].items() 
                                 if info["scan_performance"]["scan_duration_ms"] > 5000])
    }
    
    return clustering_analysis

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: analyze_liquid_clustering_opportunities")

# COMMAND ----------

def analyze_bottlenecks_with_claude(metrics: Dict[str, Any]) -> str:
    """
    Databricks Claude 3.7 Sonnetã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’è¡Œã†
    """
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¦ç´„ã®æº–å‚™
    analysis_prompt = f"""
ã‚ãªãŸã¯Databricksã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åˆ†æã—ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®šã—ã¦æ”¹å–„æ¡ˆã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚

ã€åˆ†æå¯¾è±¡ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€‘

ã‚¯ã‚¨ãƒªåŸºæœ¬æƒ…å ±:
- ã‚¯ã‚¨ãƒªID: {metrics['query_info'].get('query_id', 'N/A')}
- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {metrics['query_info'].get('status', 'N/A')}
- å®Ÿè¡Œãƒ¦ãƒ¼ã‚¶ãƒ¼: {metrics['query_info'].get('user', 'N/A')}

å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:
- ç·å®Ÿè¡Œæ™‚é–“: {metrics['overall_metrics'].get('total_time_ms', 0):,} ms ({metrics['overall_metrics'].get('total_time_ms', 0)/1000:.2f} sec)
- ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“: {metrics['overall_metrics'].get('compilation_time_ms', 0):,} ms
- å®Ÿè¡Œæ™‚é–“: {metrics['overall_metrics'].get('execution_time_ms', 0):,} ms
- èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿é‡: {metrics['overall_metrics'].get('read_bytes', 0):,} bytes ({metrics['overall_metrics'].get('read_bytes', 0)/1024/1024/1024:.2f} GB)
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆé‡: {metrics['overall_metrics'].get('read_cache_bytes', 0):,} bytes ({metrics['overall_metrics'].get('read_cache_bytes', 0)/1024/1024/1024:.2f} GB)
- èª­ã¿è¾¼ã¿è¡Œæ•°: {metrics['overall_metrics'].get('rows_read_count', 0):,} è¡Œ
- å‡ºåŠ›è¡Œæ•°: {metrics['overall_metrics'].get('rows_produced_count', 0):,} è¡Œ
- ã‚¹ãƒ”ãƒ«ã‚µã‚¤ã‚º: {metrics['overall_metrics'].get('spill_to_disk_bytes', 0):,} bytes
- Photonå®Ÿè¡Œæ™‚é–“: {metrics['overall_metrics'].get('photon_total_time_ms', 0):,} ms

ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™:
- ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“æ¯”ç‡: {metrics['bottleneck_indicators'].get('compilation_ratio', 0):.3f} ({metrics['bottleneck_indicators'].get('compilation_ratio', 0)*100:.1f}%)
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {metrics['bottleneck_indicators'].get('cache_hit_ratio', 0):.3f} ({metrics['bottleneck_indicators'].get('cache_hit_ratio', 0)*100:.1f}%)
- ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§: {metrics['bottleneck_indicators'].get('data_selectivity', 0):.3f} ({metrics['bottleneck_indicators'].get('data_selectivity', 0)*100:.1f}%)
- Photonä½¿ç”¨ç‡: {metrics['bottleneck_indicators'].get('photon_ratio', 0):.3f} ({metrics['bottleneck_indicators'].get('photon_ratio', 0)*100:.1f}%)
- ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ: {'ã‚ã‚Š' if metrics['bottleneck_indicators'].get('has_spill', False) else 'ãªã—'}
- æœ€ã‚‚é…ã„ã‚¹ãƒ†ãƒ¼ã‚¸ID: {metrics['bottleneck_indicators'].get('slowest_stage_id', 'N/A')}
- æœ€é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨ãƒãƒ¼ãƒ‰: {metrics['bottleneck_indicators'].get('highest_memory_node_name', 'N/A')}
- æœ€é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {metrics['bottleneck_indicators'].get('highest_memory_bytes', 0)/1024/1024:.2f} MB

ã‚¹ãƒ†ãƒ¼ã‚¸è©³ç´°:
{chr(10).join([f"- ã‚¹ãƒ†ãƒ¼ã‚¸{s['stage_id']}: {s['duration_ms']:,}ms, ã‚¿ã‚¹ã‚¯æ•°:{s['num_tasks']}, å®Œäº†:{s['num_complete_tasks']}, å¤±æ•—:{s['num_failed_tasks']}" for s in metrics['stage_metrics'][:10]])}

ä¸»è¦ãƒãƒ¼ãƒ‰è©³ç´°:
{chr(10).join([f"- {n['name']} (ID:{n['node_id']}): è¡Œæ•°={n['key_metrics'].get('rowsNum', 0):,}, æ™‚é–“={n['key_metrics'].get('durationMs', 0):,}ms, ãƒ¡ãƒ¢ãƒª={n['key_metrics'].get('peakMemoryBytes', 0)/1024/1024:.2f}MB" for n in metrics['node_metrics'][:15]])}

ã€Liquid Clusteringæ¨å¥¨åˆ†æã€‘
å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {metrics['liquid_clustering_analysis']['summary'].get('tables_identified', 0)}
é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {metrics['liquid_clustering_analysis']['summary'].get('high_impact_tables', 0)}

ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚«ãƒ©ãƒ :
{chr(10).join([f"- {table}: {', '.join(info['clustering_columns'])}" for table, info in metrics['liquid_clustering_analysis']['recommended_tables'].items()])}

ã‚«ãƒ©ãƒ ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³:
- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚«ãƒ©ãƒ : {', '.join(list(set(metrics['liquid_clustering_analysis']['filter_columns']))[:10])}
- JOINã‚«ãƒ©ãƒ : {', '.join(list(set(metrics['liquid_clustering_analysis']['join_columns']))[:10])}
- GROUP BYã‚«ãƒ©ãƒ : {', '.join(list(set(metrics['liquid_clustering_analysis']['groupby_columns']))[:10])}

é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚«ãƒ©ãƒ è©³ç´°:
{chr(10).join([f"- {col}: ã‚¹ã‚³ã‚¢={analysis['total_usage']}, ä½¿ç”¨ç®‡æ‰€=[{', '.join(analysis['usage_contexts'])}], ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼={analysis['filter_usage_count']}å›, JOIN={analysis['join_usage_count']}å›, GROUP BY={analysis['groupby_usage_count']}å›" for col, analysis in metrics['liquid_clustering_analysis']['detailed_column_analysis'].items() if analysis.get('performance_impact') == 'high'][:10])}

ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è©³ç´°:
{chr(10).join([f"- ãƒãƒ¼ãƒ‰: {filter_info['node_name'][:50]} | ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {filter_info['filter_expression'][:80]}" for filter_info in metrics['liquid_clustering_analysis']['pushdown_filters'][:5]])}

ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šè¦‹è¾¼ã¿:
- ã‚¹ã‚­ãƒ£ãƒ³æ”¹å–„: {metrics['liquid_clustering_analysis']['performance_impact'].get('potential_scan_improvement', 'N/A')}
- Shuffleå‰Šæ¸›: {metrics['liquid_clustering_analysis']['performance_impact'].get('potential_shuffle_reduction', 'N/A')}
- å…¨ä½“æ”¹å–„: {metrics['liquid_clustering_analysis']['performance_impact'].get('estimated_overall_improvement', 'N/A')}

ã€åˆ†æã—ã¦æ¬²ã—ã„å†…å®¹ã€‘
1. ä¸»è¦ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ç‰¹å®šã¨åŸå› åˆ†æ
2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®å„ªå…ˆé †ä½ä»˜ã‘
3. å…·ä½“çš„ãªæœ€é©åŒ–æ¡ˆã®æç¤ºï¼ˆSQLæ”¹å–„ã€è¨­å®šå¤‰æ›´ã€ã‚¤ãƒ³ãƒ•ãƒ©æœ€é©åŒ–ãªã©ï¼‰
4. **Liquid Clusteringå®Ÿè£…ã®å…·ä½“çš„æ¨å¥¨äº‹é …ã¨æ‰‹é †**
5. **å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚«ãƒ©ãƒ é¸å®šç†ç”±ã¨æœŸå¾…åŠ¹æœ**
6. **Liquid Clusteringå°å…¥æ™‚ã®æ³¨æ„ç‚¹ã¨å®Ÿè£…é †åº**
7. äºˆæƒ³ã•ã‚Œã‚‹æ”¹å–„åŠ¹æœ
8. Photonæœ€é©åŒ–ã®æ¨å¥¨äº‹é …
9. é‡è¦ãªæ³¨æ„ç‚¹ã‚„æ¨å¥¨äº‹é …

ç‰¹ã«ã€Liquid Clusteringã®å®Ÿè£…ã«ã¤ã„ã¦ã¯è©³ç´°ãªæ‰‹é †ã¨æœŸå¾…åŠ¹æœã‚’å«ã‚ã¦ã€æ—¥æœ¬èªã§è©³ç´°ãªåˆ†æçµæœã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
"""
    
    try:
        # Databricks Model Serving APIã‚’ä½¿ç”¨
        try:
            token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
        except Exception:
            # ä»£æ›¿æ‰‹æ®µã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
            import os
            token = os.environ.get('DATABRICKS_TOKEN')
            if not token:
                return "âŒ Databricksãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç’°å¢ƒå¤‰æ•°DATABRICKS_TOKENã‚’è¨­å®šã™ã‚‹ã‹ã€dbutils.secrets.get()ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
        
        try:
            workspace_url = spark.conf.get("spark.databricks.workspaceUrl")
        except Exception:
            workspace_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get("browserHostName").get()
        
        endpoint_url = f"https://{workspace_url}/serving-endpoints/databricks-claude-3-7-sonnet/invocations"
        
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "messages": [
                {
                    "role": "user",
                    "content": analysis_prompt
                }
            ],
            "max_tokens": 4000,
            "temperature": 0.1
        }
        
        print("ğŸ¤– Databricks Claude 3.7 Sonnetã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«åˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ä¸­...")
        response = requests.post(endpoint_url, headers=headers, json=payload, timeout=60)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result.get('choices', [{}])[0].get('message', {}).get('content', '')
            print("âœ… ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
            return analysis_text
        else:
            error_msg = f"APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}\nãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text}"
            print(f"âŒ {error_msg}")
            return error_msg
            
    except Exception as e:
        error_msg = f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
        print(f"âŒ {error_msg}")
        return error_msg

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: analyze_bottlenecks_with_claude")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®å®Ÿè¡Œ
# MAGIC 
# MAGIC ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œã—ã¦ã€SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
# MAGIC 
# MAGIC ### ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¨­å®šä¾‹:
# MAGIC 
# MAGIC ```python
# MAGIC # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
# MAGIC JSON_FILE_PATH = 'simple0.json'
# MAGIC 
# MAGIC # FileStore ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«
# MAGIC JSON_FILE_PATH = '/FileStore/shared_uploads/username/profiler.json'
# MAGIC 
# MAGIC # DBFS ãƒ•ã‚¡ã‚¤ãƒ«
# MAGIC JSON_FILE_PATH = '/dbfs/FileStore/shared_uploads/username/profiler.json'
# MAGIC JSON_FILE_PATH = 'dbfs:/FileStore/shared_uploads/username/profiler.json'
# MAGIC ```

# COMMAND ----------

# ğŸ”§ è¨­å®š: åˆ†æå¯¾è±¡ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š
JSON_FILE_PATH = 'simple0.json'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«

# ä»¥ä¸‹ã‹ã‚‰é¸æŠã—ã¦å¤‰æ›´ã—ã¦ãã ã•ã„:
# JSON_FILE_PATH = '/FileStore/shared_uploads/your_username/profiler_log.json'
# JSON_FILE_PATH = '/dbfs/FileStore/shared_uploads/your_username/profiler_log.json'
# JSON_FILE_PATH = 'dbfs:/FileStore/shared_uploads/your_username/profiler_log.json'

print("=" * 80)
print("ğŸš€ Databricks SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆ†æãƒ„ãƒ¼ãƒ«")
print("=" * 80)
print(f"ğŸ“ åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {JSON_FILE_PATH}")
print()

# SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
profiler_data = load_profiler_json(JSON_FILE_PATH)
if not profiler_data:
    print("âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    print("âš ï¸ å‡¦ç†ã‚’åœæ­¢ã—ã¾ã™ã€‚")
    # dbutils.notebook.exit("File loading failed")  # å®‰å…¨ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    raise RuntimeError("JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
print()

# COMMAND ----------

# ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º
extracted_metrics = extract_performance_metrics(profiler_data)
print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")

# æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¦‚è¦è¡¨ç¤º
print("\n" + "=" * 50)
print("ğŸ“ˆ æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¦‚è¦")
print("=" * 50)

query_info = extracted_metrics['query_info']
overall_metrics = extracted_metrics['overall_metrics']
bottleneck_indicators = extracted_metrics['bottleneck_indicators']

print(f"ğŸ†” ã‚¯ã‚¨ãƒªID: {query_info['query_id']}")
print(f"ğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {query_info['status']}")
print(f"ğŸ‘¤ å®Ÿè¡Œãƒ¦ãƒ¼ã‚¶ãƒ¼: {query_info['user']}")
print(f"â±ï¸ å®Ÿè¡Œæ™‚é–“: {overall_metrics['total_time_ms']:,} ms ({overall_metrics['total_time_ms']/1000:.2f} sec)")
print(f"ğŸ’¾ èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿: {overall_metrics['read_bytes']/1024/1024/1024:.2f} GB")
print(f"ğŸ“ˆ å‡ºåŠ›è¡Œæ•°: {overall_metrics['rows_produced_count']:,} è¡Œ")
print(f"ğŸ“‰ èª­ã¿è¾¼ã¿è¡Œæ•°: {overall_metrics['rows_read_count']:,} è¡Œ")
print(f"ğŸ¯ ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§: {bottleneck_indicators.get('data_selectivity', 0):.4f} ({bottleneck_indicators.get('data_selectivity', 0)*100:.2f}%)")
print(f"ğŸ”§ ã‚¹ãƒ†ãƒ¼ã‚¸æ•°: {len(extracted_metrics['stage_metrics'])}")
print(f"ğŸ—ï¸ ãƒãƒ¼ãƒ‰æ•°: {len(extracted_metrics['node_metrics'])}")

# Liquid Clusteringåˆ†æçµæœã®è¡¨ç¤º
liquid_analysis = extracted_metrics['liquid_clustering_analysis']
liquid_summary = liquid_analysis.get('summary', {})
print(f"ğŸ—‚ï¸ Liquid Clusteringå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {liquid_summary.get('tables_identified', 0)}")
print(f"ğŸ“Š é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {liquid_summary.get('high_impact_tables', 0)}")

# COMMAND ----------

# ğŸ“‹ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è©³ç´°è¡¨ç¤º
print("\n" + "=" * 50)
print("ğŸ” ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™è©³ç´°")
print("=" * 50)

for key, value in bottleneck_indicators.items():
    if 'ratio' in key:
        emoji = "ğŸ“Š" if value < 0.1 else "âš ï¸" if value < 0.3 else "ğŸš¨"
        print(f"{emoji} {key}: {value:.3f} ({value*100:.1f}%)")
    elif 'bytes' in key and key != 'has_spill':
        if value > 0:
            emoji = "ğŸ’¾" if value < 1024*1024*1024 else "âš ï¸"  # 1GBæœªæº€ã¯æ™®é€šã€ä»¥ä¸Šã¯æ³¨æ„
            print(f"{emoji} {key}: {value:,} bytes ({value/1024/1024:.2f} MB)")
    elif key == 'has_spill':
        emoji = "âŒ" if not value else "âš ï¸"
        print(f"{emoji} {key}: {'ã‚ã‚Š' if value else 'ãªã—'}")
    elif 'duration' in key:
        emoji = "â±ï¸"
        print(f"{emoji} {key}: {value:,} ms ({value/1000:.2f} sec)")
    else:
        emoji = "â„¹ï¸"
        print(f"{emoji} {key}: {value}")

print()

# COMMAND ----------

# ğŸ’¾ æŠ½å‡ºã—ãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
output_path = 'extracted_metrics.json'
with open(output_path, 'w', encoding='utf-8') as file:
    json.dump(extracted_metrics, file, indent=2, ensure_ascii=False)
print(f"âœ… æŠ½å‡ºãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")

# SparkDataFrameã¨ã—ã¦ã‚‚è¡¨ç¤º
if extracted_metrics['stage_metrics']:
    print("\nğŸ­ ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (DataFrame)")
    print("=" * 40)
    try:
        stage_df = spark.createDataFrame(extracted_metrics['stage_metrics'])
        stage_df.show(truncate=False)
    except Exception as e:
        print(f"âš ï¸ SparkDataFrameè¡¨ç¤ºã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
        # ä»£æ›¿ã¨ã—ã¦Pandasã§è¡¨ç¤º
        import pandas as pd
        stage_pd_df = pd.DataFrame(extracted_metrics['stage_metrics'])
        print(stage_pd_df.to_string(index=False))

# ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¦‚è¦
print(f"\nğŸ—ï¸ ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¦‚è¦ï¼ˆä¸Šä½10ä»¶ï¼‰")
print("=" * 50)
for i, node in enumerate(extracted_metrics['node_metrics'][:10]):
    rows_num = node['key_metrics'].get('rowsNum', 0)
    duration_ms = node['key_metrics'].get('durationMs', 0)
    memory_mb = node['key_metrics'].get('peakMemoryBytes', 0) / 1024 / 1024
    
    # æ™‚é–“ã¨ãƒ¡ãƒ¢ãƒªã«åŸºã¥ã„ã¦ã‚¢ã‚¤ã‚³ãƒ³ã‚’é¸æŠ
    time_icon = "ğŸŸ¢" if duration_ms < 1000 else "ğŸŸ¡" if duration_ms < 10000 else "ğŸ”´"
    memory_icon = "ğŸ’š" if memory_mb < 100 else "ğŸ’›" if memory_mb < 1000 else "â¤ï¸"
    
    print(f"{i+1:2d}. {time_icon}{memory_icon} {node['name'][:40]:40} | è¡Œæ•°: {rows_num:>8,} | æ™‚é–“: {duration_ms:>6,}ms | ãƒ¡ãƒ¢ãƒª: {memory_mb:>6.1f}MB")

print()

# COMMAND ----------

# ğŸ—‚ï¸ Liquid Clusteringåˆ†æçµæœã®è©³ç´°è¡¨ç¤º
print("\n" + "=" * 50)
print("ğŸ—‚ï¸ Liquid Clusteringæ¨å¥¨åˆ†æ")
print("=" * 50)

liquid_analysis = extracted_metrics['liquid_clustering_analysis']

# æ¨å¥¨ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
recommended_tables = liquid_analysis.get('recommended_tables', {})
if recommended_tables:
    print("\nğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚«ãƒ©ãƒ :")
    for table_name, table_info in recommended_tables.items():
        clustering_cols = table_info.get('clustering_columns', [])
        scan_perf = table_info.get('scan_performance', {})
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«åã®è¡¨ç¤º
        print(f"\nğŸ“Š ãƒ†ãƒ¼ãƒ–ãƒ«: {table_name}")
        print(f"   ğŸ¯ æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚«ãƒ©ãƒ : {', '.join(clustering_cols)}")
        print(f"   ğŸ“ˆ ã‚¹ã‚­ãƒ£ãƒ³è¡Œæ•°: {scan_perf.get('rows_scanned', 0):,} è¡Œ")
        print(f"   â±ï¸ ã‚¹ã‚­ãƒ£ãƒ³æ™‚é–“: {scan_perf.get('scan_duration_ms', 0):,} ms")
        print(f"   ğŸš€ ã‚¹ã‚­ãƒ£ãƒ³åŠ¹ç‡: {scan_perf.get('efficiency_score', 0):.2f} è¡Œ/ms")
        
        # ã‚«ãƒ©ãƒ ã‚¹ã‚³ã‚¢è©³ç´°
        column_scores = table_info.get('column_scores', {})
        if column_scores:
            sorted_scores = sorted(column_scores.items(), key=lambda x: x[1], reverse=True)
            print(f"   ğŸ“Š ã‚«ãƒ©ãƒ é‡è¦åº¦: {', '.join([f'{col}({score})' for col, score in sorted_scores[:3]])}")

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å½±éŸ¿åˆ†æ
performance_impact = liquid_analysis.get('performance_impact', {})
print(f"\nğŸ”„ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šè¦‹è¾¼ã¿:")
print(f"   ğŸ“ˆ ã‚¹ã‚­ãƒ£ãƒ³æ”¹å–„: {performance_impact.get('potential_scan_improvement', 'N/A')}")
print(f"   ğŸ”€ Shuffleå‰Šæ¸›: {performance_impact.get('potential_shuffle_reduction', 'N/A')}")
print(f"   ğŸ† å…¨ä½“æ”¹å–„: {performance_impact.get('estimated_overall_improvement', 'N/A')}")

# ã‚«ãƒ©ãƒ ä½¿ç”¨çµ±è¨ˆï¼ˆè©³ç´°ç‰ˆï¼‰
filter_cols = set(liquid_analysis.get('filter_columns', []))
join_cols = set(liquid_analysis.get('join_columns', []))
groupby_cols = set(liquid_analysis.get('groupby_columns', []))
detailed_column_analysis = liquid_analysis.get('detailed_column_analysis', {})

if filter_cols or join_cols or groupby_cols:
    print(f"\nğŸ” ã‚«ãƒ©ãƒ ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³:")
    if filter_cols:
        print(f"   ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚«ãƒ©ãƒ  ({len(filter_cols)}å€‹): {', '.join(list(filter_cols)[:5])}")
    if join_cols:
        print(f"   ğŸ”— JOINã‚«ãƒ©ãƒ  ({len(join_cols)}å€‹): {', '.join(list(join_cols)[:5])}")
    if groupby_cols:
        print(f"   ğŸ“Š GROUP BYã‚«ãƒ©ãƒ  ({len(groupby_cols)}å€‹): {', '.join(list(groupby_cols)[:5])}")

# é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚«ãƒ©ãƒ ã®è©³ç´°è¡¨ç¤º
high_impact_columns = {col: analysis for col, analysis in detailed_column_analysis.items() 
                      if analysis.get('performance_impact') == 'high'}

if high_impact_columns:
    print(f"\nâ­ é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚«ãƒ©ãƒ è©³ç´°:")
    for col, analysis in list(high_impact_columns.items())[:5]:
        usage_contexts = ', '.join(analysis.get('usage_contexts', []))
        total_usage = analysis.get('total_usage', 0)
        print(f"   ğŸ¯ {col}")
        print(f"      ğŸ“ˆ é‡è¦åº¦ã‚¹ã‚³ã‚¢: {total_usage} | ä½¿ç”¨ç®‡æ‰€: {usage_contexts}")
        print(f"      ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼:{analysis.get('filter_usage_count', 0)} | JOIN:{analysis.get('join_usage_count', 0)} | GROUP BY:{analysis.get('groupby_usage_count', 0)}")

# ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æƒ…å ±
pushdown_filters = liquid_analysis.get('pushdown_filters', [])
if pushdown_filters:
    print(f"\nğŸ” ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ ({len(pushdown_filters)}ä»¶):")
    for i, filter_info in enumerate(pushdown_filters[:3]):
        print(f"   {i+1}. ãƒãƒ¼ãƒ‰: {filter_info.get('node_name', '')[:30]}")
        print(f"      ğŸ“‹ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {filter_info.get('filter_expression', '')[:60]}")
        print(f"      ğŸ”§ ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {filter_info.get('metric_key', '')}")

# SQLå®Ÿè£…ä¾‹
if recommended_tables:
    print(f"\nğŸ’¡ å®Ÿè£…ä¾‹:")
    for table_name, table_info in list(recommended_tables.items())[:2]:  # ä¸Šä½2ãƒ†ãƒ¼ãƒ–ãƒ«
        clustering_cols = table_info.get('clustering_columns', [])
        if clustering_cols:
            cluster_by_clause = ', '.join(clustering_cols)
            print(f"   ALTER TABLE {table_name} CLUSTER BY ({cluster_by_clause});")

print()

# COMMAND ----------

# ğŸ¤– Databricks Claude 3.7 Sonnetã‚’ä½¿ç”¨ã—ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ
print("ğŸ¤– Claude 3.7 Sonnetã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
print("âš ï¸  Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ 'databricks-claude-3-7-sonnet' ãŒå¿…è¦ã§ã™")
print()

analysis_result = analyze_bottlenecks_with_claude(extracted_metrics)

# COMMAND ----------

# ğŸ“Š åˆ†æçµæœã®è¡¨ç¤º
print("\n" + "=" * 80)
print("ğŸ¯ ã€Databricks Claude 3.7 Sonnet ã«ã‚ˆã‚‹ SQLãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã€‘")
print("=" * 80)
print()
print(analysis_result)
print()
print("=" * 80)

# COMMAND ----------

# ğŸ’¾ åˆ†æçµæœã®ä¿å­˜ã¨å®Œäº†ã‚µãƒãƒªãƒ¼
result_output_path = 'bottleneck_analysis_result.txt'
with open(result_output_path, 'w', encoding='utf-8') as file:
    file.write("Databricks SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ\n")
    file.write("=" * 60 + "\n\n")
    file.write(f"ã‚¯ã‚¨ãƒªID: {extracted_metrics['query_info']['query_id']}\n")
    file.write(f"åˆ†ææ—¥æ™‚: {pd.Timestamp.now()}\n")
    file.write(f"å®Ÿè¡Œæ™‚é–“: {extracted_metrics['overall_metrics']['total_time_ms']:,} ms\n")
    file.write("=" * 60 + "\n\n")
    file.write(analysis_result)
print(f"âœ… åˆ†æçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {result_output_path}")

# æœ€çµ‚çš„ãªã‚µãƒãƒªãƒ¼
print("\n" + "ğŸ‰" * 20)
print("ğŸ ã€å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼ã€‘")
print("ğŸ‰" * 20)
print("âœ… SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºå®Œäº† (extracted_metrics.json)")
print("âœ… Databricks Claude 3.7 Sonnetã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Œäº†")
print("âœ… åˆ†æçµæœä¿å­˜å®Œäº† (bottleneck_analysis_result.txt)")
print()
print("ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
print(f"   ğŸ“„ {output_path}")
print(f"   ğŸ“„ {result_output_path}")
print()
print("ğŸš€ åˆ†æå®Œäº†ï¼çµæœã‚’ç¢ºèªã—ã¦ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã«ãŠå½¹ç«‹ã¦ãã ã•ã„ã€‚")
print("ğŸ‰" * 20)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“š è¿½åŠ ã®ä½¿ç”¨æ–¹æ³•ã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
# MAGIC 
# MAGIC ### ğŸ”§ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ–¹æ³•
# MAGIC 
# MAGIC #### æ–¹æ³• 1: Databricks UI ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# MAGIC 1. **Data** > **Create Table** ã‚’ã‚¯ãƒªãƒƒã‚¯
# MAGIC 2. **Upload File** ã‚’é¸æŠ
# MAGIC 3. SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—
# MAGIC 4. ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†å¾Œã€ãƒ‘ã‚¹ã‚’ã‚³ãƒ”ãƒ¼
# MAGIC 5. ä¸Šè¨˜ã® `JSON_FILE_PATH` ã«è¨­å®š
# MAGIC 
# MAGIC #### æ–¹æ³• 2: dbutils ã‚’ä½¿ç”¨
# MAGIC ```python
# MAGIC # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’FileStoreã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# MAGIC dbutils.fs.cp("file:/local/path/profiler.json", "dbfs:/FileStore/profiler.json")
# MAGIC 
# MAGIC # å¤–éƒ¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰ã®ã‚³ãƒ”ãƒ¼
# MAGIC dbutils.fs.cp("s3a://bucket/profiler.json", "dbfs:/FileStore/profiler.json")
# MAGIC ```
# MAGIC 
# MAGIC ### ğŸ›ï¸ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆ
# MAGIC 
# MAGIC - **ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º**: `extract_performance_metrics` é–¢æ•°å†…ã®é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
# MAGIC - **åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: `analyze_bottlenecks_with_claude` é–¢æ•°å†…ã®åˆ†ææŒ‡ç¤º
# MAGIC - **è¡¨ç¤ºå½¢å¼**: emoji ã¨å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®èª¿æ•´
# MAGIC 
# MAGIC ### ğŸ” ã‚¨ãƒ©ãƒ¼å¯¾å‡¦æ–¹æ³•
# MAGIC 
# MAGIC 1. **Claude ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼**: Model Serving ã§ `databricks-claude-3-7-sonnet` ãŒç¨¼åƒä¸­ã‹ç¢ºèª
# MAGIC 2. **ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼**: `dbutils.fs.ls("/FileStore/")` ã§ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ã‚’ç¢ºèª
# MAGIC 3. **ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼**: å¤§ããªJSONãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ã‚¯ãƒ©ã‚¹ã‚¿ã®ãƒ¡ãƒ¢ãƒªè¨­å®šã‚’ç¢ºèª
# MAGIC 
# MAGIC ### ğŸ’¡ é«˜åº¦ãªä½¿ç”¨ä¾‹
# MAGIC 
# MAGIC ```python
# MAGIC # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ‹¬åˆ†æ
# MAGIC profiler_files = dbutils.fs.ls("/FileStore/profiler_logs/")
# MAGIC for file_info in profiler_files:
# MAGIC     if file_info.path.endswith('.json'):
# MAGIC         profiler_data = load_profiler_json(file_info.path)
# MAGIC         metrics = extract_performance_metrics(profiler_data)
# MAGIC         # åˆ†æå‡¦ç†...
# MAGIC ```

# COMMAND ----------

# MAGIC %md
# MAGIC ---
# MAGIC 
# MAGIC ## ğŸ¯ ã“ã®Notebookã®ä½¿ç”¨æ–¹æ³•
# MAGIC 
# MAGIC 1. **ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®š**: Model Serving ã§ `databricks-claude-3-7-sonnet` ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆ
# MAGIC 2. **ãƒ•ã‚¡ã‚¤ãƒ«æº–å‚™**: SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’FileStoreã¾ãŸã¯DBFSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# MAGIC 3. **ãƒ‘ã‚¹è¨­å®š**: ã‚»ãƒ«8ã§ `JSON_FILE_PATH` ã‚’å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›´
# MAGIC 4. **å®Ÿè¡Œ**: ã€ŒRun Allã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã¾ãŸã¯å„ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ
# MAGIC 5. **çµæœç¢ºèª**: æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨AIåˆ†æçµæœã‚’ç¢ºèª
# MAGIC 
# MAGIC **ğŸ“§ ã‚µãƒãƒ¼ãƒˆ**: å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨Databricksç’°å¢ƒæƒ…å ±ã‚’ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚