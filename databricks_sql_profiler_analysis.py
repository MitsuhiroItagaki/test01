# Databricks notebook source
# MAGIC %md
# MAGIC # Databricks SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆ†æãƒ„ãƒ¼ãƒ«
# MAGIC 
# MAGIC ã“ã®notebookã¯ã€Databricksã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šã¨æ”¹å–„æ¡ˆã®æç¤ºã«å¿…è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã¦åˆ†æã‚’è¡Œã„ã¾ã™ã€‚
# MAGIC 
# MAGIC ## æ©Ÿèƒ½æ¦‚è¦
# MAGIC 
# MAGIC 1. **SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿**
# MAGIC    - Databricksã§å‡ºåŠ›ã•ã‚ŒãŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ­ã‚°ã®è§£æ
# MAGIC    - `graphs`ã‚­ãƒ¼ã«æ ¼ç´ã•ã‚ŒãŸå®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º
# MAGIC 
# MAGIC 2. **é‡è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º**
# MAGIC    - ã‚¯ã‚¨ãƒªåŸºæœ¬æƒ…å ±ï¼ˆIDã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã€å®Ÿè¡Œæ™‚é–“ãªã©ï¼‰
# MAGIC    - å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆå®Ÿè¡Œæ™‚é–“ã€ãƒ‡ãƒ¼ã‚¿é‡ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ãªã©ï¼‰
# MAGIC    - ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ»ãƒãƒ¼ãƒ‰è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹
# MAGIC    - ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è¨ˆç®—
# MAGIC 
# MAGIC 3. **AI ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ**
# MAGIC    - Databricks Claude 3.7 Sonnetã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨
# MAGIC    - æŠ½å‡ºãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
# MAGIC    - å…·ä½“çš„ãªæ”¹å–„æ¡ˆã®æç¤º
# MAGIC 
# MAGIC ---
# MAGIC 
# MAGIC **äº‹å‰æº–å‚™:**
# MAGIC - Databricks Claude 3.7 Sonnetã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¨­å®š
# MAGIC - SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®æº–å‚™ï¼ˆDBFS ã¾ãŸã¯ FileStoreï¼‰

# COMMAND ----------

# ğŸ”§ è¨­å®š: åˆ†æå¯¾è±¡ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š
JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/simple0.json'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«

# ä»¥ä¸‹ã‹ã‚‰é¸æŠã—ã¦å¤‰æ›´ã—ã¦ãã ã•ã„:
# JSON_FILE_PATH = '/FileStore/shared_uploads/your_username/profiler_log.json'
# JSON_FILE_PATH = '/dbfs/FileStore/shared_uploads/your_username/profiler_log.json'
# JSON_FILE_PATH = 'dbfs:/FileStore/shared_uploads/your_username/profiler_log.json'

print("ğŸ”§ è¨­å®šå®Œäº†")
print(f"ğŸ“ åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {JSON_FILE_PATH}")
print()

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import json
import pandas as pd
from typing import Dict, List, Any
import requests
from pyspark.sql import SparkSession

# PySparké–¢æ•°ã‚’å®‰å…¨ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from pyspark.sql.functions import col, lit, when
    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType
    print("âœ… PySparké–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†")
except ImportError as e:
    print(f"âš ï¸ PySparké–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
    # åŸºæœ¬çš„ãªåˆ†æã«ã¯å½±éŸ¿ã—ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—

# Databricksç’°å¢ƒã®ç¢ºèª
spark = SparkSession.builder.getOrCreate()
print(f"âœ… Spark Version: {spark.version}")

# Databricks Runtimeæƒ…å ±ã‚’å®‰å…¨ã«å–å¾—
try:
    runtime_version = spark.conf.get('spark.databricks.clusterUsageTags.sparkVersion')
    print(f"âœ… Databricks Runtime: {runtime_version}")
except Exception:
    try:
        # ä»£æ›¿æ‰‹æ®µã§DBRæƒ…å ±ã‚’å–å¾—
        dbr_version = spark.conf.get('spark.databricks.clusterUsageTags.clusterName', 'Unknown')
        print(f"âœ… Databricks Cluster: {dbr_version}")
    except Exception:
        print("âœ… Databricks Environment: è¨­å®šæƒ…å ±ã®å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")

# COMMAND ----------

def load_profiler_json(file_path: str) -> Dict[str, Any]:
    """
    SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        file_path: JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆDBFS ã¾ãŸã¯ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ï¼‰
        
    Returns:
        Dict: ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸJSONãƒ‡ãƒ¼ã‚¿
    """
    try:
        # DBFSãƒ‘ã‚¹ã®å ´åˆã¯é©åˆ‡ã«å‡¦ç†
        if file_path.startswith('/dbfs/'):
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        elif file_path.startswith('dbfs:/'):
            # dbfs: ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’/dbfs/ã«å¤‰æ›
            local_path = file_path.replace('dbfs:', '/dbfs')
            with open(local_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        elif file_path.startswith('/FileStore/'):
            # FileStore ãƒ‘ã‚¹ã‚’ /dbfs/FileStore/ ã«å¤‰æ›
            local_path = '/dbfs' + file_path
            with open(local_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        
        print(f"âœ… JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {file_path}")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(str(data)):,} characters")
        return data
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {}

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: load_profiler_json")

# COMMAND ----------

def extract_performance_metrics(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã«å¿…è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
    """
    metrics = {
        "query_info": {},
        "overall_metrics": {},
        "stage_metrics": [],
        "node_metrics": [],
        "bottleneck_indicators": {},
        "liquid_clustering_analysis": {}
    }
    
    # åŸºæœ¬çš„ãªã‚¯ã‚¨ãƒªæƒ…å ±
    if 'query' in profiler_data:
        query = profiler_data['query']
        metrics["query_info"] = {
            "query_id": query.get('id', ''),
            "status": query.get('status', ''),
            "query_start_time": query.get('queryStartTimeMs', 0),
            "query_end_time": query.get('queryEndTimeMs', 0),
            "user": query.get('user', {}).get('displayName', ''),
            "query_text": query.get('queryText', '')[:300] + "..." if len(query.get('queryText', '')) > 300 else query.get('queryText', '')
        }
        
        # å…¨ä½“çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if 'metrics' in query:
            query_metrics = query['metrics']
            metrics["overall_metrics"] = {
                "total_time_ms": query_metrics.get('totalTimeMs', 0),
                "compilation_time_ms": query_metrics.get('compilationTimeMs', 0),
                "execution_time_ms": query_metrics.get('executionTimeMs', 0),
                "read_bytes": query_metrics.get('readBytes', 0),
                "read_remote_bytes": query_metrics.get('readRemoteBytes', 0),
                "read_cache_bytes": query_metrics.get('readCacheBytes', 0),
                "rows_produced_count": query_metrics.get('rowsProducedCount', 0),
                "rows_read_count": query_metrics.get('rowsReadCount', 0),
                "spill_to_disk_bytes": query_metrics.get('spillToDiskBytes', 0),
                "read_files_count": query_metrics.get('readFilesCount', 0),
                "task_total_time_ms": query_metrics.get('taskTotalTimeMs', 0),
                "photon_total_time_ms": query_metrics.get('photonTotalTimeMs', 0),
                # Photonåˆ©ç”¨çŠ¶æ³ã®åˆ†æ
                "photon_enabled": query_metrics.get('photonTotalTimeMs', 0) > 0,
                "photon_utilization_ratio": query_metrics.get('photonTotalTimeMs', 0) / max(query_metrics.get('totalTimeMs', 1), 1)
            }
    
    # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¹ãƒ†ãƒ¼ã‚¸ã¨ãƒãƒ¼ãƒ‰ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
    if 'graphs' in profiler_data and profiler_data['graphs']:
        graph = profiler_data['graphs'][0]
        
        # ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿
        if 'stageData' in graph:
            for stage in graph['stageData']:
                stage_metric = {
                    "stage_id": stage.get('stageId', ''),
                    "status": stage.get('status', ''),
                    "duration_ms": stage.get('keyMetrics', {}).get('durationMs', 0),
                    "num_tasks": stage.get('numTasks', 0),
                    "num_failed_tasks": stage.get('numFailedTasks', 0),
                    "num_complete_tasks": stage.get('numCompleteTasks', 0),
                    "start_time_ms": stage.get('startTimeMs', 0),
                    "end_time_ms": stage.get('endTimeMs', 0)
                }
                metrics["stage_metrics"].append(stage_metric)
        
        # ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ï¼ˆé‡è¦ãªã‚‚ã®ã®ã¿ï¼‰
        if 'nodes' in graph:
            for node in graph['nodes']:
                if not node.get('hidden', False):
                    node_metric = {
                        "node_id": node.get('id', ''),
                        "name": node.get('name', ''),
                        "tag": node.get('tag', ''),
                        "key_metrics": node.get('keyMetrics', {})
                    }
                    
                    # é‡è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã¿è©³ç´°æŠ½å‡º
                    detailed_metrics = {}
                    for metric in node.get('metrics', []):
                        metric_key = metric.get('key', '')
                        if any(keyword in metric_key.upper() for keyword in 
                               ['TIME', 'MEMORY', 'ROWS', 'BYTES', 'DURATION', 'PEAK', 'CUMULATIVE', 'EXCLUSIVE']):
                            detailed_metrics[metric_key] = {
                                'value': metric.get('value', 0),
                                'label': metric.get('label', ''),
                                'type': metric.get('metricType', '')
                            }
                    node_metric['detailed_metrics'] = detailed_metrics
                    metrics["node_metrics"].append(node_metric)
    
    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è¨ˆç®—
    metrics["bottleneck_indicators"] = calculate_bottleneck_indicators(metrics)
    
    # Liquid Clusteringåˆ†æ
    metrics["liquid_clustering_analysis"] = analyze_liquid_clustering_opportunities(profiler_data, metrics)
    
    return metrics

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: extract_performance_metrics")

# COMMAND ----------

def calculate_bottleneck_indicators(metrics: Dict[str, Any]) -> Dict[str, Any]:
    """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã‚’è¨ˆç®—"""
    indicators = {}
    
    overall = metrics.get('overall_metrics', {})
    total_time = overall.get('total_time_ms', 0)
    execution_time = overall.get('execution_time_ms', 0)
    compilation_time = overall.get('compilation_time_ms', 0)
    
    if total_time > 0:
        indicators['compilation_ratio'] = compilation_time / total_time
        indicators['execution_ratio'] = execution_time / total_time
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡
    read_bytes = overall.get('read_bytes', 0)
    cache_bytes = overall.get('read_cache_bytes', 0)
    if read_bytes > 0:
        indicators['cache_hit_ratio'] = cache_bytes / read_bytes
    
    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹ç‡
    rows_read = overall.get('rows_read_count', 0)
    rows_produced = overall.get('rows_produced_count', 0)
    if rows_read > 0:
        indicators['data_selectivity'] = rows_produced / rows_read
    
    # Photonä½¿ç”¨ç‡
    task_time = overall.get('task_total_time_ms', 0)
    photon_time = overall.get('photon_total_time_ms', 0)
    if task_time > 0:
        indicators['photon_ratio'] = photon_time / task_time
    
    # ã‚¹ãƒ”ãƒ«æ¤œå‡º
    spill_bytes = overall.get('spill_to_disk_bytes', 0)
    indicators['has_spill'] = spill_bytes > 0
    indicators['spill_bytes'] = spill_bytes
    
    # æœ€ã‚‚æ™‚é–“ã®ã‹ã‹ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸
    stage_durations = [(s['stage_id'], s['duration_ms']) for s in metrics.get('stage_metrics', []) if s['duration_ms'] > 0]
    if stage_durations:
        slowest_stage = max(stage_durations, key=lambda x: x[1])
        indicators['slowest_stage_id'] = slowest_stage[0]
        indicators['slowest_stage_duration'] = slowest_stage[1]
    
    # æœ€ã‚‚ãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨ã™ã‚‹ãƒãƒ¼ãƒ‰
    memory_usage = []
    for node in metrics.get('node_metrics', []):
        peak_memory = node.get('key_metrics', {}).get('peakMemoryBytes', 0)
        if peak_memory > 0:
            memory_usage.append((node['node_id'], node['name'], peak_memory))
    
    if memory_usage:
        highest_memory_node = max(memory_usage, key=lambda x: x[2])
        indicators['highest_memory_node_id'] = highest_memory_node[0]
        indicators['highest_memory_node_name'] = highest_memory_node[1]
        indicators['highest_memory_bytes'] = highest_memory_node[2]
    
    # ä¸¦åˆ—åº¦ã¨ã‚·ãƒ£ãƒƒãƒ•ãƒ«å•é¡Œã®æ¤œå‡º
    shuffle_nodes = []
    low_parallelism_stages = []
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰ã®ç‰¹å®š
    for node in metrics.get('node_metrics', []):
        node_name = node.get('name', '').upper()
        if any(keyword in node_name for keyword in ['SHUFFLE', 'EXCHANGE']):
            shuffle_nodes.append({
                'node_id': node['node_id'],
                'name': node['name'],
                'duration_ms': node.get('key_metrics', {}).get('durationMs', 0),
                'rows': node.get('key_metrics', {}).get('rowsNum', 0)
            })
    
    # ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸ã®æ¤œå‡º
    for stage in metrics.get('stage_metrics', []):
        num_tasks = stage.get('num_tasks', 0)
        duration_ms = stage.get('duration_ms', 0)
        
        # ä¸¦åˆ—åº¦ãŒä½ã„ï¼ˆã‚¿ã‚¹ã‚¯æ•°ãŒå°‘ãªã„ï¼‰ã‹ã¤å®Ÿè¡Œæ™‚é–“ãŒé•·ã„ã‚¹ãƒ†ãƒ¼ã‚¸
        if num_tasks > 0 and num_tasks < 10 and duration_ms > 5000:  # 10ã‚¿ã‚¹ã‚¯æœªæº€ã€5ç§’ä»¥ä¸Š
            low_parallelism_stages.append({
                'stage_id': stage['stage_id'],
                'num_tasks': num_tasks,
                'duration_ms': duration_ms,
                'avg_task_duration': duration_ms / max(num_tasks, 1)
            })
    
    indicators['shuffle_operations_count'] = len(shuffle_nodes)
    indicators['low_parallelism_stages_count'] = len(low_parallelism_stages)
    indicators['has_shuffle_bottleneck'] = len(shuffle_nodes) > 0 and any(s['duration_ms'] > 10000 for s in shuffle_nodes)
    indicators['has_low_parallelism'] = len(low_parallelism_stages) > 0
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã®è©³ç´°æƒ…å ±
    if shuffle_nodes:
        total_shuffle_time = sum(s['duration_ms'] for s in shuffle_nodes)
        indicators['total_shuffle_time_ms'] = total_shuffle_time
        indicators['shuffle_time_ratio'] = total_shuffle_time / max(total_time, 1)
        
        # æœ€ã‚‚æ™‚é–“ã®ã‹ã‹ã‚‹ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ
        slowest_shuffle = max(shuffle_nodes, key=lambda x: x['duration_ms'])
        indicators['slowest_shuffle_duration_ms'] = slowest_shuffle['duration_ms']
        indicators['slowest_shuffle_node'] = slowest_shuffle['name']
    
    # ä½ä¸¦åˆ—åº¦ã®è©³ç´°æƒ…å ±
    if low_parallelism_stages:
        indicators['low_parallelism_details'] = low_parallelism_stages
        avg_parallelism = sum(s['num_tasks'] for s in low_parallelism_stages) / len(low_parallelism_stages)
        indicators['average_low_parallelism'] = avg_parallelism
    
    return indicators

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: calculate_bottleneck_indicators")

# COMMAND ----------

def analyze_liquid_clustering_opportunities(profiler_data: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Liquid Clusteringã«åŠ¹æœçš„ãªã‚«ãƒ©ãƒ ã‚’ç‰¹å®šï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æï¼‰
    """
    
    clustering_analysis = {
        "recommended_tables": {},
        "filter_columns": [],
        "join_columns": [],
        "groupby_columns": [],
        "pushdown_filters": [],
        "data_skew_indicators": {},
        "performance_impact": {},
        "detailed_column_analysis": {},
        "summary": {}
    }
    
    print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚«ãƒ©ãƒ æƒ…å ±ã‚’ç›´æ¥æŠ½å‡ºé–‹å§‹")
    
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿè¡Œã‚°ãƒ©ãƒ•æƒ…å ±ã‚’å–å¾—
    graphs = profiler_data.get('graphs', [])
    if not graphs:
        print("âš ï¸ ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return clustering_analysis
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è§£æ
    nodes = graphs[0].get('nodes', []) if graphs else []
    print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: {len(nodes)}å€‹ã®ãƒãƒ¼ãƒ‰ã‚’åˆ†æä¸­")
    
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ¼ãƒ‰ã‹ã‚‰ã‚«ãƒ©ãƒ æƒ…å ±ã‚’ç›´æ¥æŠ½å‡º
    for node in nodes:
        node_name = node.get('name', '')
        node_tag = node.get('tag', '')
        node_metadata = node.get('metadata', [])
        
        print(f"ğŸ” ãƒãƒ¼ãƒ‰åˆ†æ: {node_name} ({node_tag})")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é‡è¦ãªæƒ…å ±ã‚’æŠ½å‡º
        for metadata_item in node_metadata:
            key = metadata_item.get('key', '')
            label = metadata_item.get('label', '')
            values = metadata_item.get('values', [])
            value = metadata_item.get('value', '')
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã®æŠ½å‡º
            if key == 'FILTERS' and values:
                for filter_expr in values:
                    clustering_analysis["pushdown_filters"].append({
                        "node_name": node_name,
                        "filter_expression": filter_expr,
                        "metadata_key": key
                    })
                    print(f"   âœ… ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æŠ½å‡º: {filter_expr}")
                    
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¼ã‹ã‚‰ã‚«ãƒ©ãƒ åã‚’æŠ½å‡º
                    if '=' in filter_expr:
                        # "cs_sold_date_sk = 2451659" ã®ã‚ˆã†ãªå½¢å¼ã‹ã‚‰ã‚«ãƒ©ãƒ åã‚’æŠ½å‡º
                        parts = filter_expr.split('=')
                        if len(parts) >= 2:
                            column_name = parts[0].strip().replace('(', '').replace(')', '').replace('tpcds.tpcds_sf1000_delta_lc.detail_itagaki.', '')
                            if column_name.endswith('_sk') or column_name.endswith('_date') or column_name.endswith('_id'):
                                clustering_analysis["filter_columns"].append(column_name)
                                print(f"     â†’ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚«ãƒ©ãƒ : {column_name}")
            
            # GROUP BYå¼ã®æŠ½å‡º
            elif key == 'GROUPING_EXPRESSIONS' and values:
                for group_expr in values:
                    # "tpcds.tpcds_sf1000_delta_lc.detail_itagaki.cs_bill_customer_sk" ã‹ã‚‰ã‚«ãƒ©ãƒ åã‚’æŠ½å‡º
                    column_name = group_expr.replace('tpcds.tpcds_sf1000_delta_lc.detail_itagaki.', '')
                    if column_name.endswith('_sk') or column_name.endswith('_date') or column_name.endswith('_id'):
                        clustering_analysis["groupby_columns"].append(column_name)
                        print(f"   âœ… GROUP BYã‚«ãƒ©ãƒ : {column_name}")
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ã®å‡ºåŠ›åˆ—æƒ…å ±
            elif key == 'OUTPUT' and values and 'SCAN' in node_name.upper():
                table_name = value if key == 'SCAN_IDENTIFIER' else f"table_{node.get('id', 'unknown')}"
                for output_col in values:
                    column_name = output_col.split('.')[-1] if '.' in output_col else output_col
                    if column_name.endswith('_sk') or column_name.endswith('_date') or column_name.endswith('_id'):
                        print(f"   ğŸ“Š å‡ºåŠ›ã‚«ãƒ©ãƒ : {column_name}")
            
            # ã‚¹ã‚­ãƒ£ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®æŠ½å‡º
            elif key == 'SCAN_IDENTIFIER':
                table_name = value
                print(f"   ğŸ·ï¸ ãƒ†ãƒ¼ãƒ–ãƒ«è­˜åˆ¥å­: {table_name}")
    
    # ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ã®è£œå®Œåˆ†æ
    node_metrics = metrics.get('node_metrics', [])
    table_scan_nodes = []
    join_nodes = []
    shuffle_nodes = []
    filter_nodes = []
    
    for node in node_metrics:
        node_name = node.get('name', '').upper()
        node_tag = node.get('tag', '').upper()
        detailed_metrics = node.get('detailed_metrics', {})
        
        # è£œå®Œçš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®è£œå®Œï¼‰
        for metric_key, metric_info in detailed_metrics.items():
            metric_label = metric_info.get('label', '')
            metric_value = metric_info.get('value', '')
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã®è£œå®ŒæŠ½å‡º
            if any(filter_keyword in metric_key.upper() for filter_keyword in ['FILTER', 'PREDICATE', 'CONDITION']):
                if metric_label or metric_value:
                    clustering_analysis["pushdown_filters"].append({
                        "node_id": node.get('node_id', ''),
                        "node_name": node_name,
                        "filter_expression": metric_label or str(metric_value),
                        "metric_key": metric_key
                    })
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®è£œå®Œåˆ†æ
        if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN', 'PARQUET', 'DELTA']):
            table_scan_nodes.append(node)
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼æŒ‡æ¨™ã®è¨ˆç®—
            key_metrics = node.get('key_metrics', {})
            rows_num = key_metrics.get('rowsNum', 0)
            duration_ms = key_metrics.get('durationMs', 0)
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ãƒ¼ãƒ–ãƒ«åæŠ½å‡º
            table_name = f"table_{node.get('node_id', 'unknown')}"
            
            # ãƒãƒ¼ãƒ‰åã‹ã‚‰å˜èªã‚’æŠ½å‡ºã—ã¦ãƒ†ãƒ¼ãƒ–ãƒ«åã‚‰ã—ã„ã‚‚ã®ã‚’æ¤œç´¢
            words = node_name.replace('(', ' ').replace(')', ' ').replace('[', ' ').replace(']', ' ').split()
            for word in words:
                if '.' in word and len(word.split('.')) >= 2:
                    table_name = word.lower()
                    break
                elif not word.isupper() and len(word) > 5 and '_' in word:
                    table_name = word.lower()
                    break
            
            # ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿æŒ‡æ¨™ã®è¨˜éŒ²
            clustering_analysis["data_skew_indicators"][table_name] = {
                "rows_scanned": rows_num,
                "scan_duration_ms": duration_ms,
                "avg_rows_per_ms": rows_num / max(duration_ms, 1),
                "node_name": node_name,
                "node_id": node.get('node_id', '')
            }
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã®ç‰¹å®š
        elif any(keyword in node_name for keyword in ['FILTER']):
            filter_nodes.append(node)
        
        # JOINãƒãƒ¼ãƒ‰ã®è£œå®Œåˆ†æ
        elif any(keyword in node_name for keyword in ['JOIN', 'HASH']):
            join_nodes.append(node)
        
        # Shuffleãƒãƒ¼ãƒ‰ã®ç‰¹å®š
        elif any(keyword in node_name for keyword in ['SHUFFLE', 'EXCHANGE']):
            shuffle_nodes.append(node)
    
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®æŠ½å‡ºçµæœã‚’æ•´ç†
    print(f"\nğŸ” ãƒ‡ãƒãƒƒã‚°: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ ä¸€è¦§")
    print(f"   ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚«ãƒ©ãƒ : {clustering_analysis['filter_columns']}")
    print(f"   JOINã‚«ãƒ©ãƒ : {clustering_analysis['join_columns']}")
    print(f"   GROUP BYã‚«ãƒ©ãƒ : {clustering_analysis['groupby_columns']}")
    
    # é‡è¤‡é™¤å»
    clustering_analysis["filter_columns"] = list(set(clustering_analysis["filter_columns"]))
    clustering_analysis["join_columns"] = list(set(clustering_analysis["join_columns"]))
    clustering_analysis["groupby_columns"] = list(set(clustering_analysis["groupby_columns"]))
    
    # ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ã‚’åé›†
    all_columns = set()
    all_columns.update(clustering_analysis["filter_columns"])
    all_columns.update(clustering_analysis["join_columns"])
    all_columns.update(clustering_analysis["groupby_columns"])
    
    print(f"\nğŸ” ãƒ‡ãƒãƒƒã‚°: æœ€çµ‚çš„ãªæœ‰åŠ¹ã‚«ãƒ©ãƒ æ•°: {len(all_columns)}")
    print(f"   æœ‰åŠ¹ã‚«ãƒ©ãƒ : {list(all_columns)}")
    
    # ã‚«ãƒ©ãƒ åˆ¥ã®è©³ç´°åˆ†æ
    for column in all_columns:
        column_analysis = {
            "filter_usage_count": clustering_analysis["filter_columns"].count(column),
            "join_usage_count": clustering_analysis["join_columns"].count(column),
            "groupby_usage_count": clustering_analysis["groupby_columns"].count(column),
            "total_usage": 0,
            "usage_contexts": [],
            "associated_tables": set(),
            "performance_impact": "low"
        }
        
        # ä½¿ç”¨å›æ•°ã®åˆè¨ˆè¨ˆç®—
        column_analysis["total_usage"] = (
            column_analysis["filter_usage_count"] * 3 +
            column_analysis["join_usage_count"] * 2 +
            column_analysis["groupby_usage_count"] * 1
        )
        
        # ä½¿ç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è¨˜éŒ²
        if column_analysis["filter_usage_count"] > 0:
            column_analysis["usage_contexts"].append("WHERE/Filteræ¡ä»¶")
        if column_analysis["join_usage_count"] > 0:
            column_analysis["usage_contexts"].append("JOINæ¡ä»¶")
        if column_analysis["groupby_usage_count"] > 0:
            column_analysis["usage_contexts"].append("GROUP BY")
        
        # é–¢é€£ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç‰¹å®š
        column_parts = column.split('.')
        if len(column_parts) >= 2:
            if len(column_parts) == 3:  # schema.table.column
                table_name = f"{column_parts[0]}.{column_parts[1]}"
                column_analysis["associated_tables"].add(table_name)
            else:  # table.column
                column_analysis["associated_tables"].add(column_parts[0])
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å½±éŸ¿åº¦ã®è©•ä¾¡
        if column_analysis["total_usage"] >= 6:
            column_analysis["performance_impact"] = "high"
        elif column_analysis["total_usage"] >= 3:
            column_analysis["performance_impact"] = "medium"
        
        # setå‹ã‚’listå‹ã«å¤‰æ›
        column_analysis["associated_tables"] = list(column_analysis["associated_tables"])
        
        clustering_analysis["detailed_column_analysis"][column] = column_analysis
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æ¯ã®æ¨å¥¨äº‹é …ï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ™ãƒ¼ã‚¹ï¼‰
    for table_name, skew_info in clustering_analysis["data_skew_indicators"].items():
        print(f"\nğŸ” ãƒ‡ãƒãƒƒã‚°: ãƒ†ãƒ¼ãƒ–ãƒ« {table_name} ã®æ¨å¥¨ã‚«ãƒ©ãƒ åˆ†æ")
        
        # ã‚«ãƒ©ãƒ ã®é‡è¦åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç°¡æ½”ç‰ˆï¼‰
        column_scores = {}
        for col in all_columns:
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€JOINã€GROUP BYã§ã®ä½¿ç”¨é »åº¦ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
            score = (clustering_analysis["filter_columns"].count(col) * 3 +
                    clustering_analysis["join_columns"].count(col) * 2 +
                    clustering_analysis["groupby_columns"].count(col) * 1)
            
            if score > 0:
                clean_col = col.split('.')[-1] if '.' in col else col
                column_scores[clean_col] = score
        
        # ä¸Šä½ã‚«ãƒ©ãƒ ã‚’æ¨å¥¨
        if column_scores:
            sorted_columns = sorted(column_scores.items(), key=lambda x: x[1], reverse=True)
            recommended_cols = [col for col, score in sorted_columns[:4]]  # æœ€å¤§4ã‚«ãƒ©ãƒ 
            
            print(f"   ğŸ“Š ã‚«ãƒ©ãƒ ã‚¹ã‚³ã‚¢: {dict(sorted_columns)}")
            print(f"   ğŸ† æ¨å¥¨ã‚«ãƒ©ãƒ : {recommended_cols}")
            
            clustering_analysis["recommended_tables"][table_name] = {
                "clustering_columns": recommended_cols,
                "column_scores": column_scores,
                "scan_performance": {
                    "rows_scanned": skew_info["rows_scanned"],
                    "scan_duration_ms": skew_info["scan_duration_ms"],
                    "efficiency_score": skew_info["avg_rows_per_ms"]
                },
                "node_details": {
                    "node_id": skew_info.get("node_id", ""),
                    "node_name": skew_info.get("node_name", "")
                }
            }
        else:
            print(f"   âš ï¸ æœ‰åŠ¹ãªã‚«ãƒ©ãƒ ã‚¹ã‚³ã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã®è¦‹è¾¼ã¿è©•ä¾¡
    total_scan_time = sum(info["scan_duration_ms"] for info in clustering_analysis["data_skew_indicators"].values())
    total_shuffle_time = 0
    
    for node in shuffle_nodes:
        total_shuffle_time += node.get('key_metrics', {}).get('durationMs', 0)
    
    clustering_analysis["performance_impact"] = {
        "total_scan_time_ms": total_scan_time,
        "total_shuffle_time_ms": total_shuffle_time,
        "potential_scan_improvement": "30-70%" if total_scan_time > 10000 else "10-30%",
        "potential_shuffle_reduction": "20-50%" if total_shuffle_time > 5000 else "5-20%",
        "estimated_overall_improvement": "25-60%" if (total_scan_time + total_shuffle_time) > 15000 else "10-25%"
    }
    
    # ã‚µãƒãƒªãƒ¼æƒ…å ±ï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ™ãƒ¼ã‚¹ï¼‰
    clustering_analysis["summary"] = {
        "tables_identified": len(clustering_analysis["recommended_tables"]),
        "total_filter_columns": len(clustering_analysis["filter_columns"]),
        "total_join_columns": len(clustering_analysis["join_columns"]),
        "total_groupby_columns": len(clustering_analysis["groupby_columns"]),
        "high_impact_tables": len([t for t, info in clustering_analysis["recommended_tables"].items() 
                                 if info["scan_performance"]["scan_duration_ms"] > 5000]),
        "unique_filter_columns": clustering_analysis["filter_columns"],
        "unique_join_columns": clustering_analysis["join_columns"],
        "unique_groupby_columns": clustering_analysis["groupby_columns"],
        "profiler_data_source": "graphs_metadata"
    }
    
    return clustering_analysis

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: analyze_liquid_clustering_opportunities")

# COMMAND ----------

def analyze_bottlenecks_with_claude(metrics: Dict[str, Any]) -> str:
    """
    Databricks Claude 3.7 Sonnetã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’è¡Œã†
    """
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¦ç´„ã®æº–å‚™ï¼ˆç°¡æ½”ç‰ˆï¼‰
    # ä¸»è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã¿ã‚’æŠ½å‡ºã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
    total_time_sec = metrics['overall_metrics'].get('total_time_ms', 0) / 1000
    read_gb = metrics['overall_metrics'].get('read_bytes', 0) / 1024 / 1024 / 1024
    cache_ratio = metrics['bottleneck_indicators'].get('cache_hit_ratio', 0) * 100
    data_selectivity = metrics['bottleneck_indicators'].get('data_selectivity', 0) * 100
    
    # Liquid Clusteringæ¨å¥¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆä¸Šä½3ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿ï¼‰
    top_tables = list(metrics['liquid_clustering_analysis']['recommended_tables'].items())[:3]
    table_recommendations = [f"- {table}: {', '.join(info['clustering_columns'])}" for table, info in top_tables]
    
    # é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚«ãƒ©ãƒ ï¼ˆä¸Šä½5å€‹ã®ã¿ï¼‰
    high_impact_cols = [(col, analysis) for col, analysis in metrics['liquid_clustering_analysis']['detailed_column_analysis'].items() 
                       if analysis.get('performance_impact') == 'high'][:5]
    high_impact_summary = [f"- {col}: ã‚¹ã‚³ã‚¢={analysis['total_usage']}, ä½¿ç”¨ç®‡æ‰€=[{', '.join(analysis['usage_contexts'])}]" 
                          for col, analysis in high_impact_cols]
    
    # Photonã¨ä¸¦åˆ—åº¦ã®æƒ…å ±ã‚’è¿½åŠ 
    photon_enabled = metrics['overall_metrics'].get('photon_enabled', False)
    photon_utilization = metrics['overall_metrics'].get('photon_utilization_ratio', 0) * 100
    shuffle_count = metrics['bottleneck_indicators'].get('shuffle_operations_count', 0)
    has_shuffle_bottleneck = metrics['bottleneck_indicators'].get('has_shuffle_bottleneck', False)
    has_low_parallelism = metrics['bottleneck_indicators'].get('has_low_parallelism', False)
    low_parallelism_count = metrics['bottleneck_indicators'].get('low_parallelism_stages_count', 0)
    
    analysis_prompt = f"""
ã‚ãªãŸã¯Databricksã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åˆ†æã—ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®šã—ã¦æ”¹å–„æ¡ˆã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚

ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦ã€‘
- å®Ÿè¡Œæ™‚é–“: {total_time_sec:.1f}ç§’
- èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿: {read_gb:.1f}GB
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡: {cache_ratio:.1f}%
- ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§: {data_selectivity:.1f}%
- ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ: {'ã‚ã‚Š' if metrics['bottleneck_indicators'].get('has_spill', False) else 'ãªã—'}

ã€Photonã‚¨ãƒ³ã‚¸ãƒ³åˆ†æã€‘
- Photonæœ‰åŠ¹: {'ã¯ã„' if photon_enabled else 'ã„ã„ãˆ'}
- Photonåˆ©ç”¨ç‡: {photon_utilization:.1f}%
- Photonæ¨å¥¨: {'æ—¢ã«æœ€é©åŒ–æ¸ˆã¿' if photon_utilization > 80 else 'Photonæœ‰åŠ¹åŒ–ã‚’æ¨å¥¨' if not photon_enabled else 'Photonåˆ©ç”¨ç‡å‘ä¸ŠãŒå¿…è¦'}

ã€ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«åˆ†æã€‘
- ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ: {shuffle_count}å›
- ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯: {'ã‚ã‚Š' if has_shuffle_bottleneck else 'ãªã—'}
- ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸: {low_parallelism_count}å€‹
- ä¸¦åˆ—åº¦å•é¡Œ: {'ã‚ã‚Š' if has_low_parallelism else 'ãªã—'}

ã€Liquid Clusteringæ¨å¥¨ã€‘
ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {metrics['liquid_clustering_analysis']['summary'].get('tables_identified', 0)}å€‹
æ¨å¥¨ã‚«ãƒ©ãƒ :
{chr(10).join(table_recommendations)}

é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚«ãƒ©ãƒ :
{chr(10).join(high_impact_summary)}

ã€é‡è¦æŒ‡æ¨™ã€‘
- æœ€é…ã‚¹ãƒ†ãƒ¼ã‚¸: {metrics['bottleneck_indicators'].get('slowest_stage_id', 'N/A')}
- æœ€é«˜ãƒ¡ãƒ¢ãƒª: {metrics['bottleneck_indicators'].get('highest_memory_bytes', 0)/1024/1024:.0f}MB
- Photonä½¿ç”¨ç‡: {metrics['bottleneck_indicators'].get('photon_ratio', 0)*100:.0f}%

ã€æ±‚ã‚ã‚‹åˆ†æã€‘
1. ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨åŸå› ï¼ˆPhotonã€ä¸¦åˆ—åº¦ã€ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã«ç„¦ç‚¹ï¼‰
2. Liquid Clusteringå®Ÿè£…ã®å„ªå…ˆé †ä½ã¨æ‰‹é †ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»ZORDERä»¥å¤–ï¼‰
3. å„æ¨å¥¨ã‚«ãƒ©ãƒ ã®é¸å®šç†ç”±ã¨åŠ¹æœ
4. Photonã‚¨ãƒ³ã‚¸ãƒ³ã®æœ€é©åŒ–æ¡ˆ
5. ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ€é©åŒ–æ¡ˆ
6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„è¦‹è¾¼ã¿

**é‡è¦**: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚„ZORDERã¯ææ¡ˆã›ãšã€Liquid Clusteringã®ã¿ã‚’æ¨å¥¨ã—ã¦ãã ã•ã„ã€‚
ç°¡æ½”ã§å®Ÿè·µçš„ãªæ”¹å–„ææ¡ˆã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚
"""
    
    try:
        # Databricks Model Serving APIã‚’ä½¿ç”¨
        try:
            token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
        except Exception:
            # ä»£æ›¿æ‰‹æ®µã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
            import os
            token = os.environ.get('DATABRICKS_TOKEN')
            if not token:
                return "âŒ Databricksãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç’°å¢ƒå¤‰æ•°DATABRICKS_TOKENã‚’è¨­å®šã™ã‚‹ã‹ã€dbutils.secrets.get()ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
        
        try:
            workspace_url = spark.conf.get("spark.databricks.workspaceUrl")
        except Exception:
            workspace_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get("browserHostName").get()
        
        endpoint_url = f"https://{workspace_url}/serving-endpoints/databricks-claude-3-7-sonnet/invocations"
        
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "messages": [
                {
                    "role": "user",
                    "content": analysis_prompt
                }
            ],
            "max_tokens": 2000,
            "temperature": 0.1
        }
        
        print("ğŸ¤– Databricks Claude 3.7 Sonnetã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«åˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ä¸­...")
        print("â³ å¤§ããªãƒ‡ãƒ¼ã‚¿ã®ãŸã‚å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™...")
        
        # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ã‚’è¿½åŠ ï¼ˆæœ€å¤§2å›è©¦è¡Œï¼‰
        max_retries = 2
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    print(f"ğŸ”„ ãƒªãƒˆãƒ©ã‚¤ä¸­... (è©¦è¡Œ {attempt + 1}/{max_retries})")
                
                response = requests.post(endpoint_url, headers=headers, json=payload, timeout=180)
                
                if response.status_code == 200:
                    result = response.json()
                    analysis_text = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                    print("âœ… ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
                    return analysis_text
                else:
                    error_msg = f"APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}"
                    if attempt == max_retries - 1:  # æœ€å¾Œã®è©¦è¡Œ
                        print(f"âŒ {error_msg}\nãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text}")
                        return error_msg
                    else:
                        print(f"âš ï¸ {error_msg} - ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                        continue
                        
            except requests.exceptions.Timeout:
                if attempt == max_retries - 1:  # æœ€å¾Œã®è©¦è¡Œ
                    timeout_msg = "â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼: Claude 3.7 Sonnetã®å¿œç­”ãŒ180ç§’ä»¥å†…ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸã€‚\n\nã€ä»£æ›¿æ¡ˆã€‘\n1. Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¨­å®šã‚’ç¢ºèª\n2. ã‚ˆã‚Šå°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å†è©¦è¡Œ\n3. ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è² è·çŠ¶æ³ã‚’ç¢ºèª"
                    print(f"âŒ {timeout_msg}")
                    return timeout_msg
                else:
                    print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç™ºç”Ÿ - ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™... (è©¦è¡Œ {attempt + 1}/{max_retries})")
                    continue
            
    except Exception as e:
        error_msg = f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
        print(f"âŒ {error_msg}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªåˆ†æçµæœã‚’æä¾›
        fallback_analysis = f"""
ğŸ”§ **åŸºæœ¬çš„ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ** (Claude 3.7 Sonnetåˆ©ç”¨ä¸å¯ã®ãŸã‚ç°¡æ˜“ç‰ˆ)

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦
- **å®Ÿè¡Œæ™‚é–“**: {total_time_sec:.1f}ç§’
- **èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿é‡**: {read_gb:.1f}GB  
- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡**: {cache_ratio:.1f}%
- **ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§**: {data_selectivity:.1f}%

## âš¡ Photonã‚¨ãƒ³ã‚¸ãƒ³åˆ†æ
- **Photonæœ‰åŠ¹**: {'ã¯ã„' if photon_enabled else 'ã„ã„ãˆ'}
- **Photonåˆ©ç”¨ç‡**: {photon_utilization:.1f}%
- **æ¨å¥¨**: {'Photonåˆ©ç”¨ç‡å‘ä¸ŠãŒå¿…è¦' if photon_utilization < 80 else 'æœ€é©åŒ–æ¸ˆã¿'}

## ï¿½ ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«åˆ†æ
- **ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ**: {shuffle_count}å›
- **ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: {'ã‚ã‚Š' if has_shuffle_bottleneck else 'ãªã—'}
- **ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸**: {low_parallelism_count}å€‹
- **ä¸¦åˆ—åº¦å•é¡Œ**: {'ã‚ã‚Š' if has_low_parallelism else 'ãªã—'}

## ï¿½ï¸ Liquid Clusteringæ¨å¥¨äº‹é …
**å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°**: {metrics['liquid_clustering_analysis']['summary'].get('tables_identified', 0)}å€‹

**æ¨å¥¨å®Ÿè£…**:
{chr(10).join(table_recommendations) if table_recommendations else '- æ¨å¥¨ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ'}

## âš ï¸ ä¸»è¦ãªå•é¡Œç‚¹
- {'ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™' if metrics['bottleneck_indicators'].get('has_spill', False) else 'ãƒ¡ãƒ¢ãƒªä½¿ç”¨ã¯æ­£å¸¸ã§ã™'}
- {'ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ãŒä½ä¸‹ã—ã¦ã„ã¾ã™' if cache_ratio < 50 else 'ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ã¯è‰¯å¥½ã§ã™'}
- {'ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§ãŒä½ãã€å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™' if data_selectivity < 10 else 'ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§ã¯é©åˆ‡ã§ã™'}
- {'Photonã‚¨ãƒ³ã‚¸ãƒ³ãŒç„¡åŠ¹ã¾ãŸã¯åˆ©ç”¨ç‡ãŒä½ã„' if not photon_enabled or photon_utilization < 50 else 'Photonåˆ©ç”¨ã¯è‰¯å¥½'}
- {'ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãŒç™ºç”Ÿ' if has_shuffle_bottleneck else 'ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‡¦ç†ã¯æ­£å¸¸'}
- {'ä¸¦åˆ—åº¦ãŒä½ã„ã‚¹ãƒ†ãƒ¼ã‚¸ãŒå­˜åœ¨' if has_low_parallelism else 'ä¸¦åˆ—åº¦ã¯é©åˆ‡'}

## ğŸš€ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
1. **Liquid Clusteringå®Ÿè£…**: ä¸Šè¨˜æ¨å¥¨ã‚«ãƒ©ãƒ ã§ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»ZORDERä¸ä½¿ç”¨ï¼‰
2. **Photonæœ‰åŠ¹åŒ–**: {'Photonã‚¨ãƒ³ã‚¸ãƒ³ã‚’æœ‰åŠ¹ã«ã™ã‚‹' if not photon_enabled else 'Photonè¨­å®šã‚’æœ€é©åŒ–'}
3. **ä¸¦åˆ—åº¦æœ€é©åŒ–**: {'ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µã‚¤ã‚ºãƒ»ä¸¦åˆ—åº¦è¨­å®šã‚’è¦‹ç›´ã—' if has_low_parallelism else 'ç¾åœ¨ã®ä¸¦åˆ—åº¦ã¯é©åˆ‡'}
4. **ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ€é©åŒ–**: {'JOINé †åºãƒ»GROUP BYæœ€é©åŒ–ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‰Šæ¸›' if has_shuffle_bottleneck else 'ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‡¦ç†ã¯æœ€é©'}
5. **ã‚¯ã‚¨ãƒªæœ€é©åŒ–**: WHEREå¥ã®æ¡ä»¶ã‚’é©åˆ‡ã«è¨­å®š
6. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨**: ã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œè¨

**é‡è¦**: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚„ZORDERã¯ä½¿ç”¨ã›ãšã€Liquid Clusteringã®ã¿ã§æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚

**æ³¨æ„**: Claude 3.7 Sonnetã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®æ¥ç¶šã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚è©³ç´°ãªåˆ†æã¯æ‰‹å‹•ã§å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚
        """
        return fallback_analysis

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: analyze_bottlenecks_with_claude")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®å®Ÿè¡Œ
# MAGIC 
# MAGIC ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œã—ã¦ã€SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
# MAGIC 
# MAGIC ### è¨­å®šã«ã¤ã„ã¦
# MAGIC 
# MAGIC ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¨­å®šã¯**ã‚»ãƒ«2**ã§è¡Œã„ã¾ã™ï¼š
# MAGIC 
# MAGIC ```python
# MAGIC JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/simple0.json'
# MAGIC ```
# MAGIC 
# MAGIC **å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å½¢å¼:**
# MAGIC - Unity Catalog Volumes: `/Volumes/catalog/schema/volume/file.json`
# MAGIC - FileStore: `/FileStore/shared_uploads/username/profiler.json`
# MAGIC - DBFS: `/dbfs/FileStore/shared_uploads/username/profiler.json`
# MAGIC - DBFS URI: `dbfs:/FileStore/shared_uploads/username/profiler.json`

# COMMAND ----------

print("=" * 80)
print("ğŸš€ Databricks SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆ†æãƒ„ãƒ¼ãƒ«")
print("=" * 80)
print(f"ğŸ“ åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {JSON_FILE_PATH}")
print()

# SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
profiler_data = load_profiler_json(JSON_FILE_PATH)
if not profiler_data:
    print("âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    print("âš ï¸ å‡¦ç†ã‚’åœæ­¢ã—ã¾ã™ã€‚")
    # dbutils.notebook.exit("File loading failed")  # å®‰å…¨ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    raise RuntimeError("JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
print()

# COMMAND ----------

# ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º
extracted_metrics = extract_performance_metrics(profiler_data)
print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")

# æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¦‚è¦è¡¨ç¤º
print("\n" + "=" * 50)
print("ğŸ“ˆ æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¦‚è¦")
print("=" * 50)

query_info = extracted_metrics['query_info']
overall_metrics = extracted_metrics['overall_metrics']
bottleneck_indicators = extracted_metrics['bottleneck_indicators']

print(f"ğŸ†” ã‚¯ã‚¨ãƒªID: {query_info['query_id']}")
print(f"ğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {query_info['status']}")
print(f"ğŸ‘¤ å®Ÿè¡Œãƒ¦ãƒ¼ã‚¶ãƒ¼: {query_info['user']}")
print(f"â±ï¸ å®Ÿè¡Œæ™‚é–“: {overall_metrics['total_time_ms']:,} ms ({overall_metrics['total_time_ms']/1000:.2f} sec)")
print(f"ğŸ’¾ èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿: {overall_metrics['read_bytes']/1024/1024/1024:.2f} GB")
print(f"ğŸ“ˆ å‡ºåŠ›è¡Œæ•°: {overall_metrics['rows_produced_count']:,} è¡Œ")
print(f"ğŸ“‰ èª­ã¿è¾¼ã¿è¡Œæ•°: {overall_metrics['rows_read_count']:,} è¡Œ")
print(f"ğŸ¯ ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§: {bottleneck_indicators.get('data_selectivity', 0):.4f} ({bottleneck_indicators.get('data_selectivity', 0)*100:.2f}%)")
print(f"ğŸ”§ ã‚¹ãƒ†ãƒ¼ã‚¸æ•°: {len(extracted_metrics['stage_metrics'])}")
print(f"ğŸ—ï¸ ãƒãƒ¼ãƒ‰æ•°: {len(extracted_metrics['node_metrics'])}")

# Liquid Clusteringåˆ†æçµæœã®è¡¨ç¤º
liquid_analysis = extracted_metrics['liquid_clustering_analysis']
liquid_summary = liquid_analysis.get('summary', {})
print(f"ğŸ—‚ï¸ Liquid Clusteringå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {liquid_summary.get('tables_identified', 0)}")
print(f"ğŸ“Š é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {liquid_summary.get('high_impact_tables', 0)}")

# COMMAND ----------

# ğŸ“‹ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è©³ç´°è¡¨ç¤º
print("\n" + "=" * 50)
print("ğŸ” ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™è©³ç´°")
print("=" * 50)

# Photoné–¢é€£æŒ‡æ¨™
photon_enabled = overall_metrics.get('photon_enabled', False)
photon_utilization = overall_metrics.get('photon_utilization_ratio', 0) * 100
photon_emoji = "âœ…" if photon_enabled and photon_utilization > 80 else "âš ï¸" if photon_enabled else "âŒ"
print(f"{photon_emoji} Photonã‚¨ãƒ³ã‚¸ãƒ³: {'æœ‰åŠ¹' if photon_enabled else 'ç„¡åŠ¹'} (åˆ©ç”¨ç‡: {photon_utilization:.1f}%)")

# ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«é–¢é€£æŒ‡æ¨™
shuffle_count = bottleneck_indicators.get('shuffle_operations_count', 0)
has_shuffle_bottleneck = bottleneck_indicators.get('has_shuffle_bottleneck', False)
has_low_parallelism = bottleneck_indicators.get('has_low_parallelism', False)
low_parallelism_count = bottleneck_indicators.get('low_parallelism_stages_count', 0)

shuffle_emoji = "ğŸš¨" if has_shuffle_bottleneck else "âš ï¸" if shuffle_count > 5 else "âœ…"
print(f"{shuffle_emoji} ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ: {shuffle_count}å› ({'ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚ã‚Š' if has_shuffle_bottleneck else 'æ­£å¸¸'})")

parallelism_emoji = "ğŸš¨" if has_low_parallelism else "âœ…"
print(f"{parallelism_emoji} ä¸¦åˆ—åº¦: {'å•é¡Œã‚ã‚Š' if has_low_parallelism else 'é©åˆ‡'} (ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸: {low_parallelism_count}å€‹)")

print()
print("ğŸ“Š ãã®ä»–ã®æŒ‡æ¨™:")

for key, value in bottleneck_indicators.items():
    # æ–°ã—ãè¿½åŠ ã—ãŸæŒ‡æ¨™ã¯ä¸Šè¨˜ã§è¡¨ç¤ºæ¸ˆã¿ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
    if key in ['shuffle_operations_count', 'has_shuffle_bottleneck', 'has_low_parallelism', 
               'low_parallelism_stages_count', 'total_shuffle_time_ms', 'shuffle_time_ratio',
               'slowest_shuffle_duration_ms', 'slowest_shuffle_node', 'low_parallelism_details',
               'average_low_parallelism']:
        continue
        
    if 'ratio' in key:
        emoji = "ğŸ“Š" if value < 0.1 else "âš ï¸" if value < 0.3 else "ğŸš¨"
        print(f"{emoji} {key}: {value:.3f} ({value*100:.1f}%)")
    elif 'bytes' in key and key != 'has_spill':
        if value > 0:
            emoji = "ğŸ’¾" if value < 1024*1024*1024 else "âš ï¸"  # 1GBæœªæº€ã¯æ™®é€šã€ä»¥ä¸Šã¯æ³¨æ„
            print(f"{emoji} {key}: {value:,} bytes ({value/1024/1024:.2f} MB)")
    elif key == 'has_spill':
        emoji = "âŒ" if not value else "âš ï¸"
        print(f"{emoji} {key}: {'ã‚ã‚Š' if value else 'ãªã—'}")
    elif 'duration' in key:
        emoji = "â±ï¸"
        print(f"{emoji} {key}: {value:,} ms ({value/1000:.2f} sec)")
    else:
        emoji = "â„¹ï¸"
        print(f"{emoji} {key}: {value}")

print()

# COMMAND ----------

# ğŸ’¾ æŠ½å‡ºã—ãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
def convert_sets_to_lists(obj):
    """setå‹ã‚’listå‹ã«å¤‰æ›ã—ã¦JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ã«ã™ã‚‹"""
    if isinstance(obj, set):
        return list(obj)
    elif isinstance(obj, dict):
        return {key: convert_sets_to_lists(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_sets_to_lists(item) for item in obj]
    else:
        return obj

output_path = 'extracted_metrics.json'
try:
    # setå‹ã‚’listå‹ã«å¤‰æ›ã—ã¦ã‹ã‚‰JSONã«ä¿å­˜
    serializable_metrics = convert_sets_to_lists(extracted_metrics)
    with open(output_path, 'w', encoding='utf-8') as file:
        json.dump(serializable_metrics, file, indent=2, ensure_ascii=False)
    print(f"âœ… æŠ½å‡ºãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
except Exception as e:
    print(f"âš ï¸ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™: {e}")
    print("âœ… åˆ†æã¯æ­£å¸¸ã«ç¶™ç¶šã•ã‚Œã¾ã™")

# SparkDataFrameã¨ã—ã¦ã‚‚è¡¨ç¤º
if extracted_metrics['stage_metrics']:
    print("\nğŸ­ ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ¡ãƒˆãƒªã‚¯ã‚¹ (DataFrame)")
    print("=" * 40)
    try:
        stage_df = spark.createDataFrame(extracted_metrics['stage_metrics'])
        stage_df.show(truncate=False)
    except Exception as e:
        print(f"âš ï¸ SparkDataFrameè¡¨ç¤ºã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
        # ä»£æ›¿ã¨ã—ã¦Pandasã§è¡¨ç¤º
        import pandas as pd
        stage_pd_df = pd.DataFrame(extracted_metrics['stage_metrics'])
        print(stage_pd_df.to_string(index=False))

# ğŸŒ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10
print(f"\nğŸŒ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10")
print("=" * 80)
print("ğŸ“Š ã‚¢ã‚¤ã‚³ãƒ³èª¬æ˜: â±ï¸æ™‚é–“ ğŸ’¾ãƒ¡ãƒ¢ãƒª ğŸ”¥ğŸŒä¸¦åˆ—åº¦ ğŸ’¿ã‚¹ãƒ”ãƒ« âš–ï¸ã‚¹ã‚­ãƒ¥ãƒ¼")

# ãƒãƒ¼ãƒ‰ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
sorted_nodes = sorted(extracted_metrics['node_metrics'], 
                     key=lambda x: x['key_metrics'].get('durationMs', 0), 
                     reverse=True)

if sorted_nodes:
    # å…¨ä½“ã®å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
    total_duration = sum(node['key_metrics'].get('durationMs', 0) for node in sorted_nodes)
    
    print(f"ğŸ“Š å…¨ä½“å®Ÿè¡Œæ™‚é–“: {total_duration:,} ms ({total_duration/1000:.1f} sec)")
    print(f"ğŸ“ˆ TOP10åˆè¨ˆæ™‚é–“: {sum(node['key_metrics'].get('durationMs', 0) for node in sorted_nodes[:10]):,} ms")
    print()
    
    for i, node in enumerate(sorted_nodes[:10]):
        rows_num = node['key_metrics'].get('rowsNum', 0)
        duration_ms = node['key_metrics'].get('durationMs', 0)
        memory_mb = node['key_metrics'].get('peakMemoryBytes', 0) / 1024 / 1024
        
        # å…¨ä½“ã«å¯¾ã™ã‚‹æ™‚é–“ã®å‰²åˆã‚’è¨ˆç®—
        time_percentage = (duration_ms / max(total_duration, 1)) * 100
        
        # æ™‚é–“ã®é‡è¦åº¦ã«åŸºã¥ã„ã¦ã‚¢ã‚¤ã‚³ãƒ³ã‚’é¸æŠ
        if duration_ms >= 10000:  # 10ç§’ä»¥ä¸Š
            time_icon = "ï¿½"
            severity = "CRITICAL"
        elif duration_ms >= 5000:  # 5ç§’ä»¥ä¸Š
            time_icon = "ğŸŸ "
            severity = "HIGH"
        elif duration_ms >= 1000:  # 1ç§’ä»¥ä¸Š
            time_icon = "ğŸŸ¡"
            severity = "MEDIUM"
        else:
            time_icon = "ï¿½"
            severity = "LOW"
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ã‚¢ã‚¤ã‚³ãƒ³
        memory_icon = "ï¿½" if memory_mb < 100 else "âš ï¸" if memory_mb < 1000 else "ğŸš¨"
        
        # ãƒãƒ¼ãƒ‰åã‚’çŸ­ç¸®ï¼ˆ100ãƒã‚¤ãƒˆã¾ã§ï¼‰
        node_name = node['name']
        short_name = node_name[:100] + "..." if len(node_name) > 100 else node_name
        
        # ä¸¦åˆ—åº¦æƒ…å ±ã®å–å¾—
        num_tasks = 0
        for stage in extracted_metrics.get('stage_metrics', []):
            if duration_ms > 0:  # ã“ã®ãƒãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’æ¨å®š
                num_tasks = max(num_tasks, stage.get('num_tasks', 0))
        
        # ãƒ‡ã‚£ã‚¹ã‚¯ã‚¹ãƒ”ãƒ«ã‚¢ã‚¦ãƒˆã®æ¤œå‡º
        detailed_metrics = node.get('detailed_metrics', {})
        spill_detected = False
        spill_bytes = 0
        for metric_key, metric_info in detailed_metrics.items():
            if 'SPILL' in metric_key.upper() or 'DISK' in metric_key.upper():
                metric_value = metric_info.get('value', 0)
                if metric_value > 0:
                    spill_detected = True
                    spill_bytes += metric_value
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ã®æ¤œå‡ºï¼ˆè¡Œæ•°ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‹ã‚‰æ¨å®šï¼‰
        skew_detected = False
        if rows_num > 0 and memory_mb > 0:
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒè¡Œæ•°ã«æ¯”ã¹ã¦ç•°å¸¸ã«é«˜ã„å ´åˆã¯ã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§
            memory_per_row = memory_mb * 1024 * 1024 / rows_num  # bytes per row
            if memory_per_row > 10000:  # 1è¡Œã‚ãŸã‚Š10KBä»¥ä¸Šã¯é«˜ã„
                skew_detected = True
        
        # ã¾ãŸã¯å®Ÿè¡Œæ™‚é–“ãŒè¡Œæ•°ã«æ¯”ã¹ã¦ç•°å¸¸ã«é•·ã„å ´åˆ
        if rows_num > 0 and duration_ms > 0:
            ms_per_thousand_rows = (duration_ms * 1000) / rows_num
            if ms_per_thousand_rows > 1000:  # 1000è¡Œã‚ãŸã‚Š1ç§’ä»¥ä¸Šã¯é…ã„
                skew_detected = True
        
        # ä¸¦åˆ—åº¦ã‚¢ã‚¤ã‚³ãƒ³
        parallelism_icon = "ğŸ”¥" if num_tasks >= 10 else "âš ï¸" if num_tasks >= 5 else "ğŸŒ"
        # ã‚¹ãƒ”ãƒ«ã‚¢ã‚¤ã‚³ãƒ³
        spill_icon = "ğŸ’¿" if spill_detected else "âœ…"
        # ã‚¹ã‚­ãƒ¥ãƒ¼ã‚¢ã‚¤ã‚³ãƒ³
        skew_icon = "âš–ï¸" if skew_detected else "âœ…"
        
        print(f"{i+1:2d}. {time_icon}{memory_icon}{parallelism_icon}{spill_icon}{skew_icon} [{severity:8}] {short_name}")
        print(f"    â±ï¸  å®Ÿè¡Œæ™‚é–“: {duration_ms:>8,} ms ({duration_ms/1000:>6.1f} sec) - å…¨ä½“ã® {time_percentage:>5.1f}%")
        print(f"    ğŸ“Š å‡¦ç†è¡Œæ•°: {rows_num:>8,} è¡Œ")
        print(f"    ğŸ’¾ ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: {memory_mb:>6.1f} MB")
        print(f"    ğŸ”§ ä¸¦åˆ—åº¦: {num_tasks:>3d} ã‚¿ã‚¹ã‚¯ | ğŸ’¿ ã‚¹ãƒ”ãƒ«: {'ã‚ã‚Š' if spill_detected else 'ãªã—'} | âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼: {'ã‚ã‚Š' if skew_detected else 'ãªã—'}")
        
        # åŠ¹ç‡æ€§æŒ‡æ¨™ï¼ˆè¡Œ/ç§’ï¼‰ã‚’è¨ˆç®—
        if duration_ms > 0:
            rows_per_sec = (rows_num * 1000) / duration_ms
            print(f"    ğŸš€ å‡¦ç†åŠ¹ç‡: {rows_per_sec:>8,.0f} è¡Œ/ç§’")
        
        # ã‚¹ãƒ”ãƒ«è©³ç´°æƒ…å ±
        if spill_detected and spill_bytes > 0:
            print(f"    ğŸ’¿ ã‚¹ãƒ”ãƒ«è©³ç´°: {spill_bytes/1024/1024:.1f} MB")
        
        # ãƒãƒ¼ãƒ‰IDã‚‚è¡¨ç¤º
        print(f"    ğŸ†” ãƒãƒ¼ãƒ‰ID: {node.get('node_id', 'N/A')}")
        print()
        
else:
    print("âš ï¸ ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

print()

# COMMAND ----------

# ğŸ—‚ï¸ Liquid Clusteringåˆ†æçµæœã®è©³ç´°è¡¨ç¤º
print("\n" + "=" * 50)
print("ğŸ—‚ï¸ Liquid Clusteringæ¨å¥¨åˆ†æ")
print("=" * 50)

liquid_analysis = extracted_metrics['liquid_clustering_analysis']

# æ¨å¥¨ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
recommended_tables = liquid_analysis.get('recommended_tables', {})
if recommended_tables:
    print("\nğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚«ãƒ©ãƒ :")
    for table_name, table_info in recommended_tables.items():
        clustering_cols = table_info.get('clustering_columns', [])
        scan_perf = table_info.get('scan_performance', {})
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«åã®è¡¨ç¤º
        print(f"\nğŸ“Š ãƒ†ãƒ¼ãƒ–ãƒ«: {table_name}")
        print(f"   ğŸ¯ æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚«ãƒ©ãƒ : {', '.join(clustering_cols)}")
        print(f"   ğŸ“ˆ ã‚¹ã‚­ãƒ£ãƒ³è¡Œæ•°: {scan_perf.get('rows_scanned', 0):,} è¡Œ")
        print(f"   â±ï¸ ã‚¹ã‚­ãƒ£ãƒ³æ™‚é–“: {scan_perf.get('scan_duration_ms', 0):,} ms")
        print(f"   ğŸš€ ã‚¹ã‚­ãƒ£ãƒ³åŠ¹ç‡: {scan_perf.get('efficiency_score', 0):.2f} è¡Œ/ms")
        
        # ã‚«ãƒ©ãƒ ã‚¹ã‚³ã‚¢è©³ç´°
        column_scores = table_info.get('column_scores', {})
        if column_scores:
            sorted_scores = sorted(column_scores.items(), key=lambda x: x[1], reverse=True)
            print(f"   ğŸ“Š ã‚«ãƒ©ãƒ é‡è¦åº¦: {', '.join([f'{col}({score})' for col, score in sorted_scores[:3]])}")

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å½±éŸ¿åˆ†æ
performance_impact = liquid_analysis.get('performance_impact', {})
print(f"\nğŸ”„ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šè¦‹è¾¼ã¿:")
print(f"   ğŸ“ˆ ã‚¹ã‚­ãƒ£ãƒ³æ”¹å–„: {performance_impact.get('potential_scan_improvement', 'N/A')}")
print(f"   ğŸ”€ Shuffleå‰Šæ¸›: {performance_impact.get('potential_shuffle_reduction', 'N/A')}")
print(f"   ğŸ† å…¨ä½“æ”¹å–„: {performance_impact.get('estimated_overall_improvement', 'N/A')}")

# ã‚«ãƒ©ãƒ ä½¿ç”¨çµ±è¨ˆï¼ˆè©³ç´°ç‰ˆï¼‰
filter_cols = set(liquid_analysis.get('filter_columns', []))
join_cols = set(liquid_analysis.get('join_columns', []))
groupby_cols = set(liquid_analysis.get('groupby_columns', []))
detailed_column_analysis = liquid_analysis.get('detailed_column_analysis', {})

if filter_cols or join_cols or groupby_cols:
    print(f"\nğŸ” ã‚«ãƒ©ãƒ ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³:")
    if filter_cols:
        print(f"   ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚«ãƒ©ãƒ  ({len(filter_cols)}å€‹): {', '.join(list(filter_cols)[:5])}")
    if join_cols:
        print(f"   ğŸ”— JOINã‚«ãƒ©ãƒ  ({len(join_cols)}å€‹): {', '.join(list(join_cols)[:5])}")
    if groupby_cols:
        print(f"   ğŸ“Š GROUP BYã‚«ãƒ©ãƒ  ({len(groupby_cols)}å€‹): {', '.join(list(groupby_cols)[:5])}")

# é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚«ãƒ©ãƒ ã®è©³ç´°è¡¨ç¤º
high_impact_columns = {col: analysis for col, analysis in detailed_column_analysis.items() 
                      if analysis.get('performance_impact') == 'high'}

if high_impact_columns:
    print(f"\nâ­ é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚«ãƒ©ãƒ è©³ç´°:")
    for col, analysis in list(high_impact_columns.items())[:5]:
        usage_contexts = ', '.join(analysis.get('usage_contexts', []))
        total_usage = analysis.get('total_usage', 0)
        print(f"   ğŸ¯ {col}")
        print(f"      ğŸ“ˆ é‡è¦åº¦ã‚¹ã‚³ã‚¢: {total_usage} | ä½¿ç”¨ç®‡æ‰€: {usage_contexts}")
        print(f"      ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼:{analysis.get('filter_usage_count', 0)} | JOIN:{analysis.get('join_usage_count', 0)} | GROUP BY:{analysis.get('groupby_usage_count', 0)}")

# ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æƒ…å ±
pushdown_filters = liquid_analysis.get('pushdown_filters', [])
if pushdown_filters:
    print(f"\nğŸ” ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ ({len(pushdown_filters)}ä»¶):")
    for i, filter_info in enumerate(pushdown_filters[:3]):
        node_name_display = filter_info.get('node_name', '')
        truncated_node_name = node_name_display[:100] + "..." if len(node_name_display) > 100 else node_name_display
        print(f"   {i+1}. ãƒãƒ¼ãƒ‰: {truncated_node_name}")
        filter_expression = filter_info.get('filter_expression', '')
        truncated_filter = filter_expression[:128] + "..." if len(filter_expression) > 128 else filter_expression
        print(f"      ğŸ“‹ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {truncated_filter}")
        print(f"      ğŸ”§ ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {filter_info.get('metric_key', '')}")

# SQLå®Ÿè£…ä¾‹
if recommended_tables:
    print(f"\nğŸ’¡ å®Ÿè£…ä¾‹:")
    for table_name, table_info in list(recommended_tables.items())[:2]:  # ä¸Šä½2ãƒ†ãƒ¼ãƒ–ãƒ«
        clustering_cols = table_info.get('clustering_columns', [])
        if clustering_cols:
            cluster_by_clause = ', '.join(clustering_cols)
            print(f"   ALTER TABLE {table_name} CLUSTER BY ({cluster_by_clause});")

print()

# COMMAND ----------

# ğŸ¤– Databricks Claude 3.7 Sonnetã‚’ä½¿ç”¨ã—ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ
print("ğŸ¤– Claude 3.7 Sonnetã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
print("âš ï¸  Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ 'databricks-claude-3-7-sonnet' ãŒå¿…è¦ã§ã™")
print("ğŸ“ åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç°¡æ½”åŒ–ã—ã¦ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒªã‚¹ã‚¯ã‚’è»½æ¸›ã—ã¦ã„ã¾ã™...")
print()

analysis_result = analyze_bottlenecks_with_claude(extracted_metrics)

# COMMAND ----------

# ğŸ“Š åˆ†æçµæœã®è¡¨ç¤º
print("\n" + "=" * 80)
print("ğŸ¯ ã€Databricks Claude 3.7 Sonnet ã«ã‚ˆã‚‹ SQLãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã€‘")
print("=" * 80)
print()
print(analysis_result)
print()
print("=" * 80)

# COMMAND ----------

# ğŸ’¾ åˆ†æçµæœã®ä¿å­˜ã¨å®Œäº†ã‚µãƒãƒªãƒ¼
result_output_path = 'bottleneck_analysis_result.txt'
with open(result_output_path, 'w', encoding='utf-8') as file:
    file.write("Databricks SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ\n")
    file.write("=" * 60 + "\n\n")
    file.write(f"ã‚¯ã‚¨ãƒªID: {extracted_metrics['query_info']['query_id']}\n")
    file.write(f"åˆ†ææ—¥æ™‚: {pd.Timestamp.now()}\n")
    file.write(f"å®Ÿè¡Œæ™‚é–“: {extracted_metrics['overall_metrics']['total_time_ms']:,} ms\n")
    file.write("=" * 60 + "\n\n")
    file.write(analysis_result)
print(f"âœ… åˆ†æçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {result_output_path}")

# æœ€çµ‚çš„ãªã‚µãƒãƒªãƒ¼
print("\n" + "ğŸ‰" * 20)
print("ğŸ ã€å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼ã€‘")
print("ğŸ‰" * 20)
print("âœ… SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºå®Œäº† (extracted_metrics.json)")
print("âœ… Databricks Claude 3.7 Sonnetã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Œäº†")
print("âœ… åˆ†æçµæœä¿å­˜å®Œäº† (bottleneck_analysis_result.txt)")
print()
print("ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
print(f"   ğŸ“„ {output_path}")
print(f"   ğŸ“„ {result_output_path}")
print()
print("ğŸš€ åˆ†æå®Œäº†ï¼çµæœã‚’ç¢ºèªã—ã¦ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã«ãŠå½¹ç«‹ã¦ãã ã•ã„ã€‚")
print("ğŸ‰" * 20)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“š è¿½åŠ ã®ä½¿ç”¨æ–¹æ³•ã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
# MAGIC 
# MAGIC ### ğŸ”§ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ–¹æ³•
# MAGIC 
# MAGIC #### æ–¹æ³• 1: Databricks UI ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# MAGIC 1. **Data** > **Create Table** ã‚’ã‚¯ãƒªãƒƒã‚¯
# MAGIC 2. **Upload File** ã‚’é¸æŠ
# MAGIC 3. SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—
# MAGIC 4. ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†å¾Œã€ãƒ‘ã‚¹ã‚’ã‚³ãƒ”ãƒ¼
# MAGIC 5. ä¸Šè¨˜ã® `JSON_FILE_PATH` ã«è¨­å®š
# MAGIC 
# MAGIC #### æ–¹æ³• 2: dbutils ã‚’ä½¿ç”¨
# MAGIC ```python
# MAGIC # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’FileStoreã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# MAGIC dbutils.fs.cp("file:/local/path/profiler.json", "dbfs:/FileStore/profiler.json")
# MAGIC 
# MAGIC # å¤–éƒ¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰ã®ã‚³ãƒ”ãƒ¼
# MAGIC dbutils.fs.cp("s3a://bucket/profiler.json", "dbfs:/FileStore/profiler.json")
# MAGIC ```
# MAGIC 
# MAGIC ### ğŸ›ï¸ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆ
# MAGIC 
# MAGIC - **ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º**: `extract_performance_metrics` é–¢æ•°å†…ã®é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
# MAGIC - **åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: `analyze_bottlenecks_with_claude` é–¢æ•°å†…ã®åˆ†ææŒ‡ç¤º
# MAGIC - **è¡¨ç¤ºå½¢å¼**: emoji ã¨å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®èª¿æ•´
# MAGIC 
# MAGIC ### ğŸ” ã‚¨ãƒ©ãƒ¼å¯¾å‡¦æ–¹æ³•
# MAGIC 
# MAGIC 1. **Claude ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼**: Model Serving ã§ `databricks-claude-3-7-sonnet` ãŒç¨¼åƒä¸­ã‹ç¢ºèª
# MAGIC 2. **ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼**: `dbutils.fs.ls("/FileStore/")` ã§ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ã‚’ç¢ºèª
# MAGIC 3. **ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼**: å¤§ããªJSONãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ã‚¯ãƒ©ã‚¹ã‚¿ã®ãƒ¡ãƒ¢ãƒªè¨­å®šã‚’ç¢ºèª
# MAGIC 
# MAGIC ### ğŸ’¡ é«˜åº¦ãªä½¿ç”¨ä¾‹
# MAGIC 
# MAGIC ```python
# MAGIC # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ‹¬åˆ†æ
# MAGIC profiler_files = dbutils.fs.ls("/FileStore/profiler_logs/")
# MAGIC for file_info in profiler_files:
# MAGIC     if file_info.path.endswith('.json'):
# MAGIC         profiler_data = load_profiler_json(file_info.path)
# MAGIC         metrics = extract_performance_metrics(profiler_data)
# MAGIC         # åˆ†æå‡¦ç†...
# MAGIC ```

# COMMAND ----------

# MAGIC %md
# MAGIC ---
# MAGIC 
# MAGIC ## ğŸ¯ ã“ã®Notebookã®ä½¿ç”¨æ–¹æ³•
# MAGIC 
# MAGIC 1. **ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®š**: Model Serving ã§ `databricks-claude-3-7-sonnet` ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆ
# MAGIC 2. **ãƒ•ã‚¡ã‚¤ãƒ«æº–å‚™**: SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’Volumesã€FileStoreã€ã¾ãŸã¯DBFSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# MAGIC 3. **ãƒ‘ã‚¹è¨­å®š**: ã‚»ãƒ«2ã§ `JSON_FILE_PATH` ã‚’å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›´
# MAGIC 4. **å®Ÿè¡Œ**: ã€ŒRun Allã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã¾ãŸã¯å„ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ
# MAGIC 5. **çµæœç¢ºèª**: æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨AIåˆ†æçµæœã‚’ç¢ºèª
# MAGIC 
# MAGIC **ğŸ“§ ã‚µãƒãƒ¼ãƒˆ**: å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨Databricksç’°å¢ƒæƒ…å ±ã‚’ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚