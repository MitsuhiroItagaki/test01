# Databricks notebook source
# MAGIC %md
# MAGIC ## ğŸ“ ã‚»ãƒ«1: åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
# MAGIC 
# MAGIC **æœ€åˆã«ã€åˆ†æå¯¾è±¡ã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚**
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®è¨­å®šã‚’è¡Œã„ã¾ã™ï¼š
# MAGIC - ğŸ“‚ SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹è¨­å®š
# MAGIC - ğŸ“‹ å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å½¢å¼ã®ä¾‹
# MAGIC - âš™ï¸ åŸºæœ¬çš„ãªç’°å¢ƒè¨­å®š

# COMMAND ----------

# ğŸ“ SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹è¨­å®š
# 
# ä»¥ä¸‹ã®JSON_FILE_PATHã‚’å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›´ã—ã¦ãã ã•ã„ï¼š

JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/simple0.json'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«

# ğŸ“‹ å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å½¢å¼ã®ä¾‹:
# Unity Catalog Volumes:
# JSON_FILE_PATH = '/Volumes/catalog/schema/volume/profiler.json'
# 
# FileStore (æ¨å¥¨):
# JSON_FILE_PATH = '/FileStore/shared_uploads/your_username/profiler_log.json'
# 
# DBFS:
# JSON_FILE_PATH = '/dbfs/FileStore/shared_uploads/your_username/profiler_log.json'
# 
# DBFS URI:
# JSON_FILE_PATH = 'dbfs:/FileStore/shared_uploads/your_username/profiler_log.json'

print("ğŸ“ ã€åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šå®Œäº†ã€‘")
print("=" * 50)
print(f"ğŸ“„ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {JSON_FILE_PATH}")
print("=" * 50)

# âš™ï¸ åŸºæœ¬çš„ãªç’°å¢ƒè¨­å®š
import json
import pandas as pd
from typing import Dict, List, Any
from datetime import datetime

print("âœ… åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†")
print("ğŸš€ æ¬¡ã®ã‚»ãƒ«ã«é€²ã‚“ã§ãã ã•ã„")

# COMMAND ----------

# MAGIC %md
# MAGIC # Databricks SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆ†æãƒ„ãƒ¼ãƒ«
# MAGIC 
# MAGIC ã“ã®notebookã¯ã€Databricksã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šã¨æ”¹å–„æ¡ˆã®æç¤ºã«å¿…è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã¦åˆ†æã‚’è¡Œã„ã¾ã™ã€‚
# MAGIC 
# MAGIC ## æ©Ÿèƒ½æ¦‚è¦
# MAGIC 
# MAGIC 1. **SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿**
# MAGIC    - Databricksã§å‡ºåŠ›ã•ã‚ŒãŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ­ã‚°ã®è§£æ
# MAGIC    - `graphs`ã‚­ãƒ¼ã«æ ¼ç´ã•ã‚ŒãŸå®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º
# MAGIC 
# MAGIC 2. **é‡è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º**
# MAGIC    - ã‚¯ã‚¨ãƒªåŸºæœ¬æƒ…å ±ï¼ˆIDã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã€å®Ÿè¡Œæ™‚é–“ãªã©ï¼‰
# MAGIC    - å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆå®Ÿè¡Œæ™‚é–“ã€ãƒ‡ãƒ¼ã‚¿é‡ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ãªã©ï¼‰
# MAGIC    - ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ»ãƒãƒ¼ãƒ‰è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹
# MAGIC    - ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è¨ˆç®—
# MAGIC 
# MAGIC 3. **AI ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ**
# MAGIC    - è¨­å®šå¯èƒ½ãªLLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ (Databricks, OpenAI, Azure OpenAI, Anthropic)
# MAGIC    - æŠ½å‡ºãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
# MAGIC    - å…·ä½“çš„ãªæ”¹å–„æ¡ˆã®æç¤º
# MAGIC 
# MAGIC ---
# MAGIC 
# MAGIC **äº‹å‰æº–å‚™:**
# MAGIC - LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¨­å®šï¼ˆDatabricks Model Serving ã¾ãŸã¯ å¤–éƒ¨APIï¼‰
# MAGIC - å¿…è¦ãªAPIã‚­ãƒ¼ã®è¨­å®š
# MAGIC - SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®æº–å‚™ï¼ˆDBFS ã¾ãŸã¯ FileStoreï¼‰
# MAGIC 
# MAGIC ---
# MAGIC 
# MAGIC ## ğŸ“‹ ã‚»ãƒ«ç›®æ¬¡
# MAGIC 
# MAGIC ### ğŸ”§ è¨­å®šãƒ»æº–å‚™ã‚»ãƒ«
# MAGIC - **ã‚»ãƒ«1**: åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šï¼ˆå®Ÿè¡Œå‰ã«å¿…ãšè¨­å®šï¼‰
# MAGIC - **ã‚»ãƒ«2**: LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®š
# MAGIC - **ã‚»ãƒ«3**: SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°
# MAGIC 
# MAGIC ### ğŸ“Š åˆ†æé–¢æ•°å®šç¾©ã‚»ãƒ«
# MAGIC - **ã‚»ãƒ«4**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºé–¢æ•°
# MAGIC - **ã‚»ãƒ«5**: ãƒãƒ¼ãƒ‰åè§£æãƒ»æ”¹å–„é–¢æ•°
# MAGIC - **ã‚»ãƒ«6**: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™è¨ˆç®—é–¢æ•°
# MAGIC - **ã‚»ãƒ«7**: Liquid Clusteringåˆ†æé–¢æ•°
# MAGIC - **ã‚»ãƒ«8**: LLMã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æé–¢æ•°
# MAGIC - **ã‚»ãƒ«9**: å€‹åˆ¥LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ¥ç¶šé–¢æ•°
# MAGIC 
# MAGIC ### ğŸš€ ãƒ¡ã‚¤ãƒ³å‡¦ç†å®Ÿè¡Œã‚»ãƒ«
# MAGIC - **ã‚»ãƒ«10**: SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Ÿè¡Œ
# MAGIC - **ã‚»ãƒ«11**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºã¨æ¦‚è¦è¡¨ç¤º
# MAGIC - **ã‚»ãƒ«12**: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™è©³ç´°è¡¨ç¤ºã¨æ™‚é–“æ¶ˆè²»TOP10
# MAGIC - **ã‚»ãƒ«13**: LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Ÿè¡Œã®æº–å‚™
# MAGIC - **ã‚»ãƒ«14**: LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã®è¡¨ç¤º
# MAGIC - **ã‚»ãƒ«15**: åˆ†æçµæœã®ä¿å­˜ã¨å®Œäº†ã‚µãƒãƒªãƒ¼
# MAGIC 
# MAGIC ### ğŸ”§ SQLæœ€é©åŒ–æ©Ÿèƒ½ã‚»ãƒ«
# MAGIC - **ã‚»ãƒ«16**: SQLæœ€é©åŒ–é–¢é€£é–¢æ•°å®šç¾©
# MAGIC - **ã‚»ãƒ«17**: SQLã‚¯ã‚¨ãƒªæœ€é©åŒ–ã®å®Ÿè¡Œï¼ˆã‚¹ãƒ†ãƒƒãƒ—1: ã‚¯ã‚¨ãƒªæŠ½å‡ºï¼‰
# MAGIC - **ã‚»ãƒ«18**: LLMã«ã‚ˆã‚‹SQLæœ€é©åŒ–ï¼ˆã‚¹ãƒ†ãƒƒãƒ—2: æœ€é©åŒ–å®Ÿè¡Œï¼‰
# MAGIC - **ã‚»ãƒ«19**: æœ€é©åŒ–çµæœã®ä¿å­˜ï¼ˆã‚¹ãƒ†ãƒƒãƒ—3: ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼‰
# MAGIC - **ã‚»ãƒ«20**: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã®æº–å‚™ï¼ˆã‚¹ãƒ†ãƒƒãƒ—4: å®Ÿè¡Œã‚¬ã‚¤ãƒ‰ï¼‰
# MAGIC - **ã‚»ãƒ«21**: æœ€çµ‚å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼
# MAGIC 
# MAGIC ### ğŸ“š ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã‚»ãƒ«
# MAGIC - **ã‚»ãƒ«22**: è¿½åŠ ã®ä½¿ç”¨æ–¹æ³•ã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
# MAGIC - **ã‚»ãƒ«23**: ã“ã®Notebookã®ä½¿ç”¨æ–¹æ³•

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¤– ã‚»ãƒ«2: LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®š
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®è¨­å®šã‚’è¡Œã„ã¾ã™ï¼š
# MAGIC - LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®é¸æŠï¼ˆDatabricks/OpenAI/Azure/Anthropicï¼‰
# MAGIC - å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æ¥ç¶šè¨­å®š
# MAGIC - å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# COMMAND ----------

# ğŸ¤– LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®š
LLM_CONFIG = {
    # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚¿ã‚¤ãƒ—: 'databricks', 'openai', 'azure_openai', 'anthropic'
    "provider": "databricks",
    
    # Databricks Model Servingè¨­å®š
    "databricks": {
        "endpoint_name": "databricks-claude-3-7-sonnet",  # Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå
        "max_tokens": 2000,
        "temperature": 0.1,
        "thinking_enabled": True  # æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰ï¼ˆthinking: {"type": "enabled"}ï¼‰
    },
    
    # OpenAIè¨­å®š
    "openai": {
        "api_key": "",  # OpenAI APIã‚­ãƒ¼ (ç’°å¢ƒå¤‰æ•°OPENAI_API_KEYã§ã‚‚å¯)
        "model": "gpt-4o",  # gpt-4o, gpt-4-turbo, gpt-3.5-turbo
        "max_tokens": 2000,
        "temperature": 0.1
    },
    
    # Azure OpenAIè¨­å®š
    "azure_openai": {
        "api_key": "",  # Azure OpenAI APIã‚­ãƒ¼ (ç’°å¢ƒå¤‰æ•°AZURE_OPENAI_API_KEYã§ã‚‚å¯)
        "endpoint": "",  # https://your-resource.openai.azure.com/
        "deployment_name": "",  # ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå
        "api_version": "2024-02-01",
        "max_tokens": 2000,
        "temperature": 0.1
    },
    
    # Anthropicè¨­å®š
    "anthropic": {
        "api_key": "",  # Anthropic APIã‚­ãƒ¼ (ç’°å¢ƒå¤‰æ•°ANTHROPIC_API_KEYã§ã‚‚å¯)
        "model": "claude-3-5-sonnet-20241022",  # claude-3-5-sonnet-20241022, claude-3-opus-20240229
        "max_tokens": 2000,
        "temperature": 0.1
    }
}

print("ğŸ¤– LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®šå®Œäº†")
print(f"ğŸ¤– LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {LLM_CONFIG['provider']}")

if LLM_CONFIG['provider'] == 'databricks':
    print(f"ğŸ”— Databricksã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: {LLM_CONFIG['databricks']['endpoint_name']}")
    thinking_status = "æœ‰åŠ¹" if LLM_CONFIG['databricks'].get('thinking_enabled', True) else "ç„¡åŠ¹"
    print(f"ğŸ§  æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰: {thinking_status}")
elif LLM_CONFIG['provider'] == 'openai':
    print(f"ğŸ”— OpenAIãƒ¢ãƒ‡ãƒ«: {LLM_CONFIG['openai']['model']}")
elif LLM_CONFIG['provider'] == 'azure_openai':
    print(f"ğŸ”— Azure OpenAIãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ: {LLM_CONFIG['azure_openai']['deployment_name']}")
elif LLM_CONFIG['provider'] == 'anthropic':
    print(f"ğŸ”— Anthropicãƒ¢ãƒ‡ãƒ«: {LLM_CONFIG['anthropic']['model']}")

print()
print("ğŸ’¡ LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ‡ã‚Šæ›¿ãˆä¾‹:")
print('   LLM_CONFIG["provider"] = "openai"      # OpenAI GPT-4ã«åˆ‡ã‚Šæ›¿ãˆ')
print('   LLM_CONFIG["provider"] = "anthropic"   # Anthropic Claudeã«åˆ‡ã‚Šæ›¿ãˆ')
print('   LLM_CONFIG["provider"] = "azure_openai" # Azure OpenAIã«åˆ‡ã‚Šæ›¿ãˆ')
print()
print("ğŸ§  Databricksæ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰è¨­å®šä¾‹:")
print('   LLM_CONFIG["databricks"]["thinking_enabled"] = True   # æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹')
print('   LLM_CONFIG["databricks"]["thinking_enabled"] = False  # æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰ç„¡åŠ¹')
print()

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import requests
import os
from pyspark.sql import SparkSession

# PySparké–¢æ•°ã‚’å®‰å…¨ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from pyspark.sql.functions import col, lit, when
    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType
    print("âœ… PySparké–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†")
except ImportError as e:
    print(f"âš ï¸ PySparké–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
    # åŸºæœ¬çš„ãªåˆ†æã«ã¯å½±éŸ¿ã—ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—

# Databricksç’°å¢ƒã®ç¢ºèª
spark = SparkSession.builder.getOrCreate()
print(f"âœ… Spark Version: {spark.version}")

# Databricks Runtimeæƒ…å ±ã‚’å®‰å…¨ã«å–å¾—
try:
    runtime_version = spark.conf.get('spark.databricks.clusterUsageTags.sparkVersion')
    print(f"âœ… Databricks Runtime: {runtime_version}")
except Exception:
    try:
        # ä»£æ›¿æ‰‹æ®µã§DBRæƒ…å ±ã‚’å–å¾—
        dbr_version = spark.conf.get('spark.databricks.clusterUsageTags.clusterName', 'Unknown')
        print(f"âœ… Databricks Cluster: {dbr_version}")
    except Exception:
        print("âœ… Databricks Environment: è¨­å®šæƒ…å ±ã®å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“‚ ã‚»ãƒ«3: SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
# MAGIC - DBFS/FileStore/ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®è‡ªå‹•åˆ¤åˆ¥
# MAGIC - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã®è¡¨ç¤º

# COMMAND ----------

def load_profiler_json(file_path: str) -> Dict[str, Any]:
    """
    SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        file_path: JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆDBFS ã¾ãŸã¯ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ï¼‰
        
    Returns:
        Dict: ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸJSONãƒ‡ãƒ¼ã‚¿
    """
    try:
        # DBFSãƒ‘ã‚¹ã®å ´åˆã¯é©åˆ‡ã«å‡¦ç†
        if file_path.startswith('/dbfs/'):
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        elif file_path.startswith('dbfs:/'):
            # dbfs: ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’/dbfs/ã«å¤‰æ›
            local_path = file_path.replace('dbfs:', '/dbfs')
            with open(local_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        elif file_path.startswith('/FileStore/'):
            # FileStore ãƒ‘ã‚¹ã‚’ /dbfs/FileStore/ ã«å¤‰æ›
            local_path = '/dbfs' + file_path
            with open(local_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        
        print(f"âœ… JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {file_path}")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(str(data)):,} characters")
        return data
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {}

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: load_profiler_json")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“Š ã‚»ãƒ«4: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºé–¢æ•°
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º
# MAGIC - ã‚¯ã‚¨ãƒªåŸºæœ¬æƒ…å ±ã®å–å¾—
# MAGIC - å…¨ä½“/ã‚¹ãƒ†ãƒ¼ã‚¸/ãƒãƒ¼ãƒ‰åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®è¨ˆç®—
# MAGIC - ã‚¹ãƒ”ãƒ«æ¤œå‡ºã¨ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®åˆ†æ

# COMMAND ----------

def extract_performance_metrics(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã«å¿…è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
    """
    metrics = {
        "query_info": {},
        "overall_metrics": {},
        "stage_metrics": [],
        "node_metrics": [],
        "bottleneck_indicators": {},
        "liquid_clustering_analysis": {}
    }
    
    # åŸºæœ¬çš„ãªã‚¯ã‚¨ãƒªæƒ…å ±
    if 'query' in profiler_data:
        query = profiler_data['query']
        metrics["query_info"] = {
            "query_id": query.get('id', ''),
            "status": query.get('status', ''),
            "query_start_time": query.get('queryStartTimeMs', 0),
            "query_end_time": query.get('queryEndTimeMs', 0),
            "user": query.get('user', {}).get('displayName', ''),
            "query_text": query.get('queryText', '')[:300] + "..." if len(query.get('queryText', '')) > 300 else query.get('queryText', '')
        }
        
        # å…¨ä½“çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if 'metrics' in query:
            query_metrics = query['metrics']
            metrics["overall_metrics"] = {
                "total_time_ms": query_metrics.get('totalTimeMs', 0),
                "compilation_time_ms": query_metrics.get('compilationTimeMs', 0),
                "execution_time_ms": query_metrics.get('executionTimeMs', 0),
                "read_bytes": query_metrics.get('readBytes', 0),
                "read_remote_bytes": query_metrics.get('readRemoteBytes', 0),
                "read_cache_bytes": query_metrics.get('readCacheBytes', 0),
                "rows_produced_count": query_metrics.get('rowsProducedCount', 0),
                "rows_read_count": query_metrics.get('rowsReadCount', 0),
                "spill_to_disk_bytes": query_metrics.get('spillToDiskBytes', 0),
                "read_files_count": query_metrics.get('readFilesCount', 0),
                "task_total_time_ms": query_metrics.get('taskTotalTimeMs', 0),
                "photon_total_time_ms": query_metrics.get('photonTotalTimeMs', 0),
                # Photonåˆ©ç”¨çŠ¶æ³ã®åˆ†æï¼ˆPhotonå®Ÿè¡Œæ™‚é–“/ã‚¿ã‚¹ã‚¯åˆè¨ˆæ™‚é–“ï¼‰
                "photon_enabled": query_metrics.get('photonTotalTimeMs', 0) > 0,
                "photon_utilization_ratio": min(query_metrics.get('photonTotalTimeMs', 0) / max(query_metrics.get('taskTotalTimeMs', 1), 1), 1.0)
            }
    
    # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¹ãƒ†ãƒ¼ã‚¸ã¨ãƒãƒ¼ãƒ‰ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
    if 'graphs' in profiler_data and profiler_data['graphs']:
        graph = profiler_data['graphs'][0]
        
        # ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿
        if 'stageData' in graph:
            for stage in graph['stageData']:
                stage_metric = {
                    "stage_id": stage.get('stageId', ''),
                    "status": stage.get('status', ''),
                    "duration_ms": stage.get('keyMetrics', {}).get('durationMs', 0),
                    "num_tasks": stage.get('numTasks', 0),
                    "num_failed_tasks": stage.get('numFailedTasks', 0),
                    "num_complete_tasks": stage.get('numCompleteTasks', 0),
                    "start_time_ms": stage.get('startTimeMs', 0),
                    "end_time_ms": stage.get('endTimeMs', 0)
                }
                metrics["stage_metrics"].append(stage_metric)
        
        # ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ï¼ˆé‡è¦ãªã‚‚ã®ã®ã¿ï¼‰
        if 'nodes' in graph:
            for node in graph['nodes']:
                if not node.get('hidden', False):
                    node_metric = {
                        "node_id": node.get('id', ''),
                        "name": node.get('name', ''),
                        "tag": node.get('tag', ''),
                        "key_metrics": node.get('keyMetrics', {})
                    }
                    
                    # é‡è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã¿è©³ç´°æŠ½å‡ºï¼ˆã‚¹ãƒ”ãƒ«é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿½åŠ ãƒ»labelå¯¾å¿œï¼‰
                    detailed_metrics = {}
                    for metric in node.get('metrics', []):
                        metric_key = metric.get('key', '')
                        metric_label = metric.get('label', '')
                        
                        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’keyã¨labelã®ä¸¡æ–¹ã§ç¢ºèª
                        key_keywords = ['TIME', 'MEMORY', 'ROWS', 'BYTES', 'DURATION', 'PEAK', 'CUMULATIVE', 'EXCLUSIVE', 
                                       'SPILL', 'DISK', 'PRESSURE', 'SINK']
                        
                        # metric_keyã¾ãŸã¯metric_labelã«é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹å ´åˆã«æŠ½å‡º
                        is_important_metric = (
                            any(keyword in metric_key.upper() for keyword in key_keywords) or
                            any(keyword in metric_label.upper() for keyword in key_keywords)
                        )
                        
                        if is_important_metric:
                            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã¨ã—ã¦ã€labelãŒæœ‰åŠ¹ãªå ´åˆã¯labelã‚’ä½¿ç”¨ã€ãã†ã§ãªã‘ã‚Œã°keyã‚’ä½¿ç”¨
                            metric_name = metric_label if metric_label and metric_label != 'UNKNOWN_KEY' else metric_key
                            detailed_metrics[metric_name] = {
                                'value': metric.get('value', 0),
                                'label': metric_label,
                                'type': metric.get('metricType', ''),
                                'original_key': metric_key,  # å…ƒã®ã‚­ãƒ¼åã‚’ä¿å­˜
                                'display_name': metric_name  # è¡¨ç¤ºç”¨ã®åå‰
                            }
                    node_metric['detailed_metrics'] = detailed_metrics
                    metrics["node_metrics"].append(node_metric)
    
    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è¨ˆç®—
    metrics["bottleneck_indicators"] = calculate_bottleneck_indicators(metrics)
    
    # Liquid Clusteringåˆ†æ
    metrics["liquid_clustering_analysis"] = analyze_liquid_clustering_opportunities(profiler_data, metrics)
    
    return metrics

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: extract_performance_metrics")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ·ï¸ ã‚»ãƒ«5: ãƒãƒ¼ãƒ‰åè§£æãƒ»æ”¹å–„é–¢æ•°
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - æ±ç”¨çš„ãªãƒãƒ¼ãƒ‰åï¼ˆWhole Stage Codegenç­‰ï¼‰ã®å…·ä½“åŒ–
# MAGIC - é–¢é€£ãƒãƒ¼ãƒ‰ã®æ¤œç´¢ã¨æœ€é©ãªå‡¦ç†åã®é¸æŠ
# MAGIC - Photonæƒ…å ±ã‚„ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®ä»˜åŠ 
# MAGIC - å‡¦ç†åã®æ„å‘³çš„ãªæ”¹å–„

# COMMAND ----------

def get_meaningful_node_name(node: Dict[str, Any], extracted_metrics: Dict[str, Any]) -> str:
    """
    ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹ãƒãƒ¼ãƒ‰åã‚’å–å¾—ã™ã‚‹é–¢æ•°
    æ±ç”¨çš„ãªåå‰ï¼ˆWhole Stage Codegenãªã©ï¼‰ã‚’å…·ä½“çš„ãªå‡¦ç†åã«å¤‰æ›
    """
    original_name = node.get('name', '')
    node_id = node.get('node_id', node.get('id', ''))
    node_tag = node.get('tag', '')
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—
    metadata = node.get('metadata', [])
    metadata_info = {}
    for meta in metadata:
        key = meta.get('key', '')
        value = meta.get('value', '')
        label = meta.get('label', '')
        if value:
            metadata_info[key] = value
    
    # 1. æ±ç”¨çš„ãªåå‰ã‚’å…·ä½“çš„ãªåå‰ã«ç½®ãæ›ãˆ
    if 'whole stage codegen' in original_name.lower():
        # ã‚ˆã‚Šå…·ä½“çš„ãªå‡¦ç†åã‚’æ¨æ¸¬ã™ã‚‹ãŸã‚ã®ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯
        
        # ãƒãƒ¼ãƒ‰IDãƒ™ãƒ¼ã‚¹ã§ã®é–¢é€£æ€§ã‚’æ¨æ¸¬ï¼ˆéš£æ¥IDï¼‰
        node_id_num = None
        try:
            node_id_num = int(node_id) if node_id else None
        except:
            pass
        
        if node_id_num:
            # åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®è¿‘ã„IDã®å…·ä½“çš„ãªå‡¦ç†ã‚’æ¢ã™
            all_nodes = extracted_metrics.get('node_metrics', [])
            nearby_specific_nodes = []
            
            for other_node in all_nodes:
                other_id = other_node.get('node_id', '')
                other_name = other_node.get('name', '')
                
                try:
                    other_id_num = int(other_id) if other_id else None
                    if other_id_num and abs(other_id_num - node_id_num) <= 10:  # è¿‘éš£10å€‹ä»¥å†…
                        if is_specific_process_name(other_name):
                            nearby_specific_nodes.append(other_name)
                except:
                    continue
            
            # æœ€ã‚‚å…·ä½“çš„ãªå‡¦ç†åã‚’é¸æŠ
            if nearby_specific_nodes:
                specific_name = get_most_specific_process_name_from_list(nearby_specific_nodes)
                if specific_name and specific_name != original_name:
                    return f"{specific_name} (Whole Stage Codegen)"
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: tagã‹ã‚‰ã‚ˆã‚Šå…·ä½“çš„ãªæƒ…å ±ã‚’æŠ½å‡º
        if 'CODEGEN' in node_tag:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­ã‚¿ã‚°æƒ…å ±ã‚’ç¢ºèª
            child_tag = metadata_info.get('CHILD_TAG', '')
            if child_tag and child_tag != 'Child':
                return f"Whole Stage Codegen ({child_tag})"
    
    # 2. ã‚ˆã‚Šå…·ä½“çš„ãªã‚¿ã‚°æƒ…å ±ã‚’ãƒãƒ¼ãƒ‰åã«åæ˜ 
    tag_to_name_mapping = {
        'PHOTON_SHUFFLE_EXCHANGE_SINK_EXEC': 'Photon Shuffle Exchange',
        'PHOTON_GROUPING_AGG_EXEC': 'Photon Grouping Aggregate', 
        'UNKNOWN_DATA_SOURCE_SCAN_EXEC': 'Data Source Scan',
        'HASH_AGGREGATE_EXEC': 'Hash Aggregate',
        'WHOLE_STAGE_CODEGEN_EXEC': 'Whole Stage Codegen'
    }
    
    if node_tag in tag_to_name_mapping:
        mapped_name = tag_to_name_mapping[node_tag]
        if mapped_name != original_name and mapped_name != 'Whole Stage Codegen':
            # ã‚¿ã‚°ã®æ–¹ãŒã‚ˆã‚Šå…·ä½“çš„ãªå ´åˆã¯ä½¿ç”¨
            enhanced_name = mapped_name
        else:
            enhanced_name = original_name
    else:
        enhanced_name = original_name
    
    # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‡¦ç†ã®è©³ç´°ã‚’è¿½åŠ 
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’è¿½åŠ 
    if 'SCAN_TABLE' in metadata_info:
        table_name = metadata_info['SCAN_TABLE']
        if 'scan' in enhanced_name.lower():
            enhanced_name = f"Scan {table_name}"
    
    # Photonæƒ…å ±ã‚’è¿½åŠ 
    if 'IS_PHOTON' in metadata_info and metadata_info['IS_PHOTON'] == 'true':
        if not enhanced_name.startswith('Photon'):
            enhanced_name = f"Photon {enhanced_name}"
    
    return enhanced_name

def find_related_specific_nodes(target_node_id: str, nodes: list, edges: list) -> list:
    """æŒ‡å®šãƒãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹å…·ä½“çš„ãªå‡¦ç†ãƒãƒ¼ãƒ‰ã‚’æ¤œç´¢"""
    
    # ã‚¨ãƒƒã‚¸ã‹ã‚‰é–¢é€£ãƒãƒ¼ãƒ‰ã‚’ç‰¹å®š
    related_node_ids = set()
    
    # ç›´æ¥æ¥ç¶šã•ã‚Œã¦ã„ã‚‹ãƒãƒ¼ãƒ‰
    for edge in edges:
        from_id = edge.get('fromId', '')
        to_id = edge.get('toId', '')
        
        if from_id == target_node_id:
            related_node_ids.add(to_id)
        elif to_id == target_node_id:
            related_node_ids.add(from_id)
    
    # é–¢é€£ãƒãƒ¼ãƒ‰ã®è©³ç´°ã‚’å–å¾—
    related_nodes = []
    for node in nodes:
        node_id = node.get('id', '')
        if node_id in related_node_ids:
            node_name = node.get('name', '')
            # å…·ä½“çš„ãªå‡¦ç†åã‚’æŒã¤ãƒãƒ¼ãƒ‰ã®ã¿é¸æŠ
            if is_specific_process_name(node_name):
                related_nodes.append(node)
    
    return related_nodes

def is_specific_process_name(name: str) -> bool:
    """å…·ä½“çš„ãªå‡¦ç†åã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    specific_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project', 'join',
        'aggregate', 'sort', 'exchange', 'broadcast', 'scan', 'union'
    ]
    
    generic_keywords = [
        'whole stage codegen', 'stage', 'query', 'result'
    ]
    
    name_lower = name.lower()
    
    # å…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€å ´åˆ
    for keyword in specific_keywords:
        if keyword in name_lower:
            return True
    
    # æ±ç”¨çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã®å ´åˆã¯é™¤å¤–
    for keyword in generic_keywords:
        if keyword in name_lower and len(name_lower.split()) <= 3:
            return False
    
    return True

def get_most_specific_process_name(nodes: list) -> str:
    """æœ€ã‚‚å…·ä½“çš„ãªå‡¦ç†åã‚’é¸æŠ"""
    if not nodes:
        return ""
    
    # å„ªå…ˆé †ä½: ã‚ˆã‚Šå…·ä½“çš„ã§æ„å‘³ã®ã‚ã‚‹å‡¦ç†å
    priority_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project',
        'hash join', 'broadcast join', 'sort merge join',
        'hash aggregate', 'sort aggregate', 'grouping aggregate'
    ]
    
    for keyword in priority_keywords:
        for node in nodes:
            node_name = node.get('name', '').lower()
            if keyword in node_name:
                return node.get('name', '')
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®å…·ä½“çš„ãªãƒãƒ¼ãƒ‰å
    for node in nodes:
        node_name = node.get('name', '')
        if is_specific_process_name(node_name):
            return node_name
    
    return ""

def get_most_specific_process_name_from_list(node_names: list) -> str:
    """ãƒãƒ¼ãƒ‰åã®ãƒªã‚¹ãƒˆã‹ã‚‰æœ€ã‚‚å…·ä½“çš„ãªå‡¦ç†åã‚’é¸æŠ"""
    if not node_names:
        return ""
    
    # å„ªå…ˆé †ä½: ã‚ˆã‚Šå…·ä½“çš„ã§æ„å‘³ã®ã‚ã‚‹å‡¦ç†å
    priority_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project',
        'hash join', 'broadcast join', 'sort merge join',
        'hash aggregate', 'sort aggregate', 'grouping aggregate'
    ]
    
    for keyword in priority_keywords:
        for name in node_names:
            if keyword in name.lower():
                return name
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®å…·ä½“çš„ãªãƒãƒ¼ãƒ‰å
    for name in node_names:
        if is_specific_process_name(name):
            return name
    
    return ""

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: get_meaningful_node_name")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¯ ã‚»ãƒ«6: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™è¨ˆç®—é–¢æ•°
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - å®Ÿè¡Œæ™‚é–“ã¨ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“ã®æ¯”ç‡åˆ†æ
# MAGIC - ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ã¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹ç‡ã®è¨ˆç®—
# MAGIC - Photonåˆ©ç”¨ç‡ã®åˆ†æ
# MAGIC - ã‚¹ãƒ”ãƒ«æ¤œå‡ºã¨ã‚·ãƒ£ãƒƒãƒ•ãƒ«/ä¸¦åˆ—åº¦ã®å•é¡Œç‰¹å®š

# COMMAND ----------

def calculate_bottleneck_indicators(metrics: Dict[str, Any]) -> Dict[str, Any]:
    """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã‚’è¨ˆç®—"""
    indicators = {}
    
    overall = metrics.get('overall_metrics', {})
    total_time = overall.get('total_time_ms', 0)
    execution_time = overall.get('execution_time_ms', 0)
    compilation_time = overall.get('compilation_time_ms', 0)
    
    if total_time > 0:
        indicators['compilation_ratio'] = compilation_time / total_time
        indicators['execution_ratio'] = execution_time / total_time
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡
    read_bytes = overall.get('read_bytes', 0)
    cache_bytes = overall.get('read_cache_bytes', 0)
    if read_bytes > 0:
        indicators['cache_hit_ratio'] = cache_bytes / read_bytes
    
    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹ç‡
    rows_read = overall.get('rows_read_count', 0)
    rows_produced = overall.get('rows_produced_count', 0)
    if rows_read > 0:
        indicators['data_selectivity'] = rows_produced / rows_read
    
    # Photonä½¿ç”¨ç‡ï¼ˆã‚¿ã‚¹ã‚¯å®Ÿè¡Œæ™‚é–“ã«å¯¾ã™ã‚‹å‰²åˆï¼‰
    task_time = overall.get('task_total_time_ms', 0)
    photon_time = overall.get('photon_total_time_ms', 0)
    if task_time > 0:
        indicators['photon_ratio'] = min(photon_time / task_time, 1.0)  # æœ€å¤§100%ã«åˆ¶é™
    else:
        indicators['photon_ratio'] = 0.0
    
    # ã‚¹ãƒ”ãƒ«æ¤œå‡º
    spill_bytes = overall.get('spill_to_disk_bytes', 0)
    indicators['has_spill'] = spill_bytes > 0
    indicators['spill_bytes'] = spill_bytes
    
    # æœ€ã‚‚æ™‚é–“ã®ã‹ã‹ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸
    stage_durations = [(s['stage_id'], s['duration_ms']) for s in metrics.get('stage_metrics', []) if s['duration_ms'] > 0]
    if stage_durations:
        slowest_stage = max(stage_durations, key=lambda x: x[1])
        indicators['slowest_stage_id'] = slowest_stage[0]
        indicators['slowest_stage_duration'] = slowest_stage[1]
    
    # æœ€ã‚‚ãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨ã™ã‚‹ãƒãƒ¼ãƒ‰
    memory_usage = []
    for node in metrics.get('node_metrics', []):
        peak_memory = node.get('key_metrics', {}).get('peakMemoryBytes', 0)
        if peak_memory > 0:
            memory_usage.append((node['node_id'], node['name'], peak_memory))
    
    if memory_usage:
        highest_memory_node = max(memory_usage, key=lambda x: x[2])
        indicators['highest_memory_node_id'] = highest_memory_node[0]
        indicators['highest_memory_node_name'] = highest_memory_node[1]
        indicators['highest_memory_bytes'] = highest_memory_node[2]
    
    # ä¸¦åˆ—åº¦ã¨ã‚·ãƒ£ãƒƒãƒ•ãƒ«å•é¡Œã®æ¤œå‡º
    shuffle_nodes = []
    low_parallelism_stages = []
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰ã®ç‰¹å®š
    for node in metrics.get('node_metrics', []):
        node_name = node.get('name', '').upper()
        if any(keyword in node_name for keyword in ['SHUFFLE', 'EXCHANGE']):
            shuffle_nodes.append({
                'node_id': node['node_id'],
                'name': node['name'],
                'duration_ms': node.get('key_metrics', {}).get('durationMs', 0),
                'rows': node.get('key_metrics', {}).get('rowsNum', 0)
            })
    
    # ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸ã®æ¤œå‡º
    for stage in metrics.get('stage_metrics', []):
        num_tasks = stage.get('num_tasks', 0)
        duration_ms = stage.get('duration_ms', 0)
        
        # ä¸¦åˆ—åº¦ãŒä½ã„ï¼ˆã‚¿ã‚¹ã‚¯æ•°ãŒå°‘ãªã„ï¼‰ã‹ã¤å®Ÿè¡Œæ™‚é–“ãŒé•·ã„ã‚¹ãƒ†ãƒ¼ã‚¸
        if num_tasks > 0 and num_tasks < 10 and duration_ms > 5000:  # 10ã‚¿ã‚¹ã‚¯æœªæº€ã€5ç§’ä»¥ä¸Š
            low_parallelism_stages.append({
                'stage_id': stage['stage_id'],
                'num_tasks': num_tasks,
                'duration_ms': duration_ms,
                'avg_task_duration': duration_ms / max(num_tasks, 1)
            })
    
    indicators['shuffle_operations_count'] = len(shuffle_nodes)
    indicators['low_parallelism_stages_count'] = len(low_parallelism_stages)
    indicators['has_shuffle_bottleneck'] = len(shuffle_nodes) > 0 and any(s['duration_ms'] > 10000 for s in shuffle_nodes)
    indicators['has_low_parallelism'] = len(low_parallelism_stages) > 0
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã®è©³ç´°æƒ…å ±
    if shuffle_nodes:
        total_shuffle_time = sum(s['duration_ms'] for s in shuffle_nodes)
        indicators['total_shuffle_time_ms'] = total_shuffle_time
        indicators['shuffle_time_ratio'] = total_shuffle_time / max(total_time, 1)
        
        # æœ€ã‚‚æ™‚é–“ã®ã‹ã‹ã‚‹ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ
        slowest_shuffle = max(shuffle_nodes, key=lambda x: x['duration_ms'])
        indicators['slowest_shuffle_duration_ms'] = slowest_shuffle['duration_ms']
        indicators['slowest_shuffle_node'] = slowest_shuffle['name']
    
    # ä½ä¸¦åˆ—åº¦ã®è©³ç´°æƒ…å ±
    if low_parallelism_stages:
        indicators['low_parallelism_details'] = low_parallelism_stages
        avg_parallelism = sum(s['num_tasks'] for s in low_parallelism_stages) / len(low_parallelism_stages)
        indicators['average_low_parallelism'] = avg_parallelism
    
    return indicators

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: calculate_bottleneck_indicators")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ§¬ ã‚»ãƒ«7: Liquid Clusteringåˆ†æé–¢æ•°
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ã‚«ãƒ©ãƒ æƒ…å ±æŠ½å‡º
# MAGIC - ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€JOINã€GROUP BYæ¡ä»¶ã®åˆ†æ
# MAGIC - ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å½±éŸ¿ã®è©•ä¾¡
# MAGIC - ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¨å¥¨ã‚«ãƒ©ãƒ ã®ç‰¹å®š

# COMMAND ----------

def analyze_liquid_clustering_opportunities(profiler_data: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Liquid Clusteringã«åŠ¹æœçš„ãªã‚«ãƒ©ãƒ ã‚’ç‰¹å®šï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æï¼‰
    """
    
    clustering_analysis = {
        "recommended_tables": {},
        "filter_columns": [],
        "join_columns": [],
        "groupby_columns": [],
        "pushdown_filters": [],
        "data_skew_indicators": {},
        "performance_impact": {},
        "detailed_column_analysis": {},
        "summary": {}
    }
    
    print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚«ãƒ©ãƒ æƒ…å ±ã‚’ç›´æ¥æŠ½å‡ºé–‹å§‹")
    
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿè¡Œã‚°ãƒ©ãƒ•æƒ…å ±ã‚’å–å¾—
    graphs = profiler_data.get('graphs', [])
    if not graphs:
        print("âš ï¸ ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return clustering_analysis
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è§£æ
    nodes = graphs[0].get('nodes', []) if graphs else []
    print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: {len(nodes)}å€‹ã®ãƒãƒ¼ãƒ‰ã‚’åˆ†æä¸­")
    
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ¼ãƒ‰ã‹ã‚‰ã‚«ãƒ©ãƒ æƒ…å ±ã‚’ç›´æ¥æŠ½å‡º
    for node in nodes:
        node_name = node.get('name', '')
        node_tag = node.get('tag', '')
        node_metadata = node.get('metadata', [])
        
        print(f"ğŸ” ãƒãƒ¼ãƒ‰åˆ†æ: {node_name} ({node_tag})")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é‡è¦ãªæƒ…å ±ã‚’æŠ½å‡º
        for metadata_item in node_metadata:
            key = metadata_item.get('key', '')
            label = metadata_item.get('label', '')
            values = metadata_item.get('values', [])
            value = metadata_item.get('value', '')
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã®æŠ½å‡º
            if key == 'FILTERS' and values:
                for filter_expr in values:
                    clustering_analysis["pushdown_filters"].append({
                        "node_name": node_name,
                        "filter_expression": filter_expr,
                        "metadata_key": key
                    })
                    print(f"   âœ… ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æŠ½å‡º: {filter_expr}")
                    
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¼ã‹ã‚‰ã‚«ãƒ©ãƒ åã‚’æŠ½å‡º
                    if '=' in filter_expr:
                        # "cs_sold_date_sk = 2451659" ã®ã‚ˆã†ãªå½¢å¼ã‹ã‚‰ã‚«ãƒ©ãƒ åã‚’æŠ½å‡º
                        parts = filter_expr.split('=')
                        if len(parts) >= 2:
                            column_name = parts[0].strip().replace('(', '').replace(')', '').replace('tpcds.tpcds_sf1000_delta_lc.detail_itagaki.', '')
                            if column_name.endswith('_sk') or column_name.endswith('_date') or column_name.endswith('_id'):
                                clustering_analysis["filter_columns"].append(column_name)
                                print(f"     â†’ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚«ãƒ©ãƒ : {column_name}")
            
            # GROUP BYå¼ã®æŠ½å‡º
            elif key == 'GROUPING_EXPRESSIONS' and values:
                for group_expr in values:
                    # "tpcds.tpcds_sf1000_delta_lc.detail_itagaki.cs_bill_customer_sk" ã‹ã‚‰ã‚«ãƒ©ãƒ åã‚’æŠ½å‡º
                    column_name = group_expr.replace('tpcds.tpcds_sf1000_delta_lc.detail_itagaki.', '')
                    if column_name.endswith('_sk') or column_name.endswith('_date') or column_name.endswith('_id'):
                        clustering_analysis["groupby_columns"].append(column_name)
                        print(f"   âœ… GROUP BYã‚«ãƒ©ãƒ : {column_name}")
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ã®å‡ºåŠ›åˆ—æƒ…å ±
            elif key == 'OUTPUT' and values and 'SCAN' in node_name.upper():
                table_name = value if key == 'SCAN_IDENTIFIER' else f"table_{node.get('id', 'unknown')}"
                for output_col in values:
                    column_name = output_col.split('.')[-1] if '.' in output_col else output_col
                    if column_name.endswith('_sk') or column_name.endswith('_date') or column_name.endswith('_id'):
                        print(f"   ğŸ“Š å‡ºåŠ›ã‚«ãƒ©ãƒ : {column_name}")
            
            # ã‚¹ã‚­ãƒ£ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®æŠ½å‡º
            elif key == 'SCAN_IDENTIFIER':
                table_name = value
                print(f"   ğŸ·ï¸ ãƒ†ãƒ¼ãƒ–ãƒ«è­˜åˆ¥å­: {table_name}")
    
    # ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ã®è£œå®Œåˆ†æ
    node_metrics = metrics.get('node_metrics', [])
    table_scan_nodes = []
    join_nodes = []
    shuffle_nodes = []
    filter_nodes = []
    
    for node in node_metrics:
        node_name = node.get('name', '').upper()
        node_tag = node.get('tag', '').upper()
        detailed_metrics = node.get('detailed_metrics', {})
        
        # è£œå®Œçš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®è£œå®Œï¼‰
        for metric_key, metric_info in detailed_metrics.items():
            metric_label = metric_info.get('label', '')
            metric_value = metric_info.get('value', '')
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã®è£œå®ŒæŠ½å‡º
            if any(filter_keyword in metric_key.upper() for filter_keyword in ['FILTER', 'PREDICATE', 'CONDITION']):
                if metric_label or metric_value:
                    clustering_analysis["pushdown_filters"].append({
                        "node_id": node.get('node_id', ''),
                        "node_name": node_name,
                        "filter_expression": metric_label or str(metric_value),
                        "metric_key": metric_key
                    })
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®è£œå®Œåˆ†æ
        if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN', 'PARQUET', 'DELTA']):
            table_scan_nodes.append(node)
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼æŒ‡æ¨™ã®è¨ˆç®—
            key_metrics = node.get('key_metrics', {})
            rows_num = key_metrics.get('rowsNum', 0)
            duration_ms = key_metrics.get('durationMs', 0)
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ãƒ¼ãƒ–ãƒ«åæŠ½å‡º
            table_name = f"table_{node.get('node_id', 'unknown')}"
            
            # ãƒãƒ¼ãƒ‰åã‹ã‚‰å˜èªã‚’æŠ½å‡ºã—ã¦ãƒ†ãƒ¼ãƒ–ãƒ«åã‚‰ã—ã„ã‚‚ã®ã‚’æ¤œç´¢
            words = node_name.replace('(', ' ').replace(')', ' ').replace('[', ' ').replace(']', ' ').split()
            for word in words:
                if '.' in word and len(word.split('.')) >= 2:
                    table_name = word.lower()
                    break
                elif not word.isupper() and len(word) > 5 and '_' in word:
                    table_name = word.lower()
                    break
            
            # ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿æŒ‡æ¨™ã®è¨˜éŒ²
            clustering_analysis["data_skew_indicators"][table_name] = {
                "rows_scanned": rows_num,
                "scan_duration_ms": duration_ms,
                "avg_rows_per_ms": rows_num / max(duration_ms, 1),
                "node_name": node_name,
                "node_id": node.get('node_id', '')
            }
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã®ç‰¹å®š
        elif any(keyword in node_name for keyword in ['FILTER']):
            filter_nodes.append(node)
        
        # JOINãƒãƒ¼ãƒ‰ã®è£œå®Œåˆ†æ
        elif any(keyword in node_name for keyword in ['JOIN', 'HASH']):
            join_nodes.append(node)
        
        # Shuffleãƒãƒ¼ãƒ‰ã®ç‰¹å®š
        elif any(keyword in node_name for keyword in ['SHUFFLE', 'EXCHANGE']):
            shuffle_nodes.append(node)
    
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®æŠ½å‡ºçµæœã‚’æ•´ç†
    print(f"\nğŸ” ãƒ‡ãƒãƒƒã‚°: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ ä¸€è¦§")
    print(f"   ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚«ãƒ©ãƒ : {clustering_analysis['filter_columns']}")
    print(f"   JOINã‚«ãƒ©ãƒ : {clustering_analysis['join_columns']}")
    print(f"   GROUP BYã‚«ãƒ©ãƒ : {clustering_analysis['groupby_columns']}")
    
    # é‡è¤‡é™¤å»
    clustering_analysis["filter_columns"] = list(set(clustering_analysis["filter_columns"]))
    clustering_analysis["join_columns"] = list(set(clustering_analysis["join_columns"]))
    clustering_analysis["groupby_columns"] = list(set(clustering_analysis["groupby_columns"]))
    
    # ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ã‚’åé›†
    all_columns = set()
    all_columns.update(clustering_analysis["filter_columns"])
    all_columns.update(clustering_analysis["join_columns"])
    all_columns.update(clustering_analysis["groupby_columns"])
    
    print(f"\nğŸ” ãƒ‡ãƒãƒƒã‚°: æœ€çµ‚çš„ãªæœ‰åŠ¹ã‚«ãƒ©ãƒ æ•°: {len(all_columns)}")
    print(f"   æœ‰åŠ¹ã‚«ãƒ©ãƒ : {list(all_columns)}")
    
    # ã‚«ãƒ©ãƒ åˆ¥ã®è©³ç´°åˆ†æ
    for column in all_columns:
        column_analysis = {
            "filter_usage_count": clustering_analysis["filter_columns"].count(column),
            "join_usage_count": clustering_analysis["join_columns"].count(column),
            "groupby_usage_count": clustering_analysis["groupby_columns"].count(column),
            "total_usage": 0,
            "usage_contexts": [],
            "associated_tables": set(),
            "performance_impact": "low"
        }
        
        # ä½¿ç”¨å›æ•°ã®åˆè¨ˆè¨ˆç®—
        column_analysis["total_usage"] = (
            column_analysis["filter_usage_count"] * 3 +
            column_analysis["join_usage_count"] * 2 +
            column_analysis["groupby_usage_count"] * 1
        )
        
        # ä½¿ç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è¨˜éŒ²
        if column_analysis["filter_usage_count"] > 0:
            column_analysis["usage_contexts"].append("WHERE/Filteræ¡ä»¶")
        if column_analysis["join_usage_count"] > 0:
            column_analysis["usage_contexts"].append("JOINæ¡ä»¶")
        if column_analysis["groupby_usage_count"] > 0:
            column_analysis["usage_contexts"].append("GROUP BY")
        
        # é–¢é€£ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç‰¹å®š
        column_parts = column.split('.')
        if len(column_parts) >= 2:
            if len(column_parts) == 3:  # schema.table.column
                table_name = f"{column_parts[0]}.{column_parts[1]}"
                column_analysis["associated_tables"].add(table_name)
            else:  # table.column
                column_analysis["associated_tables"].add(column_parts[0])
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å½±éŸ¿åº¦ã®è©•ä¾¡
        if column_analysis["total_usage"] >= 6:
            column_analysis["performance_impact"] = "high"
        elif column_analysis["total_usage"] >= 3:
            column_analysis["performance_impact"] = "medium"
        
        # setå‹ã‚’listå‹ã«å¤‰æ›
        column_analysis["associated_tables"] = list(column_analysis["associated_tables"])
        
        clustering_analysis["detailed_column_analysis"][column] = column_analysis
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æ¯ã®æ¨å¥¨äº‹é …ï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ™ãƒ¼ã‚¹ï¼‰
    for table_name, skew_info in clustering_analysis["data_skew_indicators"].items():
        print(f"\nğŸ” ãƒ‡ãƒãƒƒã‚°: ãƒ†ãƒ¼ãƒ–ãƒ« {table_name} ã®æ¨å¥¨ã‚«ãƒ©ãƒ åˆ†æ")
        
        # ã‚«ãƒ©ãƒ ã®é‡è¦åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç°¡æ½”ç‰ˆï¼‰
        column_scores = {}
        for col in all_columns:
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€JOINã€GROUP BYã§ã®ä½¿ç”¨é »åº¦ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
            score = (clustering_analysis["filter_columns"].count(col) * 3 +
                    clustering_analysis["join_columns"].count(col) * 2 +
                    clustering_analysis["groupby_columns"].count(col) * 1)
            
            if score > 0:
                clean_col = col.split('.')[-1] if '.' in col else col
                column_scores[clean_col] = score
        
        # ä¸Šä½ã‚«ãƒ©ãƒ ã‚’æ¨å¥¨
        if column_scores:
            sorted_columns = sorted(column_scores.items(), key=lambda x: x[1], reverse=True)
            recommended_cols = [col for col, score in sorted_columns[:4]]  # æœ€å¤§4ã‚«ãƒ©ãƒ 
            
            print(f"   ğŸ“Š ã‚«ãƒ©ãƒ ã‚¹ã‚³ã‚¢: {dict(sorted_columns)}")
            print(f"   ğŸ† æ¨å¥¨ã‚«ãƒ©ãƒ : {recommended_cols}")
            
            clustering_analysis["recommended_tables"][table_name] = {
                "clustering_columns": recommended_cols,
                "column_scores": column_scores,
                "scan_performance": {
                    "rows_scanned": skew_info["rows_scanned"],
                    "scan_duration_ms": skew_info["scan_duration_ms"],
                    "efficiency_score": skew_info["avg_rows_per_ms"]
                },
                "node_details": {
                    "node_id": skew_info.get("node_id", ""),
                    "node_name": skew_info.get("node_name", "")
                }
            }
        else:
            print(f"   âš ï¸ æœ‰åŠ¹ãªã‚«ãƒ©ãƒ ã‚¹ã‚³ã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã®è¦‹è¾¼ã¿è©•ä¾¡
    total_scan_time = sum(info["scan_duration_ms"] for info in clustering_analysis["data_skew_indicators"].values())
    total_shuffle_time = 0
    
    for node in shuffle_nodes:
        total_shuffle_time += node.get('key_metrics', {}).get('durationMs', 0)
    
    clustering_analysis["performance_impact"] = {
        "total_scan_time_ms": total_scan_time,
        "total_shuffle_time_ms": total_shuffle_time,
        "potential_scan_improvement": "30-70%" if total_scan_time > 10000 else "10-30%",
        "potential_shuffle_reduction": "20-50%" if total_shuffle_time > 5000 else "5-20%",
        "estimated_overall_improvement": "25-60%" if (total_scan_time + total_shuffle_time) > 15000 else "10-25%"
    }
    
    # ã‚µãƒãƒªãƒ¼æƒ…å ±ï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ™ãƒ¼ã‚¹ï¼‰
    clustering_analysis["summary"] = {
        "tables_identified": len(clustering_analysis["recommended_tables"]),
        "total_filter_columns": len(clustering_analysis["filter_columns"]),
        "total_join_columns": len(clustering_analysis["join_columns"]),
        "total_groupby_columns": len(clustering_analysis["groupby_columns"]),
        "high_impact_tables": len([t for t, info in clustering_analysis["recommended_tables"].items() 
                                 if info["scan_performance"]["scan_duration_ms"] > 5000]),
        "unique_filter_columns": clustering_analysis["filter_columns"],
        "unique_join_columns": clustering_analysis["join_columns"],
        "unique_groupby_columns": clustering_analysis["groupby_columns"],
        "profiler_data_source": "graphs_metadata"
    }
    
    return clustering_analysis

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: analyze_liquid_clustering_opportunities")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¤– ã‚»ãƒ«8: LLMã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æé–¢æ•°
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®LLMåˆ†æç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
# MAGIC - è¤‡æ•°LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å¯¾å¿œï¼ˆDatabricks/OpenAI/Azure/Anthropicï¼‰
# MAGIC - æ—¥æœ¬èªã§ã®è©³ç´°ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
# MAGIC - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æ

# COMMAND ----------

def analyze_bottlenecks_with_llm(metrics: Dict[str, Any]) -> str:
    """
    è¨­å®šã•ã‚ŒãŸLLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’è¡Œã†
    """
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¦ç´„ã®æº–å‚™ï¼ˆç°¡æ½”ç‰ˆï¼‰
    # ä¸»è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã¿ã‚’æŠ½å‡ºã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
    total_time_sec = metrics['overall_metrics'].get('total_time_ms', 0) / 1000
    read_gb = metrics['overall_metrics'].get('read_bytes', 0) / 1024 / 1024 / 1024
    cache_ratio = metrics['bottleneck_indicators'].get('cache_hit_ratio', 0) * 100
    data_selectivity = metrics['bottleneck_indicators'].get('data_selectivity', 0) * 100
    
    # Liquid Clusteringæ¨å¥¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆä¸Šä½3ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿ï¼‰
    top_tables = list(metrics['liquid_clustering_analysis']['recommended_tables'].items())[:3]
    table_recommendations = [f"- {table}: {', '.join(info['clustering_columns'])}" for table, info in top_tables]
    
    # é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚«ãƒ©ãƒ ï¼ˆä¸Šä½5å€‹ã®ã¿ï¼‰
    high_impact_cols = [(col, analysis) for col, analysis in metrics['liquid_clustering_analysis']['detailed_column_analysis'].items() 
                       if analysis.get('performance_impact') == 'high'][:5]
    high_impact_summary = [f"- {col}: ã‚¹ã‚³ã‚¢={analysis['total_usage']}, ä½¿ç”¨ç®‡æ‰€=[{', '.join(analysis['usage_contexts'])}]" 
                          for col, analysis in high_impact_cols]
    
    # Photonã¨ä¸¦åˆ—åº¦ã®æƒ…å ±ã‚’è¿½åŠ 
    photon_enabled = metrics['overall_metrics'].get('photon_enabled', False)
    photon_utilization_ratio = metrics['overall_metrics'].get('photon_utilization_ratio', 0)
    photon_utilization = min(photon_utilization_ratio * 100, 100.0)  # æœ€å¤§100%ã«åˆ¶é™
    shuffle_count = metrics['bottleneck_indicators'].get('shuffle_operations_count', 0)
    has_shuffle_bottleneck = metrics['bottleneck_indicators'].get('has_shuffle_bottleneck', False)
    has_low_parallelism = metrics['bottleneck_indicators'].get('has_low_parallelism', False)
    low_parallelism_count = metrics['bottleneck_indicators'].get('low_parallelism_stages_count', 0)
    
    analysis_prompt = f"""
ã‚ãªãŸã¯Databricksã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åˆ†æã—ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®šã—ã¦æ”¹å–„æ¡ˆã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚

ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦ã€‘
- å®Ÿè¡Œæ™‚é–“: {total_time_sec:.1f}ç§’
- èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿: {read_gb:.1f}GB
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡: {cache_ratio:.1f}%
- ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§: {data_selectivity:.1f}%
- ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ: {'ã‚ã‚Š' if metrics['bottleneck_indicators'].get('has_spill', False) else 'ãªã—'}

ã€Photonã‚¨ãƒ³ã‚¸ãƒ³åˆ†æã€‘
- Photonæœ‰åŠ¹: {'ã¯ã„' if photon_enabled else 'ã„ã„ãˆ'}
- Photonåˆ©ç”¨ç‡: {photon_utilization:.1f}%
- Photonæ¨å¥¨: {'æ—¢ã«æœ€é©åŒ–æ¸ˆã¿' if photon_utilization > 80 else 'Photonæœ‰åŠ¹åŒ–ã‚’æ¨å¥¨' if not photon_enabled else 'Photonåˆ©ç”¨ç‡å‘ä¸ŠãŒå¿…è¦'}

ã€ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«åˆ†æã€‘
- ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ: {shuffle_count}å›
- ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯: {'ã‚ã‚Š' if has_shuffle_bottleneck else 'ãªã—'}
- ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸: {low_parallelism_count}å€‹
- ä¸¦åˆ—åº¦å•é¡Œ: {'ã‚ã‚Š' if has_low_parallelism else 'ãªã—'}

ã€Liquid Clusteringæ¨å¥¨ã€‘
ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {metrics['liquid_clustering_analysis']['summary'].get('tables_identified', 0)}å€‹
æ¨å¥¨ã‚«ãƒ©ãƒ :
{chr(10).join(table_recommendations)}

é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚«ãƒ©ãƒ :
{chr(10).join(high_impact_summary)}

ã€é‡è¦æŒ‡æ¨™ã€‘
- æœ€é…ã‚¹ãƒ†ãƒ¼ã‚¸: {metrics['bottleneck_indicators'].get('slowest_stage_id', 'N/A')}
- æœ€é«˜ãƒ¡ãƒ¢ãƒª: {metrics['bottleneck_indicators'].get('highest_memory_bytes', 0)/1024/1024:.0f}MB
- Photonä½¿ç”¨ç‡: {metrics['bottleneck_indicators'].get('photon_ratio', 0)*100:.0f}%

ã€æ±‚ã‚ã‚‹åˆ†æã€‘
1. ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨åŸå› ï¼ˆPhotonã€ä¸¦åˆ—åº¦ã€ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã«ç„¦ç‚¹ï¼‰
2. Liquid Clusteringå®Ÿè£…ã®å„ªå…ˆé †ä½ã¨æ‰‹é †ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»ZORDERä»¥å¤–ï¼‰
3. å„æ¨å¥¨ã‚«ãƒ©ãƒ ã®é¸å®šç†ç”±ã¨åŠ¹æœ
4. Photonã‚¨ãƒ³ã‚¸ãƒ³ã®æœ€é©åŒ–æ¡ˆ
5. ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ€é©åŒ–æ¡ˆ
6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„è¦‹è¾¼ã¿

**é‡è¦**: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚„ZORDERã¯ææ¡ˆã›ãšã€Liquid Clusteringã®ã¿ã‚’æ¨å¥¨ã—ã¦ãã ã•ã„ã€‚
ç°¡æ½”ã§å®Ÿè·µçš„ãªæ”¹å–„ææ¡ˆã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚
"""
    
    try:
        # è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«åŸºã¥ã„ã¦åˆ†æå®Ÿè¡Œ
        provider = LLM_CONFIG["provider"]
        print(f"ğŸ¤– {provider}ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«åˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ä¸­...")
        print("â³ å¤§ããªãƒ‡ãƒ¼ã‚¿ã®ãŸã‚å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™...")
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ã®å‡¦ç†
        if provider == "databricks":
            return _call_databricks_llm(analysis_prompt)
        elif provider == "openai":
            return _call_openai_llm(analysis_prompt)
        elif provider == "azure_openai":
            return _call_azure_openai_llm(analysis_prompt)
        elif provider == "anthropic":
            return _call_anthropic_llm(analysis_prompt)
        else:
            return f"âŒ ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}"
            
    except Exception as e:
        error_msg = f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
        print(f"âŒ {error_msg}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªåˆ†æçµæœã‚’æä¾›
        fallback_analysis = f"""
ğŸ”§ **åŸºæœ¬çš„ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ** ({provider} LLMåˆ©ç”¨ä¸å¯ã®ãŸã‚ç°¡æ˜“ç‰ˆ)

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦
- **å®Ÿè¡Œæ™‚é–“**: {total_time_sec:.1f}ç§’
- **èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿é‡**: {read_gb:.1f}GB  
- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡**: {cache_ratio:.1f}%
- **ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§**: {data_selectivity:.1f}%

## âš¡ Photonã‚¨ãƒ³ã‚¸ãƒ³åˆ†æ
- **Photonæœ‰åŠ¹**: {'ã¯ã„' if photon_enabled else 'ã„ã„ãˆ'}
- **Photonåˆ©ç”¨ç‡**: {min(photon_utilization, 100.0):.1f}%
- **æ¨å¥¨**: {'Photonåˆ©ç”¨ç‡å‘ä¸ŠãŒå¿…è¦' if photon_utilization < 80 else 'æœ€é©åŒ–æ¸ˆã¿'}

## ï¿½ ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«åˆ†æ
- **ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ**: {shuffle_count}å›
- **ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: {'ã‚ã‚Š' if has_shuffle_bottleneck else 'ãªã—'}
- **ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸**: {low_parallelism_count}å€‹
- **ä¸¦åˆ—åº¦å•é¡Œ**: {'ã‚ã‚Š' if has_low_parallelism else 'ãªã—'}

## ï¿½ï¸ Liquid Clusteringæ¨å¥¨äº‹é …
**å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°**: {metrics['liquid_clustering_analysis']['summary'].get('tables_identified', 0)}å€‹

**æ¨å¥¨å®Ÿè£…**:
{chr(10).join(table_recommendations) if table_recommendations else '- æ¨å¥¨ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ'}

## âš ï¸ ä¸»è¦ãªå•é¡Œç‚¹
- {'ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™' if metrics['bottleneck_indicators'].get('has_spill', False) else 'ãƒ¡ãƒ¢ãƒªä½¿ç”¨ã¯æ­£å¸¸ã§ã™'}
- {'ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ãŒä½ä¸‹ã—ã¦ã„ã¾ã™' if cache_ratio < 50 else 'ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ã¯è‰¯å¥½ã§ã™'}
- {'ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§ãŒä½ãã€å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™' if data_selectivity < 10 else 'ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§ã¯é©åˆ‡ã§ã™'}
- {'Photonã‚¨ãƒ³ã‚¸ãƒ³ãŒç„¡åŠ¹ã¾ãŸã¯åˆ©ç”¨ç‡ãŒä½ã„' if not photon_enabled or photon_utilization < 50 else 'Photonåˆ©ç”¨ã¯è‰¯å¥½'}
- {'ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãŒç™ºç”Ÿ' if has_shuffle_bottleneck else 'ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‡¦ç†ã¯æ­£å¸¸'}
- {'ä¸¦åˆ—åº¦ãŒä½ã„ã‚¹ãƒ†ãƒ¼ã‚¸ãŒå­˜åœ¨' if has_low_parallelism else 'ä¸¦åˆ—åº¦ã¯é©åˆ‡'}

## ğŸš€ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
1. **Liquid Clusteringå®Ÿè£…**: ä¸Šè¨˜æ¨å¥¨ã‚«ãƒ©ãƒ ã§ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ãƒ»ZORDERä¸ä½¿ç”¨ï¼‰
2. **Photonæœ‰åŠ¹åŒ–**: {'Photonã‚¨ãƒ³ã‚¸ãƒ³ã‚’æœ‰åŠ¹ã«ã™ã‚‹' if not photon_enabled else 'Photonè¨­å®šã‚’æœ€é©åŒ–'}
3. **ä¸¦åˆ—åº¦æœ€é©åŒ–**: {'ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µã‚¤ã‚ºãƒ»ä¸¦åˆ—åº¦è¨­å®šã‚’è¦‹ç›´ã—' if has_low_parallelism else 'ç¾åœ¨ã®ä¸¦åˆ—åº¦ã¯é©åˆ‡'}
4. **ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ€é©åŒ–**: {'JOINé †åºãƒ»GROUP BYæœ€é©åŒ–ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‰Šæ¸›' if has_shuffle_bottleneck else 'ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‡¦ç†ã¯æœ€é©'}
5. **ã‚¯ã‚¨ãƒªæœ€é©åŒ–**: WHEREå¥ã®æ¡ä»¶ã‚’é©åˆ‡ã«è¨­å®š
6. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨**: ã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œè¨

**é‡è¦**: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚„ZORDERã¯ä½¿ç”¨ã›ãšã€Liquid Clusteringã®ã¿ã§æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚

**æ³¨æ„**: {provider} LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®æ¥ç¶šã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚è©³ç´°ãªåˆ†æã¯æ‰‹å‹•ã§å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚
        """
        return fallback_analysis

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ”Œ ã‚»ãƒ«9: å€‹åˆ¥LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ¥ç¶šé–¢æ•°
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - Databricks Model Serving ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæ¥ç¶š
# MAGIC - OpenAI API æ¥ç¶š
# MAGIC - Azure OpenAI API æ¥ç¶š
# MAGIC - Anthropic API æ¥ç¶š
# MAGIC - å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

# COMMAND ----------

def _call_databricks_llm(prompt: str) -> str:
    """Databricks Model Serving APIã‚’å‘¼ã³å‡ºã™"""
    try:
        # Databricksãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—
        try:
            token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
        except Exception:
            token = os.environ.get('DATABRICKS_TOKEN')
            if not token:
                return "âŒ Databricksãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç’°å¢ƒå¤‰æ•°DATABRICKS_TOKENã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        
        # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹URLã®å–å¾—
        try:
            workspace_url = spark.conf.get("spark.databricks.workspaceUrl")
        except Exception:
            workspace_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get("browserHostName").get()
        
        config = LLM_CONFIG["databricks"]
        endpoint_url = f"https://{workspace_url}/serving-endpoints/{config['endpoint_name']}/invocations"
        
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        # æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ãªå ´åˆã¯è¿½åŠ 
        if config.get("thinking_enabled", True):
            payload["thinking"] = {"type": "enabled"}
        
        # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½
        max_retries = 2
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    print(f"ğŸ”„ ãƒªãƒˆãƒ©ã‚¤ä¸­... (è©¦è¡Œ {attempt + 1}/{max_retries})")
                
                response = requests.post(endpoint_url, headers=headers, json=payload, timeout=180)
                
                if response.status_code == 200:
                    result = response.json()
                    analysis_text = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                    print("âœ… ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
                    return analysis_text
                else:
                    error_msg = f"APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}"
                    if attempt == max_retries - 1:
                        print(f"âŒ {error_msg}\nãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text}")
                        return error_msg
                    else:
                        print(f"âš ï¸ {error_msg} - ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                        continue
                        
            except requests.exceptions.Timeout:
                if attempt == max_retries - 1:
                    timeout_msg = "â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼: Databricksã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®å¿œç­”ãŒ180ç§’ä»¥å†…ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸã€‚"
                    print(f"âŒ {timeout_msg}")
                    return timeout_msg
                else:
                    print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç™ºç”Ÿ - ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™... (è©¦è¡Œ {attempt + 1}/{max_retries})")
                    continue
                    
    except Exception as e:
        return f"Databricks APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}"

def _call_openai_llm(prompt: str) -> str:
    """OpenAI APIã‚’å‘¼ã³å‡ºã™"""
    try:
        config = LLM_CONFIG["openai"]
        api_key = config["api_key"] or os.environ.get('OPENAI_API_KEY')
        
        if not api_key:
            return "âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚LLM_CONFIG['openai']['api_key']ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°OPENAI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": config["model"],
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        response = requests.post("https://api.openai.com/v1/chat/completions", 
                               headers=headers, json=payload, timeout=180)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['choices'][0]['message']['content']
            print("âœ… OpenAIåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
            return analysis_text
        else:
            return f"OpenAI APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"OpenAI APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}"

def _call_azure_openai_llm(prompt: str) -> str:
    """Azure OpenAI APIã‚’å‘¼ã³å‡ºã™"""
    try:
        config = LLM_CONFIG["azure_openai"]
        api_key = config["api_key"] or os.environ.get('AZURE_OPENAI_API_KEY')
        
        if not api_key or not config["endpoint"] or not config["deployment_name"]:
            return "âŒ Azure OpenAIè¨­å®šãŒä¸å®Œå…¨ã§ã™ã€‚api_keyã€endpointã€deployment_nameã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        
        endpoint_url = f"{config['endpoint']}/openai/deployments/{config['deployment_name']}/chat/completions?api-version={config['api_version']}"
        
        headers = {
            "api-key": api_key,
            "Content-Type": "application/json"
        }
        
        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        response = requests.post(endpoint_url, headers=headers, json=payload, timeout=180)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['choices'][0]['message']['content']
            print("âœ… Azure OpenAIåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
            return analysis_text
        else:
            return f"Azure OpenAI APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"Azure OpenAI APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}"

def _call_anthropic_llm(prompt: str) -> str:
    """Anthropic APIã‚’å‘¼ã³å‡ºã™"""
    try:
        config = LLM_CONFIG["anthropic"]
        api_key = config["api_key"] or os.environ.get('ANTHROPIC_API_KEY')
        
        if not api_key:
            return "âŒ Anthropic APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚LLM_CONFIG['anthropic']['api_key']ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ANTHROPIC_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        
        headers = {
            "x-api-key": api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        payload = {
            "model": config["model"],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"],
            "messages": [{"role": "user", "content": prompt}]
        }
        
        response = requests.post("https://api.anthropic.com/v1/messages", 
                               headers=headers, json=payload, timeout=180)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['content'][0]['text']
            print("âœ… Anthropicåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
            return analysis_text
        else:
            return f"Anthropic APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"Anthropic APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}"

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: analyze_bottlenecks_with_llm")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“‹ ã‚»ãƒ«13: LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Ÿè¡Œã®æº–å‚™
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç¢ºèªã¨è¡¨ç¤º
# MAGIC - åˆ†æé–‹å§‹ã®æº–å‚™ã¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
# MAGIC - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ã«ã‚ˆã‚‹å®‰å®šæ€§å‘ä¸Š

# COMMAND ----------

# LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Ÿè¡Œã®æº–å‚™
provider = LLM_CONFIG["provider"]

print(f"\nğŸ¤– ã€{provider.upper()} LLM ã«ã‚ˆã‚‹ SQLãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™ã€‘")
print("=" * 80)

if provider == "databricks":
    endpoint = LLM_CONFIG["databricks"]["endpoint_name"]
    print(f"ğŸ”— Databricks Model Serving ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: {endpoint}")
    print("âš ï¸  Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒç¨¼åƒä¸­ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
elif provider == "openai":
    model = LLM_CONFIG["openai"]["model"]
    print(f"ğŸ”— OpenAI ãƒ¢ãƒ‡ãƒ«: {model}")
    print("âš ï¸  OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™")
elif provider == "azure_openai":
    deployment = LLM_CONFIG["azure_openai"]["deployment_name"]
    print(f"ğŸ¤– Azure OpenAI ({deployment}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print("âš ï¸  Azure OpenAI APIã‚­ãƒ¼ã¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒå¿…è¦ã§ã™")
elif provider == "anthropic":
    model = LLM_CONFIG["anthropic"]["model"]
    print(f"ğŸ¤– Anthropic ({model}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print("âš ï¸  Anthropic APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™")

print("ğŸ“ åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç°¡æ½”åŒ–ã—ã¦ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒªã‚¹ã‚¯ã‚’è»½æ¸›ã—ã¦ã„ã¾ã™...")
print()

# extracted_metricså¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
try:
    extracted_metrics
    print("âœ… extracted_metricså¤‰æ•°ãŒç¢ºèªã•ã‚Œã¾ã—ãŸ")
    analysis_result = analyze_bottlenecks_with_llm(extracted_metrics)
except NameError:
    print("âŒ extracted_metricså¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("âš ï¸ ã‚»ãƒ«11 (ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º) ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„")
    print("ğŸ“‹ æ­£ã—ã„å®Ÿè¡Œé †åº: ã‚»ãƒ«10 â†’ ã‚»ãƒ«11 â†’ ã‚»ãƒ«13")
    print("ğŸ”„ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®åˆ†æçµæœã‚’è¨­å®šã—ã¾ã™")
    analysis_result = """
ğŸ¤– LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ

âŒ åˆ†æã«å¿…è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚

ğŸ“‹ è§£æ±ºæ–¹æ³•:
1. ã‚»ãƒ«10ã§JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
2. ã‚»ãƒ«11ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã™ã‚‹  
3. ã“ã®ã‚»ãƒ«ï¼ˆã‚»ãƒ«13ï¼‰ã‚’å†å®Ÿè¡Œã™ã‚‹

âš ï¸ å…ˆã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºã‚’å®Œäº†ã—ã¦ã‹ã‚‰åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
"""
except Exception as e:
    print(f"âŒ LLMåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    analysis_result = f"LLMåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“‹ ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®å®Ÿè¡Œ
# MAGIC 
# MAGIC ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œã—ã¦ã€SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
# MAGIC 
# MAGIC ### è¨­å®šã«ã¤ã„ã¦
# MAGIC 
# MAGIC ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¨­å®šã¯**ã‚»ãƒ«1**ã§è¡Œã„ã¾ã™ï¼š
# MAGIC 
# MAGIC ```python
# MAGIC JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/simple0.json'
# MAGIC ```
# MAGIC 
# MAGIC **å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å½¢å¼:**
# MAGIC - Unity Catalog Volumes: `/Volumes/catalog/schema/volume/file.json`
# MAGIC - FileStore: `/FileStore/shared_uploads/username/profiler.json`
# MAGIC - DBFS: `/dbfs/FileStore/shared_uploads/username/profiler.json`
# MAGIC - DBFS URI: `dbfs:/FileStore/shared_uploads/username/profiler.json`

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸš€ ã‚»ãƒ«10: SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Ÿè¡Œ
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - è¨­å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
# MAGIC - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨åŸºæœ¬æƒ…å ±ã®è¡¨ç¤º
# MAGIC - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨å‡¦ç†åœæ­¢åˆ¶å¾¡

# COMMAND ----------

print("=" * 80)
print("ğŸš€ Databricks SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆ†æãƒ„ãƒ¼ãƒ«")
print("=" * 80)
print(f"ğŸ“ åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {JSON_FILE_PATH}")
print()

# SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
profiler_data = load_profiler_json(JSON_FILE_PATH)
if not profiler_data:
    print("âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    print("âš ï¸ å‡¦ç†ã‚’åœæ­¢ã—ã¾ã™ã€‚")
    # dbutils.notebook.exit("File loading failed")  # å®‰å…¨ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    raise RuntimeError("JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“Š ã‚»ãƒ«11: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºã¨æ¦‚è¦è¡¨ç¤º
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º
# MAGIC - ã‚¯ã‚¨ãƒªåŸºæœ¬æƒ…å ±ã®è¡¨ç¤º
# MAGIC - å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®è¨ˆç®—ã¨è¡¨ç¤º
# MAGIC - Liquid Clusteringã®åˆ†æçµæœè¡¨ç¤º

# COMMAND ----------

# ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º
extracted_metrics = extract_performance_metrics(profiler_data)
print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")

# æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¦‚è¦è¡¨ç¤º
print("\n" + "=" * 50)
print("ğŸ“ˆ æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¦‚è¦")
print("=" * 50)

query_info = extracted_metrics['query_info']
overall_metrics = extracted_metrics['overall_metrics']
bottleneck_indicators = extracted_metrics['bottleneck_indicators']

print(f"ğŸ†” ã‚¯ã‚¨ãƒªID: {query_info['query_id']}")
print(f"ğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {query_info['status']}")
print(f"ğŸ‘¤ å®Ÿè¡Œãƒ¦ãƒ¼ã‚¶ãƒ¼: {query_info['user']}")
print(f"â±ï¸ å®Ÿè¡Œæ™‚é–“: {overall_metrics['total_time_ms']:,} ms ({overall_metrics['total_time_ms']/1000:.2f} sec)")
print(f"ğŸ’¾ èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿: {overall_metrics['read_bytes']/1024/1024/1024:.2f} GB")
print(f"ğŸ“ˆ å‡ºåŠ›è¡Œæ•°: {overall_metrics['rows_produced_count']:,} è¡Œ")
print(f"ğŸ“‰ èª­ã¿è¾¼ã¿è¡Œæ•°: {overall_metrics['rows_read_count']:,} è¡Œ")
print(f"ğŸ¯ ãƒ‡ãƒ¼ã‚¿é¸æŠæ€§: {bottleneck_indicators.get('data_selectivity', 0):.4f} ({bottleneck_indicators.get('data_selectivity', 0)*100:.2f}%)")
print(f"ğŸ”§ ã‚¹ãƒ†ãƒ¼ã‚¸æ•°: {len(extracted_metrics['stage_metrics'])}")
print(f"ğŸ—ï¸ ãƒãƒ¼ãƒ‰æ•°: {len(extracted_metrics['node_metrics'])}")

# Liquid Clusteringåˆ†æçµæœã®è¡¨ç¤º
liquid_analysis = extracted_metrics['liquid_clustering_analysis']
liquid_summary = liquid_analysis.get('summary', {})
print(f"ğŸ—‚ï¸ Liquid Clusteringå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {liquid_summary.get('tables_identified', 0)}")
print(f"ğŸ“Š é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {liquid_summary.get('high_impact_tables', 0)}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ” ã‚»ãƒ«12: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™è©³ç´°è¡¨ç¤ºã¨æ™‚é–“æ¶ˆè²»TOP10
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - Photon ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆ©ç”¨çŠ¶æ³ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
# MAGIC - ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œã¨ä¸¦åˆ—åº¦ã®å•é¡Œæ¤œå‡º
# MAGIC - å„ç¨®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®è©³ç´°è¡¨ç¤º
# MAGIC - æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10ã®åˆ†æ

# COMMAND ----------

# ğŸ“‹ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è©³ç´°è¡¨ç¤º
print("\n" + "=" * 50)
print("ğŸ” ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™è©³ç´°")
print("=" * 50)

# Photoné–¢é€£æŒ‡æ¨™
photon_enabled = overall_metrics.get('photon_enabled', False)
photon_utilization_ratio = overall_metrics.get('photon_utilization_ratio', 0)
photon_utilization = min(photon_utilization_ratio * 100, 100.0)  # æœ€å¤§100%ã«åˆ¶é™
photon_emoji = "âœ…" if photon_enabled and photon_utilization > 80 else "âš ï¸" if photon_enabled else "âŒ"

# åˆ©ç”¨ç‡ã«é–¢ã™ã‚‹è©³ç´°æƒ…å ±
if photon_enabled:
    photon_total_ms = overall_metrics.get('photon_total_time_ms', 0)
    task_total_ms = overall_metrics.get('task_total_time_ms', 0)
    print(f"{photon_emoji} Photonã‚¨ãƒ³ã‚¸ãƒ³: æœ‰åŠ¹ (åˆ©ç”¨ç‡: {photon_utilization:.1f}%)")
    print(f"   ğŸ“Š Photonå®Ÿè¡Œæ™‚é–“: {photon_total_ms:,} ms | ã‚¿ã‚¹ã‚¯åˆè¨ˆæ™‚é–“: {task_total_ms:,} ms")
else:
    print(f"{photon_emoji} Photonã‚¨ãƒ³ã‚¸ãƒ³: ç„¡åŠ¹")

# ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«é–¢é€£æŒ‡æ¨™
shuffle_count = bottleneck_indicators.get('shuffle_operations_count', 0)
has_shuffle_bottleneck = bottleneck_indicators.get('has_shuffle_bottleneck', False)
has_low_parallelism = bottleneck_indicators.get('has_low_parallelism', False)
low_parallelism_count = bottleneck_indicators.get('low_parallelism_stages_count', 0)

shuffle_emoji = "ğŸš¨" if has_shuffle_bottleneck else "âš ï¸" if shuffle_count > 5 else "âœ…"
print(f"{shuffle_emoji} ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ: {shuffle_count}å› ({'ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚ã‚Š' if has_shuffle_bottleneck else 'æ­£å¸¸'})")

parallelism_emoji = "ğŸš¨" if has_low_parallelism else "âœ…"
print(f"{parallelism_emoji} ä¸¦åˆ—åº¦: {'å•é¡Œã‚ã‚Š' if has_low_parallelism else 'é©åˆ‡'} (ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸: {low_parallelism_count}å€‹)")

print()
print("ğŸ“Š ãã®ä»–ã®æŒ‡æ¨™:")

for key, value in bottleneck_indicators.items():
    # æ–°ã—ãè¿½åŠ ã—ãŸæŒ‡æ¨™ã¯ä¸Šè¨˜ã§è¡¨ç¤ºæ¸ˆã¿ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
    if key in ['shuffle_operations_count', 'has_shuffle_bottleneck', 'has_low_parallelism', 
               'low_parallelism_stages_count', 'total_shuffle_time_ms', 'shuffle_time_ratio',
               'slowest_shuffle_duration_ms', 'slowest_shuffle_node', 'low_parallelism_details',
               'average_low_parallelism']:
        continue
        
    if 'ratio' in key:
        emoji = "ğŸ“Š" if value < 0.1 else "âš ï¸" if value < 0.3 else "ğŸš¨"
        print(f"{emoji} {key}: {value:.3f} ({value*100:.1f}%)")
    elif 'bytes' in key and key != 'has_spill':
        if value > 0:
            emoji = "ğŸ’¾" if value < 1024*1024*1024 else "âš ï¸"  # 1GBæœªæº€ã¯æ™®é€šã€ä»¥ä¸Šã¯æ³¨æ„
            print(f"{emoji} {key}: {value:,} bytes ({value/1024/1024:.2f} MB)")
    elif key == 'has_spill':
        emoji = "âŒ" if not value else "âš ï¸"
        print(f"{emoji} {key}: {'ã‚ã‚Š' if value else 'ãªã—'}")
    elif 'duration' in key:
        emoji = "â±ï¸"
        print(f"{emoji} {key}: {value:,} ms ({value/1000:.2f} sec)")
    else:
        emoji = "â„¹ï¸"
        print(f"{emoji} {key}: {value}")

print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ’¾ ã‚»ãƒ«13: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜ã¨æ™‚é–“æ¶ˆè²»åˆ†æ
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - æŠ½å‡ºã—ãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®JSONå½¢å¼ã§ã®ä¿å­˜
# MAGIC - setå‹ã‹ã‚‰listå‹ã¸ã®å¤‰æ›å‡¦ç†
# MAGIC - æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10ã®è©³ç´°åˆ†æ
# MAGIC - ã‚¹ãƒ”ãƒ«æ¤œå‡ºã¨ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼åˆ†æ

# COMMAND ----------

# ğŸ’¾ æŠ½å‡ºã—ãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
def convert_sets_to_lists(obj):
    """setå‹ã‚’listå‹ã«å¤‰æ›ã—ã¦JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ã«ã™ã‚‹"""
    if isinstance(obj, set):
        return list(obj)
    elif isinstance(obj, dict):
        return {key: convert_sets_to_lists(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_sets_to_lists(item) for item in obj]
    else:
        return obj

from datetime import datetime
metrics_timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
output_path = f'extracted_metrics_{metrics_timestamp}.json'
try:
    # setå‹ã‚’listå‹ã«å¤‰æ›ã—ã¦ã‹ã‚‰JSONã«ä¿å­˜
    serializable_metrics = convert_sets_to_lists(extracted_metrics)
    with open(output_path, 'w', encoding='utf-8') as file:
        json.dump(serializable_metrics, file, indent=2, ensure_ascii=False)
    print(f"âœ… æŠ½å‡ºãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
except Exception as e:
    print(f"âš ï¸ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™: {e}")
    print("âœ… åˆ†æã¯æ­£å¸¸ã«ç¶™ç¶šã•ã‚Œã¾ã™")

# ğŸŒ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10
print(f"\nğŸŒ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10")
print("=" * 80)
print("ğŸ“Š ã‚¢ã‚¤ã‚³ãƒ³èª¬æ˜: â±ï¸æ™‚é–“ ğŸ’¾ãƒ¡ãƒ¢ãƒª ğŸ”¥ğŸŒä¸¦åˆ—åº¦ ğŸ’¿ã‚¹ãƒ”ãƒ« âš–ï¸ã‚¹ã‚­ãƒ¥ãƒ¼")

# ãƒãƒ¼ãƒ‰ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
sorted_nodes = sorted(extracted_metrics['node_metrics'], 
                     key=lambda x: x['key_metrics'].get('durationMs', 0), 
                     reverse=True)

if sorted_nodes:
    # å…¨ä½“ã®å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
    total_duration = sum(node['key_metrics'].get('durationMs', 0) for node in sorted_nodes)
    
    print(f"ğŸ“Š å…¨ä½“å®Ÿè¡Œæ™‚é–“: {total_duration:,} ms ({total_duration/1000:.1f} sec)")
    print(f"ğŸ“ˆ TOP10åˆè¨ˆæ™‚é–“: {sum(node['key_metrics'].get('durationMs', 0) for node in sorted_nodes[:10]):,} ms")
    print()
    
    for i, node in enumerate(sorted_nodes[:10]):
        rows_num = node['key_metrics'].get('rowsNum', 0)
        duration_ms = node['key_metrics'].get('durationMs', 0)
        memory_mb = node['key_metrics'].get('peakMemoryBytes', 0) / 1024 / 1024
        
        # å…¨ä½“ã«å¯¾ã™ã‚‹æ™‚é–“ã®å‰²åˆã‚’è¨ˆç®—
        time_percentage = (duration_ms / max(total_duration, 1)) * 100
        
        # æ™‚é–“ã®é‡è¦åº¦ã«åŸºã¥ã„ã¦ã‚¢ã‚¤ã‚³ãƒ³ã‚’é¸æŠ
        if duration_ms >= 10000:  # 10ç§’ä»¥ä¸Š
            time_icon = "ï¿½"
            severity = "CRITICAL"
        elif duration_ms >= 5000:  # 5ç§’ä»¥ä¸Š
            time_icon = "ğŸŸ "
            severity = "HIGH"
        elif duration_ms >= 1000:  # 1ç§’ä»¥ä¸Š
            time_icon = "ğŸŸ¡"
            severity = "MEDIUM"
        else:
            time_icon = "ï¿½"
            severity = "LOW"
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ã‚¢ã‚¤ã‚³ãƒ³
        memory_icon = "ï¿½" if memory_mb < 100 else "âš ï¸" if memory_mb < 1000 else "ğŸš¨"
        
        # ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹ãƒãƒ¼ãƒ‰åã‚’å–å¾—
        raw_node_name = node['name']
        node_name = get_meaningful_node_name(node, extracted_metrics)
        short_name = node_name[:100] + "..." if len(node_name) > 100 else node_name
        
        # ä¸¦åˆ—åº¦æƒ…å ±ã®å–å¾—
        num_tasks = 0
        for stage in extracted_metrics.get('stage_metrics', []):
            if duration_ms > 0:  # ã“ã®ãƒãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’æ¨å®š
                num_tasks = max(num_tasks, stage.get('num_tasks', 0))
        
        # ãƒ‡ã‚£ã‚¹ã‚¯ã‚¹ãƒ”ãƒ«ã‚¢ã‚¦ãƒˆã®æ¤œå‡ºï¼ˆå¼·åŒ–ç‰ˆï¼‰
        detailed_metrics = node.get('detailed_metrics', {})
        spill_detected = False
        spill_bytes = 0
        spill_details = []
        

        
        # å¼·åŒ–ã•ã‚ŒãŸã‚¹ãƒ”ãƒ«æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆdetailed_metricsã¨ç”Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä¸¡æ–¹ã‚’ãƒã‚§ãƒƒã‚¯ï¼‰
        
        def check_spill_metric(metric_key, metric_label, metric_value):
            """ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹å…±é€šé–¢æ•°"""
            # ã‚ˆã‚Šå…·ä½“çš„ãªã‚¹ãƒ”ãƒ«é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            spill_patterns = ['SPILL', 'DISK', 'PRESSURE']
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã«ï¼‰
            is_spill_metric = False
            metric_key_clean = metric_key.upper().replace(' ', '').replace('-', '').replace('_', '')
            metric_label_clean = metric_label.upper().replace(' ', '').replace('-', '').replace('_', '')
            
            # æ™‚é–“é–¢é€£ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’é™¤å¤–ï¼ˆã‚¹ãƒ”ãƒ«å®¹é‡ã§ã¯ãªã„ï¼‰
            time_exclusions = ['TIME', 'DURATION', 'ELAPSED', 'LATENCY', 'DELAY']
            for exclusion in time_exclusions:
                if exclusion in metric_key_clean or exclusion in metric_label_clean:
                    return False
            
            # åŸºæœ¬çš„ãªã‚¹ãƒ”ãƒ«é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ¤œæŸ»
            for pattern in spill_patterns:
                if pattern in metric_key_clean or pattern in metric_label_clean:
                    is_spill_metric = True
                    break
            
            # ã‚ˆã‚Šå…·ä½“çš„ãªã‚¹ãƒ”ãƒ«é–¢é€£ã®çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³
            spill_combinations = [
                ('SPILL', 'DISK'),      # "spilled to disk"
                ('SPILL', 'MEMORY'),    # "spilled due to memory"
                ('BYTES', 'SPILL'),     # "bytes spilled"
                ('ROWS', 'SPILL'),      # "rows spilled"
                ('SINK', 'SPILL'),      # "Sink spill"
                ('SPILL', 'PRESSURE'),  # "spilled due to pressure"
            ]
            
            for word1, word2 in spill_combinations:
                if (word1 in metric_key_clean and word2 in metric_key_clean) or \
                   (word1 in metric_label_clean and word2 in metric_label_clean):
                    is_spill_metric = True
                    break
            
            return is_spill_metric
        
        # 1. detailed_metricsã‹ã‚‰ã®ã‚¹ãƒ”ãƒ«æ¤œå‡º
        for metric_key, metric_info in detailed_metrics.items():
            metric_value = metric_info.get('value', 0)
            metric_label = metric_info.get('label', '')
            
            # ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒæ¤œå‡ºã•ã‚Œã€å€¤ãŒ0ã‚ˆã‚Šå¤§ãã„å ´åˆ
            if check_spill_metric(metric_key, metric_label, metric_value) and metric_value > 0:
                spill_detected = True
                spill_bytes += metric_value
                spill_details.append({
                    'metric_name': metric_key,
                    'value': metric_value,
                    'label': metric_label,
                    'source': 'detailed_metrics'
                })
        
        # 2. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç”Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ã®ç›´æ¥æ¤œå‡ºï¼ˆdetailed_metricsãŒä¸å®Œå…¨ãªå ´åˆï¼‰
        raw_metrics = node.get('metrics', [])
        for metric in raw_metrics:
            metric_key = metric.get('key', '')
            metric_label = metric.get('label', '')
            metric_value = metric.get('value', 0)
            
            # æ—¢ã«detailed_metricsã§å‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
            already_processed = any(detail['metric_name'] == metric_key for detail in spill_details)
            
            if not already_processed and check_spill_metric(metric_key, metric_label, metric_value) and metric_value > 0:
                spill_detected = True
                spill_bytes += metric_value
                spill_details.append({
                    'metric_name': metric_key,
                    'value': metric_value,
                    'label': metric_label,
                    'source': 'raw_metrics'
                })
        
        # 3. key_metricsã‹ã‚‰ã‚‚ã‚¹ãƒ”ãƒ«æƒ…å ±ã‚’ç¢ºèª
        key_metrics = node.get('key_metrics', {})
        for key_metric_name, key_metric_value in key_metrics.items():
            if ('spill' in key_metric_name.lower() or 'disk' in key_metric_name.lower()) and key_metric_value > 0:
                spill_detected = True
                spill_bytes += key_metric_value
                spill_details.append({
                    'metric_name': f"key_metrics.{key_metric_name}",
                    'value': key_metric_value,
                    'label': f"Key metric: {key_metric_name}",
                    'source': 'key_metrics'
                })
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ã®æ¤œå‡ºï¼ˆè¡Œæ•°ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‹ã‚‰æ¨å®šï¼‰
        skew_detected = False
        if rows_num > 0 and memory_mb > 0:
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒè¡Œæ•°ã«æ¯”ã¹ã¦ç•°å¸¸ã«é«˜ã„å ´åˆã¯ã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§
            memory_per_row = memory_mb * 1024 * 1024 / rows_num  # bytes per row
            if memory_per_row > 10000:  # 1è¡Œã‚ãŸã‚Š10KBä»¥ä¸Šã¯é«˜ã„
                skew_detected = True
        
        # ã¾ãŸã¯å®Ÿè¡Œæ™‚é–“ãŒè¡Œæ•°ã«æ¯”ã¹ã¦ç•°å¸¸ã«é•·ã„å ´åˆ
        if rows_num > 0 and duration_ms > 0:
            ms_per_thousand_rows = (duration_ms * 1000) / rows_num
            if ms_per_thousand_rows > 1000:  # 1000è¡Œã‚ãŸã‚Š1ç§’ä»¥ä¸Šã¯é…ã„
                skew_detected = True
        
        # ä¸¦åˆ—åº¦ã‚¢ã‚¤ã‚³ãƒ³
        parallelism_icon = "ğŸ”¥" if num_tasks >= 10 else "âš ï¸" if num_tasks >= 5 else "ğŸŒ"
        # ã‚¹ãƒ”ãƒ«ã‚¢ã‚¤ã‚³ãƒ³
        spill_icon = "ğŸ’¿" if spill_detected else "âœ…"
        # ã‚¹ã‚­ãƒ¥ãƒ¼ã‚¢ã‚¤ã‚³ãƒ³
        skew_icon = "âš–ï¸" if skew_detected else "âœ…"
        
        print(f"{i+1:2d}. {time_icon}{memory_icon}{parallelism_icon}{spill_icon}{skew_icon} [{severity:8}] {short_name}")
        print(f"    â±ï¸  å®Ÿè¡Œæ™‚é–“: {duration_ms:>8,} ms ({duration_ms/1000:>6.1f} sec) - å…¨ä½“ã® {time_percentage:>5.1f}%")
        print(f"    ğŸ“Š å‡¦ç†è¡Œæ•°: {rows_num:>8,} è¡Œ")
        print(f"    ğŸ’¾ ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: {memory_mb:>6.1f} MB")
        print(f"    ğŸ”§ ä¸¦åˆ—åº¦: {num_tasks:>3d} ã‚¿ã‚¹ã‚¯ | ğŸ’¿ ã‚¹ãƒ”ãƒ«: {'ã‚ã‚Š' if spill_detected else 'ãªã—'} | âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼: {'ã‚ã‚Š' if skew_detected else 'ãªã—'}")
        
        # åŠ¹ç‡æ€§æŒ‡æ¨™ï¼ˆè¡Œ/ç§’ï¼‰ã‚’è¨ˆç®—
        if duration_ms > 0:
            rows_per_sec = (rows_num * 1000) / duration_ms
            print(f"    ğŸš€ å‡¦ç†åŠ¹ç‡: {rows_per_sec:>8,.0f} è¡Œ/ç§’")
        
        # ã‚¹ãƒ”ãƒ«è©³ç´°æƒ…å ±ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        if spill_detected:
            if spill_bytes > 0:
                print(f"    ğŸ’¿ ã‚¹ãƒ”ãƒ«è©³ç´°: {spill_bytes/1024/1024:.1f} MB")
            


        
        # ãƒãƒ¼ãƒ‰IDã‚‚è¡¨ç¤º
        print(f"    ğŸ†” ãƒãƒ¼ãƒ‰ID: {node.get('node_id', node.get('id', 'N/A'))}")
        print()
        
else:
    print("âš ï¸ ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

print()

# ğŸ”¥ Sparkã‚¹ãƒ†ãƒ¼ã‚¸å®Ÿè¡Œåˆ†æ
if extracted_metrics['stage_metrics']:
    print("\nğŸ”¥ Sparkã‚¹ãƒ†ãƒ¼ã‚¸å®Ÿè¡Œåˆ†æ")
    print("=" * 60)
    
    stage_metrics = extracted_metrics['stage_metrics']
    total_stages = len(stage_metrics)
    completed_stages = len([s for s in stage_metrics if s.get('status') == 'COMPLETE'])
    failed_stages = len([s for s in stage_metrics if s.get('num_failed_tasks', 0) > 0])
    
    print(f"ğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¸æ¦‚è¦: å…¨{total_stages}ã‚¹ãƒ†ãƒ¼ã‚¸ (å®Œäº†:{completed_stages}, å¤±æ•—ã‚¿ã‚¹ã‚¯ã‚ã‚Š:{failed_stages})")
    print()
    
    # ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
    sorted_stages = sorted(stage_metrics, key=lambda x: x.get('duration_ms', 0), reverse=True)
    
    print("â±ï¸ ã‚¹ãƒ†ãƒ¼ã‚¸å®Ÿè¡Œæ™‚é–“ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
    print("-" * 60)
    
    for i, stage in enumerate(sorted_stages[:5]):  # TOP5ã‚¹ãƒ†ãƒ¼ã‚¸ã®ã¿è¡¨ç¤º
        stage_id = stage.get('stage_id', 'N/A')
        status = stage.get('status', 'UNKNOWN')
        duration_ms = stage.get('duration_ms', 0)
        num_tasks = stage.get('num_tasks', 0)
        failed_tasks = stage.get('num_failed_tasks', 0)
        complete_tasks = stage.get('num_complete_tasks', 0)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã«å¿œã˜ãŸã‚¢ã‚¤ã‚³ãƒ³
        if status == 'COMPLETE' and failed_tasks == 0:
            status_icon = "âœ…"
        elif failed_tasks > 0:
            status_icon = "âš ï¸"
        else:
            status_icon = "â“"
        
        # ä¸¦åˆ—åº¦ã‚¢ã‚¤ã‚³ãƒ³
        parallelism_icon = "ğŸ”¥" if num_tasks >= 10 else "âš ï¸" if num_tasks >= 5 else "ğŸŒ"
        
        # å®Ÿè¡Œæ™‚é–“ã®é‡è¦åº¦
        if duration_ms >= 10000:
            time_icon = "ğŸ”´"
            severity = "CRITICAL"
        elif duration_ms >= 5000:
            time_icon = "ğŸŸ "
            severity = "HIGH"
        elif duration_ms >= 1000:
            time_icon = "ğŸŸ¡"
            severity = "MEDIUM"
        else:
            time_icon = "ğŸŸ¢"
            severity = "LOW"
        
        print(f"{i+1}. {status_icon}{parallelism_icon}{time_icon} ã‚¹ãƒ†ãƒ¼ã‚¸ {stage_id} [{severity:8}]")
        print(f"   â±ï¸ å®Ÿè¡Œæ™‚é–“: {duration_ms:,} ms ({duration_ms/1000:.1f} sec)")
        print(f"   ğŸ”§ ã‚¿ã‚¹ã‚¯: {complete_tasks}/{num_tasks} å®Œäº† (å¤±æ•—: {failed_tasks})")
        
        # ã‚¿ã‚¹ã‚¯ã‚ãŸã‚Šã®å¹³å‡æ™‚é–“
        if num_tasks > 0:
            avg_task_time = duration_ms / num_tasks
            print(f"   ğŸ“Š å¹³å‡ã‚¿ã‚¹ã‚¯æ™‚é–“: {avg_task_time:.1f} ms")
        
        # åŠ¹ç‡æ€§è©•ä¾¡
        if num_tasks > 0:
            task_efficiency = "é«˜åŠ¹ç‡" if num_tasks >= 10 and failed_tasks == 0 else "è¦æ”¹å–„" if failed_tasks > 0 else "æ¨™æº–"
            print(f"   ğŸ¯ åŠ¹ç‡æ€§: {task_efficiency}")
        
        print()
    
    if len(sorted_stages) > 5:
        print(f"... ä»– {len(sorted_stages) - 5} ã‚¹ãƒ†ãƒ¼ã‚¸")
    
    # å•é¡Œã®ã‚ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    problematic_stages = [s for s in stage_metrics if s.get('num_failed_tasks', 0) > 0 or s.get('duration_ms', 0) > 30000]
    if problematic_stages:
        print("\nğŸš¨ æ³¨æ„ãŒå¿…è¦ãªã‚¹ãƒ†ãƒ¼ã‚¸:")
        print("-" * 40)
        for stage in problematic_stages[:3]:
            stage_id = stage.get('stage_id', 'N/A')
            duration_sec = stage.get('duration_ms', 0) / 1000
            failed_tasks = stage.get('num_failed_tasks', 0)
            
            issues = []
            if failed_tasks > 0:
                issues.append(f"å¤±æ•—ã‚¿ã‚¹ã‚¯{failed_tasks}å€‹")
            if duration_sec > 30:
                issues.append(f"é•·æ™‚é–“å®Ÿè¡Œ({duration_sec:.1f}sec)")
            
            print(f"   âš ï¸ ã‚¹ãƒ†ãƒ¼ã‚¸ {stage_id}: {', '.join(issues)}")
    
    # DataFrameå½¢å¼ã§ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    print(f"\nğŸ“‹ è©³ç´°ãƒ‡ãƒ¼ã‚¿ (DataFrameå½¢å¼):")
    print("-" * 40)
    try:
        import pandas as pd
        # ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†ã—ã¦ã‚ã‹ã‚Šã‚„ã™ãè¡¨ç¤º
        display_data = []
        for stage in stage_metrics:
            display_data.append({
                'ã‚¹ãƒ†ãƒ¼ã‚¸ID': stage.get('stage_id', 'N/A'),
                'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': stage.get('status', 'UNKNOWN'),
                'å®Ÿè¡Œæ™‚é–“(ç§’)': round(stage.get('duration_ms', 0) / 1000, 1),
                'ã‚¿ã‚¹ã‚¯æ•°': stage.get('num_tasks', 0),
                'å®Œäº†ã‚¿ã‚¹ã‚¯': stage.get('num_complete_tasks', 0),
                'å¤±æ•—ã‚¿ã‚¹ã‚¯': stage.get('num_failed_tasks', 0),
                'å¹³å‡ã‚¿ã‚¹ã‚¯æ™‚é–“(ms)': round(stage.get('duration_ms', 0) / max(stage.get('num_tasks', 1), 1), 1)
            })
        
        df = pd.DataFrame(display_data)
        df = df.sort_values('å®Ÿè¡Œæ™‚é–“(ç§’)', ascending=False)
        print(df.to_string(index=False))
        
    except Exception as e:
        print(f"âš ï¸ DataFrameè¡¨ç¤ºã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
        # ã‚·ãƒ³ãƒ—ãƒ«ãªè¡¨å½¢å¼ã§è¡¨ç¤º
        print(f"{'ã‚¹ãƒ†ãƒ¼ã‚¸ID':<10} {'å®Ÿè¡Œæ™‚é–“':<10} {'ã‚¿ã‚¹ã‚¯':<8} {'å¤±æ•—':<6} {'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹'}")
        print("-" * 50)
        for stage in sorted_stages:
            stage_id = str(stage.get('stage_id', 'N/A'))[:8]
            duration_sec = stage.get('duration_ms', 0) / 1000
            num_tasks = stage.get('num_tasks', 0)
            failed = stage.get('num_failed_tasks', 0)
            status = stage.get('status', 'UNKNOWN')[:8]
            print(f"{stage_id:<10} {duration_sec:<10.1f} {num_tasks:<8} {failed:<6} {status}")
    
    print()
else:
    print("\nğŸ”¥ Sparkã‚¹ãƒ†ãƒ¼ã‚¸å®Ÿè¡Œåˆ†æ")
    print("=" * 60)
    print("âš ï¸ ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    print()

print()

# COMMAND ----------

# ğŸ—‚ï¸ Liquid Clusteringåˆ†æçµæœã®è©³ç´°è¡¨ç¤º
print("\n" + "=" * 50)
print("ğŸ—‚ï¸ Liquid Clusteringæ¨å¥¨åˆ†æ")
print("=" * 50)

liquid_analysis = extracted_metrics['liquid_clustering_analysis']

# æ¨å¥¨ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
recommended_tables = liquid_analysis.get('recommended_tables', {})
if recommended_tables:
    print("\nğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚«ãƒ©ãƒ :")
    for table_name, table_info in recommended_tables.items():
        clustering_cols = table_info.get('clustering_columns', [])
        scan_perf = table_info.get('scan_performance', {})
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«åã®è¡¨ç¤º
        print(f"\nğŸ“Š ãƒ†ãƒ¼ãƒ–ãƒ«: {table_name}")
        print(f"   ğŸ¯ æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚«ãƒ©ãƒ : {', '.join(clustering_cols)}")
        print(f"   ğŸ“ˆ ã‚¹ã‚­ãƒ£ãƒ³è¡Œæ•°: {scan_perf.get('rows_scanned', 0):,} è¡Œ")
        print(f"   â±ï¸ ã‚¹ã‚­ãƒ£ãƒ³æ™‚é–“: {scan_perf.get('scan_duration_ms', 0):,} ms")
        print(f"   ğŸš€ ã‚¹ã‚­ãƒ£ãƒ³åŠ¹ç‡: {scan_perf.get('efficiency_score', 0):.2f} è¡Œ/ms")
        
        # ã‚«ãƒ©ãƒ ã‚¹ã‚³ã‚¢è©³ç´°
        column_scores = table_info.get('column_scores', {})
        if column_scores:
            sorted_scores = sorted(column_scores.items(), key=lambda x: x[1], reverse=True)
            print(f"   ğŸ“Š ã‚«ãƒ©ãƒ é‡è¦åº¦: {', '.join([f'{col}({score})' for col, score in sorted_scores[:3]])}")

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å½±éŸ¿åˆ†æ
performance_impact = liquid_analysis.get('performance_impact', {})
print(f"\nğŸ”„ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šè¦‹è¾¼ã¿:")
print(f"   ğŸ“ˆ ã‚¹ã‚­ãƒ£ãƒ³æ”¹å–„: {performance_impact.get('potential_scan_improvement', 'N/A')}")
print(f"   ğŸ”€ Shuffleå‰Šæ¸›: {performance_impact.get('potential_shuffle_reduction', 'N/A')}")
print(f"   ğŸ† å…¨ä½“æ”¹å–„: {performance_impact.get('estimated_overall_improvement', 'N/A')}")

# ã‚«ãƒ©ãƒ ä½¿ç”¨çµ±è¨ˆï¼ˆè©³ç´°ç‰ˆï¼‰
filter_cols = set(liquid_analysis.get('filter_columns', []))
join_cols = set(liquid_analysis.get('join_columns', []))
groupby_cols = set(liquid_analysis.get('groupby_columns', []))
detailed_column_analysis = liquid_analysis.get('detailed_column_analysis', {})

if filter_cols or join_cols or groupby_cols:
    print(f"\nğŸ” ã‚«ãƒ©ãƒ ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³:")
    if filter_cols:
        print(f"   ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚«ãƒ©ãƒ  ({len(filter_cols)}å€‹): {', '.join(list(filter_cols)[:5])}")
    if join_cols:
        print(f"   ğŸ”— JOINã‚«ãƒ©ãƒ  ({len(join_cols)}å€‹): {', '.join(list(join_cols)[:5])}")
    if groupby_cols:
        print(f"   ğŸ“Š GROUP BYã‚«ãƒ©ãƒ  ({len(groupby_cols)}å€‹): {', '.join(list(groupby_cols)[:5])}")

# é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚«ãƒ©ãƒ ã®è©³ç´°è¡¨ç¤º
high_impact_columns = {col: analysis for col, analysis in detailed_column_analysis.items() 
                      if analysis.get('performance_impact') == 'high'}

if high_impact_columns:
    print(f"\nâ­ é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã‚«ãƒ©ãƒ è©³ç´°:")
    for col, analysis in list(high_impact_columns.items())[:5]:
        usage_contexts = ', '.join(analysis.get('usage_contexts', []))
        total_usage = analysis.get('total_usage', 0)
        print(f"   ğŸ¯ {col}")
        print(f"      ğŸ“ˆ é‡è¦åº¦ã‚¹ã‚³ã‚¢: {total_usage} | ä½¿ç”¨ç®‡æ‰€: {usage_contexts}")
        print(f"      ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼:{analysis.get('filter_usage_count', 0)} | JOIN:{analysis.get('join_usage_count', 0)} | GROUP BY:{analysis.get('groupby_usage_count', 0)}")

# ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æƒ…å ±
pushdown_filters = liquid_analysis.get('pushdown_filters', [])
if pushdown_filters:
    print(f"\nğŸ” ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ ({len(pushdown_filters)}ä»¶):")
    for i, filter_info in enumerate(pushdown_filters[:3]):
        node_name_display = filter_info.get('node_name', '')
        truncated_node_name = node_name_display[:100] + "..." if len(node_name_display) > 100 else node_name_display
        print(f"   {i+1}. ãƒãƒ¼ãƒ‰: {truncated_node_name}")
        filter_expression = filter_info.get('filter_expression', '')
        truncated_filter = filter_expression[:128] + "..." if len(filter_expression) > 128 else filter_expression
        print(f"      ğŸ“‹ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {truncated_filter}")
        print(f"      ğŸ”§ ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {filter_info.get('metric_key', '')}")

# SQLå®Ÿè£…ä¾‹
if recommended_tables:
    print(f"\nğŸ’¡ å®Ÿè£…ä¾‹:")
    for table_name, table_info in list(recommended_tables.items())[:2]:  # ä¸Šä½2ãƒ†ãƒ¼ãƒ–ãƒ«
        clustering_cols = table_info.get('clustering_columns', [])
        if clustering_cols:
            cluster_by_clause = ', '.join(clustering_cols)
            print(f"   ALTER TABLE {table_name} CLUSTER BY ({cluster_by_clause});")

print()

# COMMAND ----------

# ğŸ¤– è¨­å®šã•ã‚ŒãŸLLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ
provider = LLM_CONFIG["provider"]
if provider == "databricks":
    endpoint_name = LLM_CONFIG["databricks"]["endpoint_name"]
    print(f"ğŸ¤– Databricks Model Serving ({endpoint_name}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print(f"âš ï¸  Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ '{endpoint_name}' ãŒå¿…è¦ã§ã™")
elif provider == "openai":
    model = LLM_CONFIG["openai"]["model"]
    print(f"ğŸ¤– OpenAI ({model}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print("âš ï¸  OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™")
elif provider == "azure_openai":
    deployment = LLM_CONFIG["azure_openai"]["deployment_name"]
    print(f"ğŸ¤– Azure OpenAI ({deployment}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print("âš ï¸  Azure OpenAI APIã‚­ãƒ¼ã¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒå¿…è¦ã§ã™")
elif provider == "anthropic":
    model = LLM_CONFIG["anthropic"]["model"]
    print(f"ğŸ¤– Anthropic ({model}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print("âš ï¸  Anthropic APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™")

print("ğŸ“ åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç°¡æ½”åŒ–ã—ã¦ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒªã‚¹ã‚¯ã‚’è»½æ¸›ã—ã¦ã„ã¾ã™...")
print()

analysis_result = analyze_bottlenecks_with_llm(extracted_metrics)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¯ ã‚»ãƒ«14: LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã®è¡¨ç¤º
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«ã‚ˆã‚‹è©³ç´°åˆ†æçµæœã®è¡¨ç¤º
# MAGIC - ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šã¨æ”¹å–„ææ¡ˆã®å¯è¦–åŒ–
# MAGIC - åˆ†æçµæœã®æ•´å½¢ã¨èª­ã¿ã‚„ã™ã„è¡¨ç¤º

# COMMAND ----------

# ğŸ“Š åˆ†æçµæœã®è¡¨ç¤º
print("\n" + "=" * 80)
print(f"ğŸ¯ ã€{provider.upper()} LLM ã«ã‚ˆã‚‹ SQLãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã€‘")
print("=" * 80)
print()
print(analysis_result)
print()
print("=" * 80)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ’¾ ã‚»ãƒ«15: åˆ†æçµæœã®ä¿å­˜ã¨å®Œäº†ã‚µãƒãƒªãƒ¼
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - LLMåˆ†æçµæœã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ä¿å­˜
# MAGIC - åˆ†æå¯¾è±¡ã®åŸºæœ¬æƒ…å ±ã®è¨˜éŒ²
# MAGIC - å…¨ä½“å‡¦ç†ã®å®Œäº†ã‚µãƒãƒªãƒ¼è¡¨ç¤º
# MAGIC - å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§è¡¨ç¤º

# COMMAND ----------

# ğŸ’¾ åˆ†æçµæœã®ä¿å­˜ã¨å®Œäº†ã‚µãƒãƒªãƒ¼
from datetime import datetime
result_timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
result_output_path = f'bottleneck_analysis_result_{result_timestamp}.txt'
with open(result_output_path, 'w', encoding='utf-8') as file:
    file.write("Databricks SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ\n")
    file.write("=" * 60 + "\n\n")
    file.write(f"ã‚¯ã‚¨ãƒªID: {extracted_metrics['query_info']['query_id']}\n")
    file.write(f"åˆ†ææ—¥æ™‚: {pd.Timestamp.now()}\n")
    file.write(f"å®Ÿè¡Œæ™‚é–“: {extracted_metrics['overall_metrics']['total_time_ms']:,} ms\n")
    file.write("=" * 60 + "\n\n")
    file.write(analysis_result)
print(f"âœ… åˆ†æçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {result_output_path}")

# æœ€çµ‚çš„ãªã‚µãƒãƒªãƒ¼
print("\n" + "ğŸ‰" * 20)
print("ğŸ ã€å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼ã€‘")
print("ğŸ‰" * 20)
print("âœ… SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
print(f"âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºå®Œäº† ({output_path})")
print("âœ… Databricks Claude 3.7 Sonnetã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Œäº†")
print(f"âœ… åˆ†æçµæœä¿å­˜å®Œäº† ({result_output_path})")
print()
print("ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
print(f"   ğŸ“„ {output_path}")
print(f"   ğŸ“„ {result_output_path}")
print()
print("ğŸš€ åˆ†æå®Œäº†ï¼çµæœã‚’ç¢ºèªã—ã¦ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã«ãŠå½¹ç«‹ã¦ãã ã•ã„ã€‚")
print("ğŸ‰" * 20)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ”§ ã‚»ãƒ«16: SQLæœ€é©åŒ–é–¢é€£é–¢æ•°å®šç¾©
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - `extract_original_query_from_profiler_data`: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã‚’æŠ½å‡º
# MAGIC - `generate_optimized_query_with_llm`: LLMåˆ†æçµæœã«åŸºã¥ãã‚¯ã‚¨ãƒªæœ€é©åŒ–
# MAGIC - `save_optimized_sql_files`: æœ€é©åŒ–çµæœã®å„ç¨®ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜

# COMMAND ----------

def extract_original_query_from_profiler_data(profiler_data: Dict[str, Any]) -> str:
    """
    ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã‚’æŠ½å‡º
    """
    
    # è¤‡æ•°ã®å ´æ‰€ã‹ã‚‰SQLã‚¯ã‚¨ãƒªã‚’æ¢ã™
    query_candidates = []
    
    # 1. query.queryText ã‹ã‚‰æŠ½å‡º
    if 'query' in profiler_data and 'queryText' in profiler_data['query']:
        query_text = profiler_data['query']['queryText']
        if query_text and query_text.strip():
            query_candidates.append(query_text.strip())
    
    # 2. metadata ã‹ã‚‰æŠ½å‡º
    if 'metadata' in profiler_data:
        metadata = profiler_data['metadata']
        for key, value in metadata.items():
            if 'sql' in key.lower() or 'query' in key.lower():
                if isinstance(value, str) and value.strip():
                    query_candidates.append(value.strip())
    
    # 3. graphs ã® metadata ã‹ã‚‰æŠ½å‡º
    if 'graphs' in profiler_data:
        for graph in profiler_data['graphs']:
            nodes = graph.get('nodes', [])
            for node in nodes:
                node_metadata = node.get('metadata', [])
                for meta in node_metadata:
                    if meta.get('key', '').upper() in ['SQL', 'QUERY', 'SQL_TEXT']:
                        value = meta.get('value', '')
                        if value and value.strip():
                            query_candidates.append(value.strip())
    
    # æœ€ã‚‚é•·ã„ã‚¯ã‚¨ãƒªã‚’é¸æŠï¼ˆé€šå¸¸ã€æœ€ã‚‚å®Œå…¨ãªã‚¯ã‚¨ãƒªï¼‰
    if query_candidates:
        original_query = max(query_candidates, key=len)
        return original_query
    
    return ""

def generate_optimized_query_with_llm(original_query: str, analysis_result: str, metrics: Dict[str, Any]) -> str:
    """
    LLMåˆ†æçµæœã«åŸºã¥ã„ã¦SQLã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–
    """
    
    # æœ€é©åŒ–ã®ãŸã‚ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’æº–å‚™
    optimization_context = []
    
    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æƒ…å ±ã®æŠ½å‡º
    bottlenecks = metrics.get('bottleneck_indicators', {})
    
    if bottlenecks.get('has_spill', False):
        spill_gb = bottlenecks.get('spill_bytes', 0) / 1024 / 1024 / 1024
        optimization_context.append(f"ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ: {spill_gb:.1f}GB - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®æ”¹å–„ãŒå¿…è¦")
    
    if bottlenecks.get('has_shuffle_bottleneck', False):
        optimization_context.append("ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ - JOINã¨GROUP BYã®æœ€é©åŒ–ãŒå¿…è¦")
    
    if bottlenecks.get('cache_hit_ratio', 0) < 0.5:
        optimization_context.append("ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ä½ä¸‹ - ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ€é©åŒ–ãŒå¿…è¦")
    
    # Liquid Clusteringæ¨å¥¨æƒ…å ±
    liquid_analysis = metrics.get('liquid_clustering_analysis', {})
    recommended_tables = liquid_analysis.get('recommended_tables', {})
    
    clustering_recommendations = []
    for table, info in recommended_tables.items():
        cols = info.get('clustering_columns', [])
        if cols:
            clustering_recommendations.append(f"ãƒ†ãƒ¼ãƒ–ãƒ« {table}: {', '.join(cols)} ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¨å¥¨")
    
    # æœ€é©åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
    optimization_prompt = f"""
ã‚ãªãŸã¯Databricksã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«SQLã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚

ã€å…ƒã®SQLã‚¯ã‚¨ãƒªã€‘
```sql
{original_query}
```

ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æçµæœã€‘
{analysis_result}

ã€ç‰¹å®šã•ã‚ŒãŸãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã€‘
{chr(10).join(optimization_context) if optimization_context else "ä¸»è¦ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"}

ã€Liquid Clusteringæ¨å¥¨ã€‘
{chr(10).join(clustering_recommendations) if clustering_recommendations else "ç‰¹åˆ¥ãªæ¨å¥¨äº‹é …ã¯ã‚ã‚Šã¾ã›ã‚“"}

ã€æœ€é©åŒ–è¦æ±‚ã€‘
1. ä¸Šè¨˜ã®åˆ†æçµæœã«åŸºã¥ã„ã¦ã€å…ƒã®SQLã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„
2. æœ€é©åŒ–ã®ãƒã‚¤ãƒ³ãƒˆã‚’å…·ä½“çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„
3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã®è¦‹è¾¼ã¿ã‚’å®šé‡çš„ã«ç¤ºã—ã¦ãã ã•ã„
4. å®Ÿè¡Œå¯èƒ½ãªSQLã‚³ãƒ¼ãƒ‰ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„

ã€å‡ºåŠ›å½¢å¼ã€‘
## ğŸš€ æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒª

```sql
-- æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã‚’ã“ã“ã«è¨˜è¿°
[æœ€é©åŒ–ã•ã‚ŒãŸSQL]
```

## ğŸ“Š æœ€é©åŒ–ã®ãƒã‚¤ãƒ³ãƒˆ

1. **[æœ€é©åŒ–é …ç›®1]**: [èª¬æ˜]
2. **[æœ€é©åŒ–é …ç›®2]**: [èª¬æ˜]
3. **[æœ€é©åŒ–é …ç›®3]**: [èª¬æ˜]

## ğŸ“ˆ æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ

- **å®Ÿè¡Œæ™‚é–“**: [ç¾åœ¨] â†’ [æœ€é©åŒ–å¾Œ] (æ”¹å–„ç‡: [XX%])
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: [æ”¹å–„å†…å®¹]
- **ã‚¹ãƒ”ãƒ«å‰Šæ¸›**: [æ”¹å–„å†…å®¹]

æ³¨æ„ï¼šå®Ÿéš›ã®ç’°å¢ƒã§å®Ÿè¡Œå‰ã«ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®å‹•ä½œç¢ºèªã‚’æ¨å¥¨ã—ã¾ã™ã€‚
"""

    # è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
    provider = LLM_CONFIG["provider"]
    
    try:
        if provider == "databricks":
            optimized_result = _call_databricks_llm(optimization_prompt)
        elif provider == "openai":
            optimized_result = _call_openai_llm(optimization_prompt)
        elif provider == "azure_openai":
            optimized_result = _call_azure_openai_llm(optimization_prompt)
        elif provider == "anthropic":
            optimized_result = _call_anthropic_llm(optimization_prompt)
        else:
            return "âš ï¸ è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒèªè­˜ã§ãã¾ã›ã‚“"
        
        return optimized_result
        
    except Exception as e:
        return f"âš ï¸ SQLæœ€é©åŒ–ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def save_optimized_sql_files(original_query: str, optimized_result: str, metrics: Dict[str, Any]) -> Dict[str, str]:
    """
    æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œå¯èƒ½ãªå½¢ã§ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    """
    
    import re
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    query_id = metrics.get('query_info', {}).get('query_id', 'unknown')
    
    # ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
    original_filename = f"original_query_{timestamp}.sql"
    with open(original_filename, 'w', encoding='utf-8') as f:
        f.write(f"-- ã‚ªãƒªã‚¸ãƒŠãƒ«SQLã‚¯ã‚¨ãƒª\n")
        f.write(f"-- ã‚¯ã‚¨ãƒªID: {query_id}\n")
        f.write(f"-- æŠ½å‡ºæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"-- ãƒ•ã‚¡ã‚¤ãƒ«: {original_filename}\n\n")
        f.write(original_query)
    
    # æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®æŠ½å‡ºã¨ä¿å­˜
    optimized_filename = f"optimized_query_{timestamp}.sql"
    
    # æœ€é©åŒ–çµæœã‹ã‚‰SQLã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    sql_pattern = r'```sql\s*(.*?)\s*```'
    sql_matches = re.findall(sql_pattern, optimized_result, re.DOTALL | re.IGNORECASE)
    
    optimized_sql = ""
    if sql_matches:
        # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸSQLãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½¿ç”¨
        optimized_sql = sql_matches[0].strip()
    else:
        # SQLãƒ–ãƒ­ãƒƒã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€SQLé–¢é€£ã®è¡Œã‚’æŠ½å‡º
        lines = optimized_result.split('\n')
        sql_lines = []
        in_sql_section = False
        
        for line in lines:
            if any(keyword in line.upper() for keyword in ['SELECT', 'FROM', 'WHERE', 'WITH', 'CREATE']):
                in_sql_section = True
            
            if in_sql_section:
                if line.strip().startswith('#') or line.strip().startswith('*'):
                    in_sql_section = False
                else:
                    sql_lines.append(line)
        
        optimized_sql = '\n'.join(sql_lines).strip()
    
    # æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
    with open(optimized_filename, 'w', encoding='utf-8') as f:
        f.write(f"-- æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒª\n")
        f.write(f"-- å…ƒã‚¯ã‚¨ãƒªID: {query_id}\n")
        f.write(f"-- æœ€é©åŒ–æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"-- ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒª: {original_filename}\n")
        f.write(f"-- ãƒ•ã‚¡ã‚¤ãƒ«: {optimized_filename}\n\n")
        
        if optimized_sql:
            f.write(optimized_sql)
        else:
            f.write("-- âš ï¸ SQLã‚³ãƒ¼ãƒ‰ã®è‡ªå‹•æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ\n")
            f.write("-- ä»¥ä¸‹ã¯æœ€é©åŒ–åˆ†æã®å…¨çµæœã§ã™:\n\n")
            f.write(f"/*\n{optimized_result}\n*/")
    
    # åˆ†æãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
    report_filename = f"optimization_report_{timestamp}.md"
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(f"# SQLæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ\n\n")
        f.write(f"**ã‚¯ã‚¨ãƒªID**: {query_id}\n")
        f.write(f"**æœ€é©åŒ–æ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ•ã‚¡ã‚¤ãƒ«**: {original_filename}\n")
        f.write(f"**æœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ«**: {optimized_filename}\n\n")
        f.write(f"## æœ€é©åŒ–åˆ†æçµæœ\n\n")
        f.write(optimized_result)
        f.write(f"\n\n## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹å‚è€ƒæƒ…å ±\n\n")
        
        # ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¿½åŠ 
        overall_metrics = metrics.get('overall_metrics', {})
        f.write(f"- **å®Ÿè¡Œæ™‚é–“**: {overall_metrics.get('total_time_ms', 0):,} ms\n")
        f.write(f"- **èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿**: {overall_metrics.get('read_bytes', 0) / 1024 / 1024 / 1024:.2f} GB\n")
        f.write(f"- **ã‚¹ãƒ”ãƒ«**: {metrics.get('bottleneck_indicators', {}).get('spill_bytes', 0) / 1024 / 1024 / 1024:.2f} GB\n")
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ
    test_script_filename = f"test_optimized_query_{timestamp}.py"
    with open(test_script_filename, 'w', encoding='utf-8') as f:
        # f-stringã®ä¸­ã§ä¸‰é‡å¼•ç”¨ç¬¦ã‚’å«ã‚€å ´åˆã®ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†
        escaped_original_query = original_query.replace('"""', '\\"""')
        escaped_optimized_sql = optimized_sql.replace('"""', '\\"""') if optimized_sql else '-- SQLã‚³ãƒ¼ãƒ‰ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ'
        
        test_script_content = f"""#!/usr/bin/env python3
\"\"\"
æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒªã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
ã‚¯ã‚¨ãƒªID: {query_id}
ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {timestamp}
\"\"\"

# Databricksç’°å¢ƒã§ã®å®Ÿè¡Œä¾‹
def test_optimized_query():
    
    # å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    try:
        from pyspark.sql import SparkSession
        import time
    except ImportError as e:
        print(f"âš ï¸ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {{e}}")
        return
    
    # Sparkã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å–å¾—
    spark = SparkSession.builder.appName("OptimizedQueryTest").getOrCreate()
    
    print("ğŸš€ æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒªã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("=" * 60)
    
    # ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®å®Ÿè¡Œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    print("\\nğŸ“Š ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®å®Ÿè¡Œ...")
    original_sql = \"\"\"
{escaped_original_query}
    \"\"\"
    
    start_time = time.time()
    try:
        # original_result = spark.sql(original_sql)
        # original_count = original_result.count()
        print("âš ï¸ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®å®Ÿè¡Œã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã•ã‚Œã¦ã„ã¾ã™")
        print("   å¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ¡ãƒ³ãƒˆã‚’è§£é™¤ã—ã¦ãã ã•ã„")
        original_execution_time = 0
    except Exception as e:
        print(f"âŒ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {{e}}")
        original_execution_time = 0
    
    original_execution_time = time.time() - start_time
    
    # æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®å®Ÿè¡Œ
    print("\\nğŸš€ æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®å®Ÿè¡Œ...")
    optimized_sql = \"\"\"
{escaped_optimized_sql}
    \"\"\"
    
    start_time = time.time()
    try:
        # optimized_result = spark.sql(optimized_sql)
        # optimized_count = optimized_result.count()
        print("âš ï¸ æœ€é©åŒ–ã‚¯ã‚¨ãƒªã®å®Ÿè¡Œã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã•ã‚Œã¦ã„ã¾ã™")
        print("   å‹•ä½œç¢ºèªå¾Œã€ã‚³ãƒ¡ãƒ³ãƒˆã‚’è§£é™¤ã—ã¦ãã ã•ã„")
        optimized_execution_time = 0
    except Exception as e:
        print(f"âŒ æœ€é©åŒ–ã‚¯ã‚¨ãƒªã®å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {{e}}")
        optimized_execution_time = 0
    
    optimized_execution_time = time.time() - start_time
    
    # çµæœã®æ¯”è¼ƒ
    print("\\nğŸ“Š å®Ÿè¡Œçµæœã®æ¯”è¼ƒ:")
    print(f"   ã‚ªãƒªã‚¸ãƒŠãƒ«å®Ÿè¡Œæ™‚é–“: {{original_execution_time:.2f}} ç§’")
    print(f"   æœ€é©åŒ–å®Ÿè¡Œæ™‚é–“: {{optimized_execution_time:.2f}} ç§’")
    
    if original_execution_time > 0 and optimized_execution_time > 0:
        improvement = ((original_execution_time - optimized_execution_time) / original_execution_time) * 100
        print(f"   ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„: {{improvement:.1f}}%")
    
    print("\\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    test_optimized_query()
"""
        f.write(test_script_content)
    
    return {
        'original_file': original_filename,
        'optimized_file': optimized_filename,
        'report_file': report_filename,
        'test_script': test_script_filename
    }

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: SQLæœ€é©åŒ–é–¢é€£é–¢æ•°")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸš€ ã‚»ãƒ«17: SQLã‚¯ã‚¨ãƒªæœ€é©åŒ–ã®å®Ÿè¡Œï¼ˆã‚¹ãƒ†ãƒƒãƒ—1: ã‚¯ã‚¨ãƒªæŠ½å‡ºï¼‰
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®æŠ½å‡º
# MAGIC - æŠ½å‡ºã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®è©³ç´°è¡¨ç¤ºï¼ˆ64KBã¾ã§ï¼‰
# MAGIC - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã®è¨­å®šï¼‰

# COMMAND ----------

# ğŸš€ SQLã‚¯ã‚¨ãƒªæœ€é©åŒ–ã®å®Ÿè¡Œ
print("\n" + "ğŸš€" * 20)
print("ğŸ”§ ã€SQLã‚¯ã‚¨ãƒªæœ€é©åŒ–ã®å®Ÿè¡Œã€‘")
print("ğŸš€" * 20)

# 1. ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®æŠ½å‡º
print("\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—1: ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®æŠ½å‡º")
print("-" * 40)

original_query = extract_original_query_from_profiler_data(profiler_data)

if original_query:
    print(f"âœ… ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã‚’æŠ½å‡ºã—ã¾ã—ãŸ ({len(original_query)} æ–‡å­—)")
    print(f"ğŸ” ã‚¯ã‚¨ãƒªãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
    # 64KB (65536æ–‡å­—) ã¾ã§è¡¨ç¤º
    max_display_chars = 65536
    if len(original_query) > max_display_chars:
        preview = original_query[:max_display_chars] + f"\n... (æ®‹ã‚Š {len(original_query) - max_display_chars} æ–‡å­—ã¯çœç•¥)"
    else:
        preview = original_query
    print(f"   {preview}")
else:
    print("âš ï¸ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    print("   æ‰‹å‹•ã§ã‚¯ã‚¨ãƒªã‚’è¨­å®šã—ã¦ãã ã•ã„")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã‚’è¨­å®š
    original_query = """
    -- ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªï¼ˆå®Ÿéš›ã®ã‚¯ã‚¨ãƒªã«ç½®ãæ›ãˆã¦ãã ã•ã„ï¼‰
    SELECT 
        customer_id,
        SUM(order_amount) as total_amount,
        COUNT(*) as order_count
    FROM orders 
    WHERE order_date >= '2023-01-01'
    GROUP BY customer_id
    ORDER BY total_amount DESC
    LIMIT 100
    """
    print(f"ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã‚’è¨­å®šã—ã¾ã—ãŸ")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¤– ã‚»ãƒ«18: LLMã«ã‚ˆã‚‹SQLæœ€é©åŒ–ï¼ˆã‚¹ãƒ†ãƒƒãƒ—2: æœ€é©åŒ–å®Ÿè¡Œï¼‰
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - LLMã‚’ä½¿ç”¨ã—ãŸæŠ½å‡ºã‚¯ã‚¨ãƒªã®æœ€é©åŒ–
# MAGIC - æœ€é©åŒ–çµæœã®è©³ç´°è¡¨ç¤ºï¼ˆ1000è¡Œã¾ã§ï¼‰
# MAGIC - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ä»£æ›¿å‡¦ç†

# COMMAND ----------

# ğŸ¤– ã‚¹ãƒ†ãƒƒãƒ—2: LLMã«ã‚ˆã‚‹SQLæœ€é©åŒ–
print("\nğŸ¤– ã‚¹ãƒ†ãƒƒãƒ—2: LLMã«ã‚ˆã‚‹SQLæœ€é©åŒ–")
print("-" * 40)

if original_query.strip():
    print(f"ğŸ”„ {provider.upper()} ã‚’ä½¿ç”¨ã—ã¦ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ä¸­...")
    
    optimized_result = generate_optimized_query_with_llm(
        original_query, 
        analysis_result, 
        extracted_metrics
    )
    
    if optimized_result and not optimized_result.startswith("âš ï¸"):
        print("âœ… SQLæœ€é©åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
        print(f"ğŸ“„ æœ€é©åŒ–çµæœã®è©³ç´°:")
        
        # æœ€é©åŒ–çµæœã®è©³ç´°ã‚’è¡¨ç¤ºï¼ˆ1000è¡Œã¾ã§ï¼‰
        lines = optimized_result.split('\n')
        max_display_lines = 1000
        
        if len(lines) <= max_display_lines:
            # å…¨è¡Œè¡¨ç¤º
            for line in lines:
                print(f"   {line}")
        else:
            # 1000è¡Œã¾ã§è¡¨ç¤º
            for line in lines[:max_display_lines]:
                print(f"   {line}")
            print(f"   ... (æ®‹ã‚Š {len(lines) - max_display_lines} è¡Œã¯çœç•¥ã€è©³ç´°ã¯ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª)")
        
    else:
        print(f"âŒ SQLæœ€é©åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        print(f"   ã‚¨ãƒ©ãƒ¼: {optimized_result}")
        optimized_result = "æœ€é©åŒ–ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ‰‹å‹•ã§ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
else:
    print("âš ï¸ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãŒç©ºã®ãŸã‚ã€æœ€é©åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
    optimized_result = "ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æœ€é©åŒ–ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ’¾ ã‚»ãƒ«19: æœ€é©åŒ–çµæœã®ä¿å­˜ï¼ˆã‚¹ãƒ†ãƒƒãƒ—3: ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼‰
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
# MAGIC - ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã€æœ€é©åŒ–ã‚¯ã‚¨ãƒªã€ãƒ¬ãƒãƒ¼ãƒˆã€ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ç”Ÿæˆ
# MAGIC - ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°æƒ…å ±è¡¨ç¤º

# COMMAND ----------

# ğŸ’¾ ã‚¹ãƒ†ãƒƒãƒ—3: æœ€é©åŒ–çµæœã®ä¿å­˜
print("\nğŸ’¾ ã‚¹ãƒ†ãƒƒãƒ—3: æœ€é©åŒ–çµæœã®ä¿å­˜")
print("-" * 40)

# å¿…è¦ãªå¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
missing_variables = []

# original_query ã®ãƒã‚§ãƒƒã‚¯
try:
    original_query
except NameError:
    missing_variables.append("original_query (ã‚»ãƒ«17ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)")
    original_query = ""

# optimized_result ã®ãƒã‚§ãƒƒã‚¯  
try:
    optimized_result
except NameError:
    missing_variables.append("optimized_result (ã‚»ãƒ«18ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)")
    optimized_result = ""

# extracted_metrics ã®ãƒã‚§ãƒƒã‚¯
try:
    extracted_metrics
except NameError:
    missing_variables.append("extracted_metrics (ã‚»ãƒ«11ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)")
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ã—ã¦æœ€å°é™ã®æ§‹é€ ã‚’è¨­å®š
    extracted_metrics = {
        'query_info': {'query_id': 'unknown'},
        'overall_metrics': {},
        'bottleneck_indicators': {}
    }

if missing_variables:
    print("âŒ å¿…è¦ãªå¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“:")
    for var in missing_variables:
        print(f"   â€¢ {var}")
    print("\nâš ï¸ ä¸Šè¨˜ã®ã‚»ãƒ«ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ã‹ã‚‰ã€ã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    print("ğŸ“‹ æ­£ã—ã„å®Ÿè¡Œé †åº: ã‚»ãƒ«10 â†’ ã‚»ãƒ«11 â†’ ... â†’ ã‚»ãƒ«17 â†’ ã‚»ãƒ«18 â†’ ã‚»ãƒ«19")
    print("\nğŸ”„ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¦å‡¦ç†ã‚’ç¶™ç¶šã—ã¾ã™ã€‚")

# å¤‰æ•°ãŒå­˜åœ¨ã™ã‚‹ï¼ˆã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãŒè¨­å®šã•ã‚ŒãŸï¼‰å ´åˆã®å‡¦ç†
if original_query.strip() and optimized_result.strip():
    print("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆä¸­...")
    
    try:
        saved_files = save_optimized_sql_files(
            original_query,
            optimized_result,
            extracted_metrics
        )
        
        print("âœ… ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸ:")
        for file_type, filename in saved_files.items():
            file_type_jp = {
                'original_file': 'ã‚ªãƒªã‚¸ãƒŠãƒ«SQLã‚¯ã‚¨ãƒª',
                'optimized_file': 'æœ€é©åŒ–SQLã‚¯ã‚¨ãƒª',
                'report_file': 'æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ',
                'test_script': 'ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ'
            }
            print(f"   ğŸ“„ {file_type_jp.get(file_type, file_type)}: {filename}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèª
        import os
        print(f"\nğŸ“Š ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°:")
        for file_type, filename in saved_files.items():
            if os.path.exists(filename):
                file_size = os.path.getsize(filename)
                print(f"   {filename}: {file_size:,} bytes")
            else:
                print(f"   âš ï¸ {filename}: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        print("âš ï¸ ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’è¨­å®šã—ã¾ã™ã€‚")
        saved_files = {}
        
else:
    print("âš ï¸ ã‚¯ã‚¨ãƒªã¾ãŸã¯æœ€é©åŒ–çµæœãŒä¸å®Œå…¨ãªãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
    saved_files = {}

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ§ª ã‚»ãƒ«20: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã®æº–å‚™ï¼ˆã‚¹ãƒ†ãƒƒãƒ—4: å®Ÿè¡Œã‚¬ã‚¤ãƒ‰ï¼‰
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ä½¿ç”¨æ–¹æ³•èª¬æ˜
# MAGIC - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã®æ‰‹é †ã‚¬ã‚¤ãƒ‰
# MAGIC - è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œæ–¹æ³•
# MAGIC - é‡è¦ãªæ³¨æ„äº‹é …ã®è¡¨ç¤º

# COMMAND ----------

# ğŸ§ª ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã®æº–å‚™
print("\nğŸ§ª ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã®æº–å‚™")
print("-" * 40)

# saved_fileså¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
try:
    saved_files
except NameError:
    print("âŒ saved_fileså¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    print("âš ï¸ ã‚»ãƒ«19 (æœ€é©åŒ–çµæœã®ä¿å­˜) ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    saved_files = {}

if saved_files:
    test_script = saved_files.get('test_script', '')
    optimized_file = saved_files.get('optimized_file', '')
    
    print("ğŸš€ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã®æ‰‹é †:")
    print("1. ç”Ÿæˆã•ã‚ŒãŸSQLãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¢ºèª")
    print("2. å¿…è¦ã«å¿œã˜ã¦ã‚¯ã‚¨ãƒªã‚’æ‰‹å‹•èª¿æ•´")
    print("3. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å®Ÿè¡Œ")
    print("4. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã¨æ¯”è¼ƒ")
    
    if test_script:
        print(f"\nğŸ”§ è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œ:")
        print(f"   python {test_script}")
    
    if optimized_file:
        print(f"\nğŸ“ æœ€é©åŒ–ã•ã‚ŒãŸSQLã®æ‰‹å‹•å®Ÿè¡Œ:")
        print(f"   # Databricks SQLã‚¨ãƒ‡ã‚£ã‚¿ã§ {optimized_file} ã‚’å®Ÿè¡Œ")
        print(f"   # ã¾ãŸã¯ä»¥ä¸‹ã®Pythonã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨:")
        print(f"   spark.sql(open('{optimized_file}').read()).show()")
    
    print(f"\nâš ï¸ é‡è¦ãªæ³¨æ„äº‹é …:")
    print(f"   â€¢ æœ¬ç•ªç’°å¢ƒã§ã®å®Ÿè¡Œå‰ã«ã€å¿…ãšãƒ†ã‚¹ãƒˆç’°å¢ƒã§æ¤œè¨¼ã—ã¦ãã ã•ã„")
    print(f"   â€¢ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ§‹é€ ã‚„ã‚µã‚¤ã‚ºã«ã‚ˆã£ã¦çµæœã¯å¤‰ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
    print(f"   â€¢ ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ã®ç¢ºèª: EXPLAIN æ–‡ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")

else:
    print("âš ï¸ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ ã‚»ãƒ«21: æœ€çµ‚å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼
# MAGIC 
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - å…¨å‡¦ç†ã®å®Œäº†çŠ¶æ³ç¢ºèª
# MAGIC - ç”Ÿæˆã•ã‚ŒãŸå…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§è¡¨ç¤º
# MAGIC - æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®æç¤º
# MAGIC - å‡¦ç†å®Œäº†ã®ç·åˆå ±å‘Š

# COMMAND ----------

# ğŸ“Š æœ€çµ‚ã‚µãƒãƒªãƒ¼ã®æ›´æ–°
print("\n" + "ğŸ‰" * 25)
print("ğŸ ã€SQLæœ€é©åŒ–å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼ã€‘")
print("ğŸ‰" * 25)

print("âœ… SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºå®Œäº†")
print("âœ… Databricks Claude 3.7 Sonnetã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Œäº†")
print("âœ… åˆ†æçµæœä¿å­˜å®Œäº†")
print("âœ… ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªæŠ½å‡ºå®Œäº†")
print("âœ… LLMã«ã‚ˆã‚‹SQLæœ€é©åŒ–å®Œäº†")
print("âœ… æœ€é©åŒ–çµæœãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå®Œäº†")
print("âœ… ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆå®Œäº†")

# å¿…è¦ãªå¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
missing_summary_vars = []

try:
    output_path
except NameError:
    missing_summary_vars.append("output_path (ã‚»ãƒ«13ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)")

try:
    result_output_path
except NameError:
    missing_summary_vars.append("result_output_path (ã‚»ãƒ«15ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)")

try:
    saved_files
except NameError:
    missing_summary_vars.append("saved_files (ã‚»ãƒ«19ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)")
    saved_files = {}

print(f"\nğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:")

if 'output_path' in globals():
    print(f"   ğŸ“„ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ: {output_path}")
else:
    print("   ğŸ“„ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ: (ã‚»ãƒ«13ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)")

if 'result_output_path' in globals():
    print(f"   ğŸ“„ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æãƒ¬ãƒãƒ¼ãƒˆ: {result_output_path}")
else:
    print("   ğŸ“„ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æãƒ¬ãƒãƒ¼ãƒˆ: (ã‚»ãƒ«15ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)")

if saved_files:
    for file_type, filename in saved_files.items():
        file_type_jp = {
            'original_file': 'ğŸ“„ ã‚ªãƒªã‚¸ãƒŠãƒ«SQL',
            'optimized_file': 'ğŸš€ æœ€é©åŒ–SQL',
            'report_file': 'ğŸ“Š æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ',
            'test_script': 'ğŸ§ª ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ'
        }
        icon_name = file_type_jp.get(file_type, f"ğŸ“„ {file_type}")
        print(f"   {icon_name}: {filename}")

print(f"\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
print(f"   1. ç”Ÿæˆã•ã‚ŒãŸSQLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª")
print(f"   2. ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã®å‹•ä½œç¢ºèª")
print(f"   3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š")
print(f"   4. æœ¬ç•ªç’°å¢ƒã¸ã®é©ç”¨æ¤œè¨")

print("ğŸ‰" * 25)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“š ã‚»ãƒ«22: è¿½åŠ ã®ä½¿ç”¨æ–¹æ³•ã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
# MAGIC 
# MAGIC ### ğŸ”§ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ–¹æ³•
# MAGIC 
# MAGIC #### æ–¹æ³• 1: Databricks UI ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# MAGIC 1. **Data** > **Create Table** ã‚’ã‚¯ãƒªãƒƒã‚¯
# MAGIC 2. **Upload File** ã‚’é¸æŠ
# MAGIC 3. SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—
# MAGIC 4. ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†å¾Œã€ãƒ‘ã‚¹ã‚’ã‚³ãƒ”ãƒ¼
# MAGIC 5. ä¸Šè¨˜ã® `JSON_FILE_PATH` ã«è¨­å®š
# MAGIC 
# MAGIC #### æ–¹æ³• 2: dbutils ã‚’ä½¿ç”¨
# MAGIC ```python
# MAGIC # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’FileStoreã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# MAGIC dbutils.fs.cp("file:/local/path/profiler.json", "dbfs:/FileStore/profiler.json")
# MAGIC 
# MAGIC # å¤–éƒ¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰ã®ã‚³ãƒ”ãƒ¼
# MAGIC dbutils.fs.cp("s3a://bucket/profiler.json", "dbfs:/FileStore/profiler.json")
# MAGIC ```
# MAGIC 
# MAGIC ### ğŸ›ï¸ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆ
# MAGIC 
# MAGIC - **LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼**: `LLM_CONFIG` ã§ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨APIã‚­ãƒ¼ã‚’åˆ‡ã‚Šæ›¿ãˆ
# MAGIC - **ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º**: `extract_performance_metrics` é–¢æ•°å†…ã®é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
# MAGIC - **åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: `analyze_bottlenecks_with_llm` é–¢æ•°å†…ã®åˆ†ææŒ‡ç¤º
# MAGIC - **è¡¨ç¤ºå½¢å¼**: emoji ã¨å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®èª¿æ•´
# MAGIC 
# MAGIC ### ğŸ” ã‚¨ãƒ©ãƒ¼å¯¾å‡¦æ–¹æ³•
# MAGIC 
# MAGIC 1. **LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼**: 
# MAGIC    - Databricks: Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®çŠ¶æ…‹ç¢ºèª
# MAGIC    - OpenAI/Azure/Anthropic: APIã‚­ãƒ¼ã¨ã‚¯ã‚©ãƒ¼ã‚¿ç¢ºèª
# MAGIC 2. **ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼**: `dbutils.fs.ls("/FileStore/")` ã§ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ã‚’ç¢ºèª
# MAGIC 3. **ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼**: å¤§ããªJSONãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ã‚¯ãƒ©ã‚¹ã‚¿ã®ãƒ¡ãƒ¢ãƒªè¨­å®šã‚’ç¢ºèª
# MAGIC 
# MAGIC ### ğŸ’¡ é«˜åº¦ãªä½¿ç”¨ä¾‹
# MAGIC 
# MAGIC ```python
# MAGIC # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ‹¬åˆ†æ
# MAGIC profiler_files = dbutils.fs.ls("/FileStore/profiler_logs/")
# MAGIC for file_info in profiler_files:
# MAGIC     if file_info.path.endswith('.json'):
# MAGIC         profiler_data = load_profiler_json(file_info.path)
# MAGIC         metrics = extract_performance_metrics(profiler_data)
# MAGIC         # åˆ†æå‡¦ç†...
# MAGIC ```

# COMMAND ----------

# MAGIC %md
# MAGIC ---
# MAGIC 
# MAGIC ## ğŸ¯ ã‚»ãƒ«23: ã“ã®Notebookã®ä½¿ç”¨æ–¹æ³•
# MAGIC 
# MAGIC 1. **LLMè¨­å®š**: ã‚»ãƒ«2ã§ `LLM_CONFIG` ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨APIã‚­ãƒ¼ã‚’è¨­å®š
# MAGIC 2. **ãƒ•ã‚¡ã‚¤ãƒ«æº–å‚™**: SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’Volumesã€FileStoreã€ã¾ãŸã¯DBFSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# MAGIC 3. **ãƒ‘ã‚¹è¨­å®š**: ã‚»ãƒ«1ã§ `JSON_FILE_PATH` ã‚’å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›´
# MAGIC 4. **å®Ÿè¡Œ**: ã€ŒRun Allã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã¾ãŸã¯å„ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œ
# MAGIC 5. **çµæœç¢ºèª**: æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨AIåˆ†æçµæœã‚’ç¢ºèª
# MAGIC 
# MAGIC **ğŸ“§ ã‚µãƒãƒ¼ãƒˆ**: å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨Databricksç’°å¢ƒæƒ…å ±ã‚’ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚